{"1676209673.3742688": {"單位": "", "標題": "螞蟻", "作者": "", "發布日": 0, "摘要": "", "全文": "螞蟻屬完全變態昆蟲，要經過卵、幼蟲、蛹等階段才發展為成蟲，大部分的物種在幼蟲階段沒有移動能力，由工蟻餵養，工蟻覓食時會把液態食物吃下，回巢後再吐出餵給幼蟲，成蟲之間也以這種方式交換食物，此行為稱作「交哺 」（trophallaxis)，幼蟲發育時需要舒適的溫度，因此工蟻經常將它們搬來搬去，維持合適的生長環境。大部分螞蟻可粗分為三個階級：工蟻、雄蟻和蟻后，雌幼蟲發育成工蟻或是蟻后則取決於幼蟲期獲得的養分[來源請求]。工蟻沒有翅膀，只有雄蟻和處女雌蟻有翅膀，雌蟻交配後會把翅膀拔掉，雄蟻在交配後一段時間便會死亡。"}, "1676210646.4363058": {"單位": "", "標題": "(ㄧ)、研究計畫之背景與目的", "作者": "", "發布日": 0, "摘要": "", "全文": "自然語言生成(NaturalLanguageGeneration)將是未來深度學習領域之熱門研究議題。2022年年末ChatGPT語言模型所帶來的示範性與展示效果預期將吸引更多研究目光。而個人於中興大學之研究團隊近年於自然語言生成之耕耘已建立初步研究成果。在過去朝著自然語言生成方向前進，我們探索自動生成試題之研究方向前進。於前期研究計畫「基於深度學習語言模型之閱讀測驗自動化生成研究109-2221-E-005-058-MY3」中，我們著眼於基於深度學習模型進行閱讀測驗(ReadingComprehensionTesting)題型之自動出題。在計畫執行過程中，我們•Ying-HongChan,Yao-ChungFan:KeywordProvisionQuestionGenerationforFacilitatingEducationalReadingComprehensionPreparation.INLG(InternationalNaturalLanguageGen-erationConference)2022.•Ho-LamChung,Ying-HongChan,Yao-ChungFan:ABERT-basedDistractorGenerationSchemewithMulti-taskingandNegativeAnswerTrainingStrategies.EMNLP(EmpiricalMethodsinNaturalLanguageProcessing)Findings2020.•Shang-HsuanChiang,Ssu-ChengWang,Yao-ChungFan:CDGP:AutomaticClozeDistractorGenerationbasedonPre-trainedLanguageModel.EMNLP(EmpiricalMethodsinNaturalLanguageProcessing)Findings2022.•Hsien-YungPeng,Ho-LamChung,Ying-HongChan,Yao-ChungFan:MisleadingInferenceGenerationviaProximalPolicyOptimization.PAKDD(AdvancesinKnowledgeDiscoveryandDataMining-26thPacific-AsiaConference)2022此外我們也將前期計畫之自動出題研究成果進行實作，可於以下兩個網址進行實測。•問句生成QuestionGeneration：給定文章、指定片段，透過自然語言生成模型進行問句生成。目前系統可於https://classic.queratorai.com/進行實測。•克漏字干擾選項生成DistractorGeneration：此一部分針對給定之文章進行克漏字誘導選項生成。目前系統可於：https://cdgp-demo.nlpnchu.org/進行實測。基於過去之研究成果累積，本計劃延續自然語言生成技術於自動出題之研究成果，擬定知識類型選擇題(MultipleChoiceQuestion,MCQ)自動生成為目標。以四技二專統一測驗考試為例(如Table2所示)，知識類型選擇題將包含題幹與四個選項(其中包含正確答案與干擾選項)，主要針對某一知識範圍進行測驗。我們認為知識範圍選擇題之自動生成可分為兩部分：(1)問題題幹(Stem)生成與(2)干擾選項(Distractor)生成。題幹生成主要目標為選擇題問題本身之生成，而干擾選項則著眼於誘導選項之生成。知識類型選擇題為學習測驗最常見的測驗模式之一。對比於閱讀測驗生成，知識類型選擇題的自動生成，將可減輕教師準備個人化教學測驗之負擔，也為學生提供更有效率之學習檢測方式。以現有我們成果為基礎，一個基本之自動選擇題出題系統作法為(如圖1)所示：我們可利用資訊檢索技術(如BM25Retriever)，從給定之文集資料庫(目前使用2021中文維基百科所有中文文章)中檢索主題字詞相關文章後，結合前期計畫所開發之問句生成模型(QuestionGeneration)[4]生成題幹，而後再利用詞向量模型(例如word2vec模型)找出相近字詞以為干擾選項。我們將這樣的構想先行實作一系統(稱之為Retrieve-and-GeneratePipeline)，目前此系統可於https://mcq-demo.nlpnchu.org/使用。使用者可藉由輸入主題相關字詞，選擇題生成系統將自動生成輸入字詞相關之選擇題結果。舉例而言，我們以「骨髓、白血病、淋巴」為主題字詞輸入至系統。我們挑選較佳之成果顯示於Table1。自動生成選擇題AutomaticGeneratedMCQQ1:哪一種癌症種類的統稱中可以看出它會造成不正常白血球的大量增生?(a)白血病、(b)乳腺癌、(c)腎病、(d)帕金森氏症Q1:白血病的發病原因通常是發生在哪一個器官上?(a)肝臟、(b)淋巴、(c)甲狀腺、(d)骨髓Q:白血病是最常見的兒童癌症，其中80%的病例是慢性的哪一種性病變?(a)淋巴結、(b)微血管、(c)淋巴、(d)黏膜Table1:目前選擇題自動生成結果範例。目前系統可於https://mcq-demo.nlpnchu.org/。目前我們挑選較佳之生成結果顯示於此Table中。建議審查委員自行輸入關鍵字詞組，來觀察我們Figure1:Retrieve-and-Generate基本架構。本例中我們以”骨髓、白血病、淋巴”為關鍵詞進行文章檢索，而再使用”骨髓”為答案，分別生成問句與干擾選項。從展示之結果中，我們發現自動選擇題出題之可能性。但仔細觀察會發現現有系統離實際應用仍有空間須待努力。也因此觸發本期研究計畫之提出。具體而言，我們認為選擇題自•句子等級之干擾選項生成(Sentence-LevelDistractorGeneration)首先，關於干擾選項的部分，若仰賴詞向量模型來產生干擾選項，我們所能取得的干擾選項便僅限於單詞(Word-Level)等級。從統測試題中，我們發現許多題目之干擾選項屬於句子等級(Sentence-Level)。以Table2中Q3與Q4選擇題為例，我們可以看到選項皆為句子等級之描述，此為目前基於詞向量來產生干擾選項的做法所不能。•反事實干擾選項生成(Counter-FactualSentenceGeneration)我們觀察到選擇題題型中，有著下列何者為錯誤或正確類型之考題。其干擾選項是一個較長的描述句。以Table2為例，Q5與Q6為分別詢問下列選項何者正確/錯誤。此題型，為變像之是非題。除了句子等級生成外，核心技術牽涉的是反事實(Counter-Factual)句子之生成。如何生成此類型之句子以為干擾選項也將是研究重點所在。•選擇題生成品質提升(MCQQuestionQualityImprovement)此外，我們也看到retreive-and-generatepipeline的做法，所產生的題幹本身有時是一個過於空泛之問題(例如，下列何種疾病為兒童常見之癌症？)或者並非語意正確之語句(例如，中樞性的什麼症狀是缺乏抗利尿激素的?)。如何有效判斷生成不良之語句，減低人工進一步過濾/挑選題目之負擔也是需進行研究之議題。此外，基於詞向量之干擾選項作法也有待商榷;隨機從相近詞向量挑選distractor，是否能確保distractor之誘導品類型I:單詞等級的干擾選項Word-LevelDistractorsQ1：淋巴器官是人體防禦系統中重要的一環，可提供淋巴細胞發育或活化所需要的條件與狀態。下列何者是淋巴球進行防禦與活化的器官？(a)骨髓(b)胸腺(c)肝臟(d)脾臟Q2：健康人體感覺神經傳入大腦的訊息大部分經過轉接，(a)視丘(b)下視丘(c)中腦(d)橋腦類型II:句子等級的干擾選項Sentence-LevelDistractors(a)無就巢性(b)體型大(c)著肉多(d)耐粗食Q4：下列何者為「聯合國國際糧農組織」(FAO)成立最重要的目標及首要任務？(a)消除及戰勝飢餓(b)聯絡及交流農業資訊(c)自然環境的永續及保護(d)實現國際間的公平貿易類型III:句子等級且與事實相違背的干擾選項Counter-FactualDistractorsQ5：有關人類上皮細胞的細胞週期之敘述，下列何者正確？(c)後期，分離的染色分體往細胞中央移動(d)末期，細胞膜內凹且細胞質開始一分為二Q6：有關一健康成人泌尿系統功能的敘述，下列何者錯誤？(a)腎臟每日約產生濾液180公升及尿液1.5公升(b)腎小管藉壓力差方式再吸收葡萄糖回微血管(c)腎臟藉排尿量多寡來維持體內水分的恆定(d)若出現尿毒症狀時，可使用血液透析方式進行治療Table2:四技二專統一測驗選擇題類型概觀。Q1與Q2擷取自110學年度護理基礎生物考科，Q3與Q4110學年度農業群農業概論考科，Q5與Q6取自110學年度農業群基礎生物考科質？也許所挑選的distractor並不具誘導性，使得受測者很輕易地可挑選出正確答案，或者挑選到答案的同義詞，造成一題有多個正確答案。根據先前計畫執行訪談教師所知，於測驗時為考量受測全體鑑別度，教師出題時會考量干擾選項彼此之間之難易度；通常三個干擾選項中，會有一個離正確答案較遠之干擾選項(誘導性較低)，一個次之，一個最為困難最具有誘導性。藉此來有適當的鑑別度。目前我們直接使用詞向量之作法並未考量此設計。•主題題組生成(MCQQuestionGroupGeneration)目前我們的系統僅為單一關鍵字詞之出題。我們認為一個良好的自動出題應該是以題組為單位進行出題；使用者指定一主題知識，系統根據此一主題知識範圍進行5-10題之選擇題自動生成，題目之間也應該彼此考量。目前Retreive-and-Generatepipeline的做法，我們使用使用者給定的關鍵字為答案，進行題目之生成。然而比較好的做法應為將使用者給定的關鍵字集合為知識範圍之指定，系統根據該範圍自動挑選重要概念關鍵字進行答案挑選，而後再進行選擇題題組之生成。針對上述之研究挑戰，於本研究規劃中我們擬定下列八項研究議題進行探討。•研究項目ㄧ：基於語言模型與知識圖譜之Text2Text干擾選項生成模型(DistractorGenerationbasedonText2TextLanguageModelandKnowledgeGraph)Figure2:Text2Text干擾選項生成架構Figure3:基於Masked-LM之反事實干擾選項現有之Distractor生成研究多是基於CandicateGenerating-and-Ranking架構，先透過知識庫(如Wordnet)或者透過MaskedLM的方式取得候選字後，再適當進行候選詞特徵排序以為最終干擾選項。對此我們有不同之想法，我們預計嘗試把干擾選項生成設定為一個Text2Text任務；如圖2所例，預期的輸入為題幹Q與答案A之串接字串，透過ModelEncoder與Decoder轉化輸出為一連串之干擾選項字串“重組DNA技術[SEP]電腦斷層掃描[SEP]組織培養”。我們透過語言生成模型直接將輸入對比轉換為輸出。此外，透過此一Text2Text方式，生成句子等級的干擾選項也將為可能。再者，近年來語言模型研究趨勢除了將模型參數量與訓練資料持續增加外，引入知識圖譜來強化生成模型於常識理解與推理能力也成為研究社群的關注所在[5][13]。也因此，探索整合知識圖譜於選擇題自動生成也因此成為我們規劃之研究項目。希冀藉由整合知識圖譜來提升干擾選項之誘導性與題幹生成之正確性。•研究項目二：基於Masked-LM之反事實干擾選項生成研究(Counter-FactualSentenceGenerationbasedonMasked-LM)如同前言所述，“下列何者正確/錯誤”題目為常見之類型選擇題題型。對此，我們預計嘗試以下之作法。首先，我們假定可取得正確敘述之語句S。我們先針對語句S中的字詞進行命名實體辨識(NamedEntityRecognition,NER)，其後再利用語言模型針對所取得之命名實體進行WholeWordMasking(WWM)，填入機率較低之填空結果，藉此生成反事實描述之語句。如Figure4所例，我們將所給定之事實描述，藉由NER方式挑選”葡萄糖”一次進行Masked，再藉由語言模型推測該Masked字詞，並挑選機率相對較低之預測填入。藉此創造出反事實之陳述句。然而，如何確保不挑選到同義詞？要進行多少個字詞之Masked？如何自動從主題文章中挑選事實陳述句以進行改寫？等等•研究項目三：基於知識圖譜強化之反事實干擾選項生成(Counter-FactualSentenceGen-erationbasedonKnowledge-GraphAugmentedLanguageModel)針對反事實干擾選項生成之議題，除了研究項目二之Masked-LM作法外，我們另一個構想為利用知識圖譜來強化Counter-Factual語句之生成。如Figure4所示，我們基本想法為先萃取與題幹相關聯之知識圖譜子圖(LocalSubgraph)並取得知識三元組，以此輸入至語言模型中，訓練語言模型反向改寫三元組語句，以此為反事實干擾選項。然而，此模式之訓練資料該如何取得？如何從知識圖譜上萃取子圖，並生成合適三元組？如何引入GraphNeuralNetwork來萃取結構？為此研究項目需進行探討之工作內容。•研究項目四：基於對比式學習之干擾選項生成模型強化(DistractorGenerationModelbasedonContrastiveLearning)我們過去的研究經驗[8]與相關研究[35]皆指出當使用Generative語言模型來生成句子Figure4:基於知識圖譜強化之反事實干擾選項生成構想層級之輸出時，模型經常會產生語意之相同，但字面的結果。如Table3所示，選項A跟C雖然字面敘述上不同，但實際上都是描述同樣的意思。生成重複選項將導致選擇副交感神經興奮會導致使心跳加速，血壓上升Table3:模型生成的誤導選項重複出現的範例對此，我們預計探索對比式學習(ContrastiveLearning)[7][35]來提升生成品質。對比式學習於自然語言之應用中主要想法為透過選取PositiveSample以及NegativSample使得模型去學習到生成句子跟同義句子的相同以及跟異類句子的不同之處，這樣我們可以使用重複選項當作我們的NegativeSample，然後資料集中的干擾選項當作PostiveSample，期望模型可以學習避免生成出重複的誤導選項。因此，我們預計探索對比式•研究項目五：使用Open-DomainQuestionAnswering錯誤結果之干擾選項挑選研究(Us-ingIncorrectAnswersfromODQAasDistractors)而除了利用生成模型產生Distractors外，我們發現利用Open-DomainQuestionAnswer-ing(ODQA)模型所產生的錯誤結果是一個值得探討的干擾選項的取得方式。具體而言，現有的ODQA模型主要包含一個Retriever與一個Reader，Retriever負責檢索相關的文章，而Reader利用檢索出來的文章利用閱讀理解模型(ReadingComprehensionModel)來找尋答案片段。由於ODQA並非完美，因此也會有錯誤結果之回報。但我們發現錯誤結果與正確答案並非天差地遠，閱讀理解模型所回報之結果通常也與答案接以圖5為例，當我們下達Q：「下列何者為目前世界上應用最廣的殺草劑，其具備有生長素之特性，常用於闊葉雜草防治，亦為選擇性殺草劑的一種？」，ODQA系統將取得圖5中右半部之答案：「萌前除草劑、2,4-D、非選擇性除草劑、10殺丹粒劑」。其中，2,4-D為正確答案，餘者為錯誤答案。然而我們覺得這些錯誤答案，可以當作Q之干擾選項來使用。有別於基於生成模型研究項目，利用ODQA模型之錯誤答案來當作干擾選項應可提供不同思路進行干擾選項生成。這樣的想法乍看可行，但仍有若干技術點需進行探討。諸如，如何控制ODQA模型之錯誤？我們需要在Reader部分弱化Reader的能力，或者弱化Retriever能力，來製造ODQA的錯誤？如何從錯誤回報結果挑選干擾選項？這些皆是尚待深入研究之議題。也因此觸發本研究項目之規劃。Figure5:利用ODQA模型之錯誤答案來當作Distractors•研究項目六：基於NodeCentrality考量主題涵蓋性之選擇題題組生成(MCQGroupGenerationconsideringTopicCoveragebasedGraphNodeCentrality)上述所規劃的選擇題生成做法，主要針對單題進行生成。若要產生多題/題組考題，就是重複相同流程，獨立產生個別考題後，整合為一題組。然而，題組生成做法應當考慮題與題之間之關聯性與互補性。因此，我們認為比較好的做法應當為根據使用者所指定的主題，透過(1)知識圖譜子圖萃取來涵蓋所指定主題之內容與(2)透過主題重要概念萃取：藉由知識圖譜上NodeCentrality方式來萃取關鍵字，以為題幹之生成。此外，如何根據主題互補性，來挑選個別生成之題組，用以組成選擇題題組也是需進行•研究項目七：考量題幹可用性與干擾選項間關聯之Retrieve-and-Generate架構優化(Op-timizingRetrieve-and-GenerateFrameworkconsideringStemUtilityandDistractorCorrela-tion)本研究項目著眼於優化現有Retrieve-and-Generate架構設定。Retrieve-and-Generate架構雖然簡單，但我們可以發現其於單字等級的干擾選項生成已有一定的可用性。我們預計可優化之處為：(1)我們發現引用現有系統於題幹生成時，會有題幹生成結果不佳的狀況。對此，我們發現其中的一個原因為Retriever所檢索之文章品質不佳(文章過短或者與主題不相關)，導致不良之題幹生成。因此如何改善檢索機制來挑選較佳的文章用以進行生成，便是值得探討的課題。再者，題幹生成不佳也有部分原因是來自於問句生成模型本身之生成錯誤，如何有效過濾生成不佳的題幹也因此是一個可需進行研究之項目。(2)如同前言所述，干擾選項之挑選應該考量彼此間之關聯，而非獨立挑選。如何納入此觀點來進行干擾選項之生成也因此成為預計關注之議題。又再者，我們基於word2vec方式之字詞等級干擾選項挑選是否可以引入語言模型之詞向量來求取•研究項目八：生成難度可控之選擇題生成研究Difficulty-ControlledMCQGeneration發展自動化生成考題之可能願景為：針對個人化學習自對應生成難度適中之測驗，以增加學習與檢測之效率。因此，如何針對現有技術探討生成結果難度控制也因此成為一可探索之研究議題。由於我們的基礎架構為基於文章檢索來取得包含使用者所給定的關鍵字之文章，並使用該文章與關鍵字為問題生成模型之輸入，來生成選擇題問句。我們若挑選較為冷門之文章進行生成，則預期可以生成較為困難之題幹。而就干擾選項而言，若我們基於研究項目五「使用Open-DomainQuestionAnswering錯誤結果之干擾選項挑選研究」，利用ODQA來取得干擾選項之作法。我們可藉由調整輸入給閱讀理解模型之文章來產生難度較高的錯誤。具體而言，給定一有包含正確答案A之文章C，我們先將答案片段A於文章D中刪除，而後將其輸入至閱讀理解模型，藉此產生錯誤之閱讀理解結果，以這樣的錯誤結果當干擾選項預期將有較高之誘導能力。相類似的想法也可以套用為挑選Retriever認為相關度比較不高的文章以為閱讀理解模型之輸入。此外，就生成題目之廣度與深度而言，我們可藉由控制取得之文章來調配題組之廣度與深度。例如，若我們以單篇或少數篇文章進行生成，我們所生成的主題題組針對測驗內容而言，應該可產生知識性較為深入之試題。而若針對較多篇文章，則應可產生範圍較廣之測驗試題。上述之研究議題，乍看可行但仍有許多細節需進行探索，如上述研究背景目的所述，本研究計畫擬探索基於現有Retrieve-and-GeneratePipeline之選擇題自動生成架構進行強化。我們以三年為期，規劃在此目標下擬定八項關鍵技術進行研究探討。如Figure6所示，八個工作項目又可區分為三項主軸：「選擇題生成品質提升」、「反事實選項生成」與「群組生成與難度控制」。其中，於「選擇題生成品質提升」研究主軸中，我們探索利用語言模型與知識圖譜架構來生成，將選擇題選項生成問題設定成為一Text2Text任務，輸入為問題題幹與正確答案所串接之TextSequence，而輸出為所生成之distractor串接之TextSequence。此外，我們也嘗試引入對比式學習來改善Text2Text生成模式之品質。我們也預期規劃探索利用ODQA方式來取得干擾選項。而於「反事實選項生成」研究主軸中，我們探索反事實陳述句之生成。我們預計嘗試基於MaskedLM取代重要名詞之反事實語句生成作法與基於知識圖譜知識三元組萃取改寫之作法。於第三個研究主軸中，我們則嘗試針對題組生成與難度可控制之干擾選項生成進行研究。MultipleChoice,Multi-此資料集特色在於為Multi-hop形式，答案需從複數文檔中推理而成；資料來自維基百科以及PubMed，PubMed為生物醫學論文的搜尋引擎，在兩個來源各自提供51,318(Wiki)和2,508(PubMed)筆問題。資料來自國高中英文考試，提供超過10萬筆問題以及2.8萬篇文本，題型上為英文閱讀測驗，會需要閱讀故事或對話紀錄，而針對該文本會有複數題目，每個題目此資料集的挑戰在於，內容來自於7個不同領域(新聞、小學自然科學、旅遊、小說故事等等)，為文本與題目提供多樣性，總計約1萬筆問題與800篇文本。MultipleChoice,Open-為Open-domain形式，並不提供文本，只有問題與答案；包含12,247筆問題、一個正確答案和四個干擾選項；題目類型為知識與常識題，來自於ConceptNet與同時包含兩種形式資料，填空題與Open-domain；填空題形式命名為QUASAR-S，其中資料來源自StackOverflow，提供約37K筆問題。Open-domain形式命名為QUASAR-T，提供約43K筆問題。來自StanfordUniversity的閱讀理解資料集，包含了超過100K筆問題和536篇對應的維基百科文本；延續了SQuAD1.1的題目外，[31]另外加入約50K筆無答案的題目，進一步測試機器能否在無對應答案時回答否，而MedQA是基於美國醫師執照考試(USMLE)所產生的資料集，其中問題皆為較為困難的專業醫學相關的知識，且包含了英文、簡體中文、繁體中文分別有約12K、34K、14K個問題集，這種專業的知識能使機器針對單一領域產生更具參考價值的distractor，目前此任務排名上最佳為ValentinLiévin團隊做出的Codex5-shot訓練機器學習模型進行閱讀理解(MachineReadingComprehensionTask)為最常見的深度學習自然語言任務。諸如，SQuAD[32]、RACE[21]、DROP[10]與HotPotQA[41]等資料集，皆是測驗/訓練機器學習模型於答案找尋、邏輯推理與摘要之能力。其中，許多機器閱讀理解任務採用多選題方式進行測驗。選擇題的自動答題與選擇題的自動生成為一體兩面之應用。因此具有選擇題樣態的資料集為我們選擇題自動生成研究之根本資料集，我們將目前所探知之相選擇題與干擾選項生成之相關研究可按應用類型可區隔為英文學習測驗與知識學習測驗。干擾選項生成研究於英文學習測驗應用，主要以自動生成閱讀理解與克漏字類型的考題為目標。閱讀理解之干擾選項生成與本研究計畫之知識類型選擇題之自動出題目標不同。因此本研究回顧中，我們不做此分類每一研究細部說明。我們將此類型之相關研究按不同象限做分類：(1)干擾選項層級之差異(生成字詞等級與句子等級之差異)象限，(2)題目類型(閱讀理解與克漏字測驗)，(3)生成方式(CandidateRanking與Generative)。我們按此三象限，將目前之相關研究摘要整理於Table5中。而知識學習測驗類型的選擇題生成，則於這一兩年逐漸吸引相關研究投入。我們認為與本研究計畫較為相關之研究有[22][40][3]，以下我們針對個別文獻討論如下。首先，與我們有相同目標者為[22]，也是針對特定知識主題生成選擇題。其主要作法Table5:現有閱讀測驗與克漏字干擾選項生成研究整理表為：先由人工給定一個主題，程式使用該主題名稱的維基百科頁面作為起始點(例如給定“體育”後搜尋體育相關維基百科)，保留熱門文章並取出文章的第一段擷取摘要，由此產生大量句子。並將這些句子依單純英文文法規則生成簡單5W問句與答案，例如who問句是將第一個動詞前的名詞作為正確答案並從句子中移除，what問句是將第一個夾在動詞間的名詞移除作為正確答案並從句子中移除，移除答案後的句子再帶入問題模板生成制式問句。生出五種問句後再依英文文法的詞與詞的相關性去決定哪個問句最適合。在干擾選項生成的部分，先隨機選取其他句子所產生的答案，再藉由語言模型去排序與問題的相關性，取前三高之預測為干擾選項。但本論文並未使用問句生成模型，採取之作法也與本計畫所規劃有所不同。於論文中[40]，則探討如何讓干擾選項更加多樣且與正確答案相差不遠(以具誘導性)。對此，創造了一款監督式分類任務來找出文章中屬於關鍵句子(及對產生干擾選項生成有用的句子)，作者在訓練之前會將訓練用的干擾選項和文章中的每個句子做比對，若該句子和干擾選項相似度達到一定程度，則該句子將成為文章中的關鍵句，透過這個方式在文章中能被修改成為新的干擾選項的句子將會達到多樣性的效果，且透過相似度對比，干擾選項也不會跟原始文章相差太遠從而提升干擾選項的品質。本研究之作法可供我們借鏡參考對關鍵文章/句選擇設計，用以提升干擾選項之多樣性。而與本計畫研究項目五「使用Open-DomainQuestionAnswer錯誤結果之干擾選項」走非生成想法相似之研究為[3]。其主要想法為作者發現於干擾選項資料集中，有許多干擾選項一再被使用。促使論文作者想為新的問題檢索以重複使用干擾選項。作者提出一個Transformer-baseddistractorretrievalmodels，該模型通過考慮問題的上下文來選擇干擾選項模型包含三個部分，分別為D-SIM模型、Q-SIM模型以及DQ-SIM模型。在D-SIM模型中使用mBERT計算同一個MCQ中問題和答案以及對應的干擾選項的向量，並計算他們之間的相似度。而在Q-SIM模型中作者假設不同問題共享一個或多個干擾選項或答案，代表問題可能在語意上相關，因此他們相關的干擾選項可以用作彼此的良好候選干擾選項，因此作者將訓練資料重新排序，使這些共享至少一個干擾選項或答案的問題聚集在一起，接著將資料送進Q-SIM模型計算不同的MCQ中問題之間的相似度。在DQ-SIM模型線性結合前兩個模型，從各自的優勢中獲益，透過基於分數和基於排名的合併策略計算來自D-SIM模型和Q-SIM模型的分數，並設計演算法使得高排名之干擾選項有較大的權重。重複使用現有資料集中之干擾選項為一創新想法，但仍與我們之ODQA做法有所不同。[OpenDomainQuestionAnswering相關研究]本計畫的選擇題生成架構主要基於兩元件:(1)文章萃取與(2)問句生成。目前我們所使用的文章萃取機制為傳統之BM25檢索。而近年來由於ODQA研究領域之發展，基於深度學習之DenseRetrieval技巧也紛紛於ODQA研究領域中所提出。我們預期DenseRetrieval技術也將於我們知識類型選擇題自動出題中擔任關鍵角色。茲簡要回顧現有ODQA之研究如下。ODQA架構中Retriver之重要性不言可喻。不同層級的語意檢索也影響著ODQA的效能。於[28]研究中，作者提出一個簡單而有效的pipeline系統，研究段落和句子級別語義檢索及其對下游任務的潛在影響，作法上是先檢索段落級別的段落，然後從上一步檢索後取得的段落中再去檢索句子級別的段落。該系統在事實驗證和Multi-hopQA方面進行評估，實驗結果顯示QA任務與事實驗證皆仰來上游檢索之結果。於[23]論文中，作者在ODQA架構的retreiver端提出了一個有效率的弱監督式學習的方法(InverseClozeTask)來對retreiver進行預訓練，給予模型完全無人工標記的維基百科原始資料，藉由論文中的方法將資料切分成固定長度的段落，並隨機選取其中的一個句子當作標籤，讓模型去學習如何從給定的句子去資料庫中找尋與之最相關的文章。在實驗數據中此方法訓練出的ODQA模型分數高於傳統的BM25檢索方法。而於DPR[16]論文中，作者為解決傳統BM25無法檢索語意相似段落的問題，提出了基於深度學習模型之DensePassageRetrieval，並提出使用in-batchnegative的技術，在密集向量的檢索取得極佳的效果改進。我們也預計引入中文DRP模型來提升目前題幹生成所需之而於GAR[27]研究中，作者們探討傳統基於BM25的檢索無法檢索沒有單詞重疊或語意相似的段落的問題，而基於密集向量的檢索方式在計算上又過於複雜，且會有資訊損失的問題。該論文透過預訓練語言模型自動生成與查詢語句相關的內容，可增加查詢語句的資訊量，隨後透過BM25進行檢索，該方法比起傳統BM25能檢索到更精確的段落，且執行效率也快於基於密集向量的檢索方式。透過GAR中的PLM可對查詢語句的資訊進行補充，原論文中是補充答案、相關段落及段落標題，可基於該手法讓PLM生成問題與原始答案的關聯，對這些關聯進行組合或刪除即可檢索與原答案有相似關係的查詢語句，可檢索到適合製作干於[19]，作者觀察到基於語言模型之檢索技術雖然效果很好，但比以往的方法需要極高的計算成本，特別是在計算每個query與document間的relevancescore時。本篇論文作者提出了ColBERT，引入lateinteraction架構，對query與document進行獨立的encoding，於模型輸出最終階段才進行相似度運算。而[18]中，則使用近一步基於ColBERT當基礎模型來進行retreiver端的訓練，使用了Relevance-guidedSupervision(RGS)的方法來訓練retreiver的模型。相較於傳統的retreiver訓練資料，傳統方法需人工標註的訓練資料，但作者認為人工標住資料往往存在偏見，進而影響模型的效能，所以在論文中提出了用訓練好的模型來找尋最適合的訓練資料，並把這些資料用於訓練下一個模型，使用這個方式來進行後續的訓練，最後得出最佳的模型。此方法訓練出的模型分數明顯高於目前opendomain的其他方式。在找尋訓練資料時，模型會先搜尋前k筆資料，並把前20筆資料中含有shortanswer的資料當成possitiveexample，而前20筆以外剩餘的資料中不含有shortanswer的資料當成negativeexample，此方法尋找的負面訓練資料交由reader進行預測能得到與正確答案相關的錯誤答案，如能改良ranking的方式，應用於干擾選項生成，預期將能提升干擾選項之品質。[ContrastiveLearning於自然語言生成相關研究]對比式學習始見於電腦視覺領域，其於影像應用上有著顯著的效果提升。因此，許多自然語言處理之研究者也將其引用至自然語言生成領域，諸如[11][7][35]。於[11]研究中，作者們提出了使用ContrastiveLearning的作法來學習SentenceEmbedding，他們使用BERT作為基礎模型，透過不同的dropoutmask使模型能用同樣的輸入產生不同的SentenceEmbedding，但它們之間的距離會是近的，也就可作為PositiveSample。然後再使用其他in-batch的句子作為NegativeSample，以達到非監督式訓練。而在有訓練資料的情況下，如NLI資料集，則可以將Eentailment視為PositiveSample，Contradiction以及其他in-batch的句子視為Negative而再基於[11]研究後，於[7]研究中，作者們提出加上了Discriminator來加強SentenceEmbedding的學習效果。具體而言，對每筆輸入來說，會透過SentenceEncoder產生一個SentenceEmbedding，同時也會隨機的Masking輸入句子其中的幾個Token，並透過一個fixed的Generator來還原MaskToken，而Discriminator要試著透過沒被修改過的SentenceEmbedding的資訊去辨認修改後的句子中的哪一個Token有被修改過，整個框架會透過SentenceEmbedding的ContrastiveLearningLoss以及Discriminator的ReplacedTokenDetection而與我們預計引入對比式學習來提升句子等級的干擾選項生成效果想法接近的研究也可見於[35]。該研究指出發現了以往autoregressive模型使用BeamSearch去決定要生成的文本時，會產生語意不自然以及沒意義的重複文字，儘管後續有人使用取樣以及修改ObjectiveFunction來嘗試解決這個問題，但這個方法時常會產生語意不連貫的問題。因此作者提出了使用ContrastiveLearningLoss加上原先的MaximumLikelihoodEstimationLoss來訓練模型，並且使用ContrastiveSearch來作為decodingmethod。[Counter-FactualSentenceGeneration相關研究]於Counter-FactualGeneration研究中，[44,33]為相關研究之先驅。主要想法是使用神經網路來生成反事實敘述，這個概念背後的想法是同時使用語言模型和各種採樣的策略來達成生成反事實敘述的效果。接續之前的成果，[39]提出了Polyjuice架構，可以控制輸出的反事實敘述的類型。另外一種生成干擾選項的研究是使用干擾選項生成(DistractorGeneration)的手法，干擾選項生成(DistractorGeneration)的目標是給予了一篇文章、問題和正確答案後，模型能夠生成似是而非的干擾選項。於研究[24]中，作者最先提出使用了神經網路來生成干擾選項，而研究[8]則延續了前者的成果，把重點放在生成複數的干擾選項並提出了一個負樣本學習策略(negativetrainingstrategy)來加強干擾選項生成。但這上述兩個研究主要都著眼於閱讀理解題型中干擾選項之生成，與本研究知識類型選擇題生成目標所有不同。而在關於AbductiveInferenceGeneration的相關研究中，於[30]中作者提出了故事改寫的任務，此任務目標是要改寫一段故事的敘述並且導致產生與原先不同的結局，與顯永的論文的方式的差異為故事改寫任務是根據給定的誤導推論來生成不同的結局，在MIG任務中是給定敘述句與其前後文來生成誤導推論。[2]研究中是讓模型根據上下文來推論中間適合加入的句子，此研究主要是產生正確的推論敘述，而在MIG任務[29]主要是產生相反的誤導推論敘述。於我們先前的研究[29]中，我們提出一個自然語言的誤導推論生成任務，任務步驟是當給定了一個陳述事實的句子並給定其上下文，將其定義為(x,y,z)與誤導推論ˆy，模型會生成與該事實相反的推論句子，由此可以讓模型產生出多選題似是而非的干擾選項。然而這樣任務與我們所設定之反事實出題仍有所不同。而與我們生成Counter-Factual陳述句想法接近的研究為：[46]。於該研究中，作者探討有別於過往的是非題，將文章中原始句子作為True的問句並通過反義詞替換關鍵字生成False問句，本篇作者通過使用一種新穎的masking-and-infilling策略來生成更靈活與複雜的問句。要被mask的關鍵字找法有很多種，像是SRL結果中的predicate，句子中連接詞(”that”、”when”、”since”)之後的部分或是數字等等，來找出值得提問的句子。Infilling的作法則是以BART為基底作為生成模型，將原始句子的關鍵字mask後，提供前後句子作為BART的上下文後產生句子填入。作者為了找值得提問的句子所提出的方法亦可用來作為多選題產生題目的答案選擇方法，而infilling的想法如果使用得宜，在提供出題範圍的前後文後，也能將欲出題句子挖空讓BART模型填補後產生選項。雖然與我們目標相似，但仍與我整合知識圖譜與語言模型為近年語言模型訓練熱門關注議題。於[42]研究中，作者們提出一CounterfactualGenerationControllableCounterfactualGenerationDistractorGenerationMisleadingInferenceGeneration種能將文本和知識圖譜的資訊進行深度雙向、自我監督的預訓練的方法，透過作者的方法可以將一些特定領域的KnowledgeBase資料集結合相關文獻去訓練語言知識模型，讓語言模型和知識圖譜之間的資訊互相交換，並且在回答特定領域問答的任務上，表現優於現有的語言模型和知識圖譜增強模型。我們想透過作者提供的預訓練模型，來做多選題選項生成，透過知識圖譜所額外提供的資訊，讓在考慮生成選項時，除了題目和答案的文字外，用這些文字從知識圖譜萃取相關的知識圖譜子圖幫助生成選項有更多參考資訊。知識圖譜於問句生成之研究也見於[5][13]。於[5]研究中，作者之想法為使用知識圖譜進行問題生成任務時，通常只會取出資料與關係的三元組合，並不能完整利用知識圖譜中豐富的資訊，因此作者基於GraphNeuralNetworks提出了BidirectionalGatedGraphNeuralNetwork(BiGGNN)，將知識圖譜中的node及edge編碼成向量，使得每個node都包含了周圍其他node的資訊，並對答案的node進行標註，隨後透過RNN進行問句生成，生成過程中RNN會決定生成新文字或直接複製某個node的資訊放進問題。我們應可借鏡此研究論文的想法，可將知識圖譜及答案的資訊完整編碼，可基於此特性，在生成問題時同時考慮答案附近的node，讓模型在建立問題的同時也能根據附近的node產生干擾選項。而於[13]研究中，則提出的ISEEQ架構，主要目的是希望從簡短的敘述句中去尋找更多訊息來反問使用者是否符合關於敘述句背後未提及的情況，在使用時，使用者只需輸入簡短的敘述句，模型會先從知識圖譜擴展並增加query的關鍵字，透過這些關鍵字模型就能retrieve更多相關的文章進行ranking，在從這些文章中使用QG模型產生一些原始query中未提及但可能相關背景知識，例如當使用者說道:“我感到疲勞且渾身無力”，ISEEQ能回覆:“請問病徵持續了多久?”，或是“你最近14天有出國過嗎?”這類可能與其他知識相關的問句。這種方式最大的貢獻及是讓機器學習到”反問”的能力，而非一味的透過query生成出結果。這樣的想法，或許可引用來提升題幹生成的精確度，舉例來說:有位教師想出關於日治時期的歷史題目，我們就能先藉由ISEEQ來詢問是要出與日治時期戰爭相關的題目或是與日治時期建設相關的題目，以此來取得更精確的題幹生成需求。[1]J.Araki,D.Rajagopal,S.Sankaranarayanan,S.Holm,Y.Yamakawa,andT.Mitamura.Gen-eratingquestionsandmultiple-choiceanswersusingsemanticanalysisoftexts.InProceedingsofCOLING2016,the26thInternationalConferenceonComputationalLinguistics:TechnicalPapers,pages1125–1136,2016.[2]C.Bhagavatula,R.L.Bras,C.Malaviya,K.Sakaguchi,A.Holtzman,H.Rashkin,D.Downey,S.W.-t.Yih,andY.Choi.Abductivecommonsensereasoning.arXivpreprintarXiv:1908.05739,[3]S.K.Bitew,A.Hadifar,L.Sterckx,J.Deleu,C.Develder,andT.Demeester.Learningtoreusedistractorstosupportmultiplechoicequestiongenerationineducation.IEEETransactionsonLearningTechnologies,2022.[4]Y.-H.ChanandY.-C.Fan.Arecurrentbert-basedmodelforquestiongeneration.InProceedingsofthe2ndWorkshoponMachineReadingforQuestionAnswering,pages154–162,2019.[5]Y.Chen,L.Wu,andM.J.Zaki.Towardsubgraphguidedknowledgegraphquestiongenerationwithgraphneuralnetworks.arXivpreprintarXiv:2004.06015,2020.[6]S.-H.Chiang,S.-C.Wang,andY.-C.Fan.Cdgp:Automaticclozedistractorgenerationbasedonpre-trainedlanugagemodel.InFindingsoftheAssociationforComputationalLinguistics:[7]Y.-S.Chuang,R.Dangovski,H.Luo,Y.Zhang,S.Chang,M.Soljac￿ic￿,S.-W.Li,W.-t.Yih,Y.Kim,andJ.Glass.Diffcse:Difference-basedcontrastivelearningforsentenceembeddings.arXivpreprintarXiv:2204.10298,2022.[8]H.-L.Chung,Y.-H.Chan,andY.-C.Fan.Abert-baseddistractorgenerationschemewithmulti-taskingandnegativeanswertrainingstrategies.InFindingsoftheAssociationforComputationalLinguistics:EMNLP2020,pages4390–4400,2020.[9]B.Dhingra,K.Mazaitis,andW.W.Cohen.Quasar:Datasetsforquestionansweringbysearchandreading.arXivpreprintarXiv:1707.03904,2017.[10]D.Dua,Y.Wang,P.Dasigi,G.Stanovsky,S.Singh,andM.Gardner.ingcomprehensionbenchmarkrequiringdiscretereasoningoverparagraphs.arXiv:1903.00161,2019.[11]T.Gao,X.Yao,andD.Chen.Simcse:Simplecontrastivelearningofsentenceembeddings.arXivpreprintarXiv:2104.08821,2021.[12]Y.Gao,L.Bing,P.Li,I.King,andM.R.Lyu.Generatingdistractorsforreadingcomprehen-sionquestionsfromrealexaminations.InProceedingsoftheAAAIConferenceonArtificialIntelligence,volume33,pages6423–6430,2019.[13]M.Gaur,K.Gunaratna,V.Srinivasan,andH.Jin.Iseeq:Informationseekingquestiongenerationusingdynamicmeta-informationretrievalandknowledgegraphs.InProceedingsoftheAAAIConferenceonArtificialIntelligence,volume36,pages10672–10680,2022.[14]Q.Guo,C.Kulkarni,A.Kittur,J.P.Bigham,andE.Brunskill.Questimator:Generatingknowl-edgeassessmentsforarbitrarytopics.InIJCAI-16:ProceedingsoftheAAAITwenty-FifthInternationalJointConferenceonArtificialIntelligence,2016.[15]D.Jin,E.Pan,N.Oufattole,W.-H.Weng,H.Fang,andP.Szolovits.Whatdiseasedoesthispa-tienthave?alarge-scaleopendomainquestionansweringdatasetfrommedicalexams.AppliedSciences,11(14):6421,2021.[16]V.Karpukhin,B.Og￿uz,S.Min,P.Lewis,L.Wu,S.Edunov,D.Chen,andW.-t.Yih.Densepassageretrievalforopen-domainquestionanswering.arXivpreprintarXiv:2004.04906,2020.[17]D.Khashabi,S.Chaturvedi,M.Roth,S.Upadhyay,andD.Roth.Lookingbeyondthesurface:Achallengesetforreadingcomprehensionovermultiplesentences.InProceedingsofthe2018ConferenceoftheNorthAmericanChapteroftheAssociationforComputationalLinguistics:HumanLanguageTechnologies,Volume1(LongPapers),pages252–262,2018.[18]O.Khattab,C.Potts,andM.Zaharia.Relevance-guidedsupervisionforopenqawithcolbert.TransactionsoftheAssociationforComputationalLinguistics,9:929–944,2021.[19]O.KhattabandM.Zaharia.Colbert:Efficientandeffectivepassagesearchviacontextualizedlateinteractionoverbert.InProceedingsofthe43rdInternationalACMSIGIRconferenceonresearchanddevelopmentinInformationRetrieval,pages39–48,2020.[20]G.Kumar,R.E.Banchs,andL.F.D’Haro.Revup:Automaticgap-fillquestiongenerationfromeducationaltexts.InProceedingsoftheTenthWorkshoponInnovativeUseofNLPforBuildingEducationalApplications,pages154–161,2015.[21]G.Lai,Q.Xie,H.Liu,Y.Yang,andE.Hovy.Race:Large-scalereadingcomprehensiondatasetfromexaminations.arXivpreprintarXiv:1704.04683,2017.[22]G.LeBerre,C.Cerisara,P.Langlais,andG.Lapalme.Unsupervisedmultiple-choicequestiongenerationforout-of-domainq&afine-tuning.InProceedingsofthe60thAnnualMeetingoftheAssociationforComputationalLinguistics(Volume2:ShortPapers),pages732–738,2022.[23]K.Lee,M.-W.Chang,andK.Toutanova.Latentretrievalforweaklysupervisedopendomainquestionanswering.arXivpreprintarXiv:1906.00300,2019.[24]C.Liang,X.Yang,N.Dave,D.Wham,B.Pursel,andC.L.Giles.Distractorgenerationformultiplechoicequestionsusinglearningtorank.InProceedingsofthethirteenthworkshoponinnovativeuseofNLPforbuildingeducationalapplications,pages284–290,2018.[25]C.Liang,X.Yang,D.Wham,B.Pursel,R.Passonneaur,andC.L.Giles.Distractorgenera-tionwithgenerativeadversarialnetsforautomaticallycreatingfill-in-the-blankquestions.InProceedingsoftheKnowledgeCaptureConference,pages1–4,2017.[26]V.Liévin,C.E.Hother,andO.Winther.Canlargelanguagemodelsreasonaboutmedicalquestions?arXivpreprintarXiv:2207.08143,2022.[27]Y.Mao,P.He,X.Liu,Y.Shen,J.Gao,J.Han,andW.Chen.Generation-augmentedretrievalforopen-domainquestionanswering.arXivpreprintarXiv:2009.08553,2020.[28]Y.Nie,S.Wang,andM.Bansal.Revealingtheimportanceofsemanticretrievalformachinereadingatscale.arXivpreprintarXiv:1909.08041,2019.[29]H.-Y.Peng,H.-L.Chung,Y.-H.Chan,andY.-C.Fan.Misleadinginferencegenerationviaproxi-malpolicyoptimization.InPacific-AsiaConferenceonKnowledgeDiscoveryandDataMining,pages497–509.Springer,2022.[30]L.Qin,A.Bosselut,A.Holtzman,C.Bhagavatula,E.Clark,andY.Choi.Counterfactualstoryreasoningandgeneration.arXivpreprintarXiv:1909.04076,2019.[31]P.Rajpurkar,R.Jia,andP.Liang.Knowwhatyoudon’tknow:Unanswerablequestionsforsquad.arXivpreprintarXiv:1806.03822,2018.[32]P.Rajpurkar,J.Zhang,K.Lopyrev,andP.Liang.Squad:100,000+questionsformachinecomprehensionoftext.arXivpreprintarXiv:1606.05250,2016.[33]S.Ren,Y.Deng,K.He,andW.Che.Generatingnaturallanguageadversarialexamplesthroughprobabilityweightedwordsaliency.InProceedingsofthe57thannualmeetingoftheassociationforcomputationallinguistics,pages1085–1097,2019.[34]S.RenandK.Q.Zhu.Knowledge-drivendistractorgenerationforcloze-stylemultiplechoicequestions.InProceedingsoftheAAAIConferenceonArtificialIntelligence,volume35,pages[35]Y.Su,T.Lan,Y.Wang,D.Yogatama,L.Kong,andN.Collier.Acontrastiveframeworkforneuraltextgeneration.arXivpreprintarXiv:2202.06417,2022.[36]A.Talmor,J.Herzig,N.Lourie,andJ.Berant.Commonsenseqa:Aquestionansweringchallengetargetingcommonsenseknowledge.arXivpreprintarXiv:1811.00937,2018.[37]J.Welbl,N.F.Liu,andM.Gardner.Crowdsourcingmultiplechoicesciencequestions.arXivpreprintarXiv:1707.06209,2017.[38]J.Welbl,P.Stenetorp,andS.Riedel.Constructingdatasetsformulti-hopreadingcomprehensionacrossdocuments.TransactionsoftheAssociationforComputationalLinguistics,6:287–302,[39]T.Wu,M.T.Ribeiro,J.Heer,andD.S.Weld.Polyjuice:Automated,general-purposecounter-factualgeneration.arXivpreprintarXiv:2101.00288,1(2),2021.[40]J.Xie,N.Peng,Y.Cai,T.Wang,andQ.Huang.Diversedistractorgenerationforconstruct-inghigh-qualitymultiplechoicequestions.IEEE/ACMTransactionsonAudio,Speech,andLanguageProcessing,30:280–291,2021.[41]Z.Yang,P.Qi,S.Zhang,Y.Bengio,W.W.Cohen,R.Salakhutdinov,andC.D.Manning.Hotpotqa:Adatasetfordiverse,explainablemulti-hopquestionanswering.arXiv:1809.09600,2018.[42]M.Yasunaga,A.Bosselut,H.Ren,X.Zhang,C.D.Manning,P.Liang,andJ.Leskovec.Deepbidirectionallanguage-knowledgegraphpretraining.arXivpreprintarXiv:2210.09338,2022.[43]C.Y.Yeung,J.S.Lee,andB.K.Tsou.Difficulty-awaredistractorgenerationforgap-fillitems.InProceedingsoftheThe17thAnnualWorkshopoftheAustralasianLanguageTechnologyAssociation,pages159–164,2019.[44]C.Zhang,Y.Sun,H.Chen,andJ.Wang.Generatingadequatedistractorsformultiple-choicequestions.arXivpreprintarXiv:2010.12658,2020.[45]X.Zhou,S.Luo,andY.Wu.Co-attentionhierarchicalnetwork:Generatingcoherentlongdistractorsforreadingcomprehension.InProceedingsoftheAAAIConferenceonArtificialIntelligence,volume34,pages9725–9732,2020.[46]B.Zou,P.Li,L.Pan,andA.Aw.Automatictrue/falsequestiongenerationforeducationalpur-pose.InProceedingsofthe17thWorkshoponInnovativeUseofNLPforBuildingEducationalApplications(BEA2022),pages61–70,2022.於本研究計畫中，我們目標基於自然語言生成技術進行選擇題自動出題研究。我們預期的展示情境為：給定一知識範圍(如農業文集)，我們可於系統直接輸入一組主題關鍵詞，而後自動生成關於該主題相關之選擇題題組(含字詞等級、句子等級、挑選是非類型之選擇題)。然而，如同前言背景中所言，現有技術離目標仍有許多待強化之處，對此我們擬定三大出題目標：字詞等級干擾選項、句子等級干擾選項與反事實干擾選項進行研究。針對此三大目標，我們擬定(1)基於知識圖譜之強化與(2)基於生成語言模型與對比學習之強化三個核心技術進行探討。研究技術項目與項目間之關係與進行規劃如Figure7所示。於第一年度中，我們規劃「考量題幹可用性與干擾選項間關聯之Retrieve-and-Generate架構優化」、「基於語言模型與知識圖譜之Text2Text干擾選項生成」與「基於Masked-LM之反事實干擾選項生成研究」進行研究。「考量題幹可用性與干擾選項間關聯之Retrieve-and-Generate架構優化」著眼於優化目前https://mcq-demo.nlpnchu.org之WordLevel之選擇題生成系統。希冀自動過濾相似或語意不正確題幹，並且考量干擾選項彼此間之關聯進行最終干擾選項集合之綜合挑選。「基於生成語言模型之Text2Text干擾選項生成模型」則考量Text2Text模式利用生成模型直接生成干擾選項序列，藉此嘗試生成句子等級之干擾選項。並進一步地探索結合知識圖譜與語言模型之干擾選項生成，希冀透過知識圖譜之整合來生成更有誘導性且合理之字詞等級干擾選項。此外，於「基於Masked-LM之反事實干擾選項生成研究」項目中，我們預計著手挑戰反事實干擾選項生成，我們嘗試利用透過NER擷取給定事實陳述句中之關鍵字並Masked-LM該關鍵字來填入機率較低之字詞，以為反事實語句生成之作法。我們預期第一年度工作完成後，將可優化現有單詞等級干擾選項之選擇題自動生成系統，並於第二年度中，我們則規劃「基於知識圖譜強化之反事實干擾選項生成」、「基於對比式學習之干擾選項生成模型強化」與「利用Open-DomainQuestionAnswering錯誤結果之干擾選項挑選」。於此年度中，進一步嘗試利用知識圖譜萃取三元組知識後進行反事實語句之生成，基本想法為透過萃取三元組後再進一步輸入語言模型，並希望語言模型適當地進行反事實語句的改寫。此一部分預計使用第一年度之「基於Masked-LM之反事實干擾選項生成研究」之基礎以為訓練資料。對於字詞等級的干擾選項，我們則探索利用Open-DomainQuestionAnswering模型所錯誤回報之答案當作干擾選項。我們希望透過此不同之想法來取得更多樣的干擾選項。再者，如前言所述，生成模型生成序列時常出現生成結果重複的狀況，對此我們嘗試利用對比式來改善生成模型這方面的缺點，以優化干擾選項之生成。第二年度工作項目完成後，我們可初步提供反事實干擾選項之選擇題生成、優化後之句子等級干擾選項生成與基於ODQA方式取得之字詞等級干擾選項生成。於第三年度中，我們則規劃「基於NodeCentrality考量主題涵蓋性之選擇題題組生成」與「生成難度可控之干擾選項生成研究」。如同前言所述，一個理想的選擇題出題系統應該考慮題組生成，整體題目之主題涵蓋性，對此我們也將嘗試基於知識圖譜涵蓋的方式考慮節點centrality重要度方式來進行題組之生成。同時我們也將嘗試題目難易度之控制生成研究。我們的想法是當自動生成成為可能後未來的出題應該是個人化模式；針對個人的學習狀況進行測驗，因此我們規劃「個人化與生成難度可控之干擾選項生成研究」進行探討。第三年度，我們也預計投入人力進行系統的實作開發與整合。以下茲針對擬訂之研究議題其問題挑戰與1.基於語言模型與知識圖譜之Text2Text干擾選項生成模型近年來，語言模型除了使用CandicateGeneratingandRanking架構的MaskedLM之外，使用Encoder-Decoder架構的Text2Text模型於生成任務上也有很好的成果，因此我們的想法是把干擾選項生成Model視為一個Text2Text任務，嘗試讓模型直接生成干擾選項。且使用Text2Text模型生成Distractor有別於用MaskedLM生成Distractor，我們希望模型能生成的Distractor有以下特性(1)非單詞之Distractor:目前在Distractor生成上多為單詞生成，要生成一段敘述/句子的干擾選項還沒有很好的做法。(2)特定domain之MCQ:目前的MCQ多為英文測驗，若想針對特定領域出選擇題，使用MaskedLM生成的Distractor會因語料庫限制而為達到上述的兩種目標需求，我們設Q為輸入之問句，A為輸入問句之答案，並將Q與A字串接字串當作輸入，訓練模型輸出一連串問句Q的干擾選項字串D=(d1,d2,d3)。模型架構如圖2所示，我們將「哪一種技術最適用於嚴重特殊傳染性肺炎之病原快速篩檢?」與「聚合￿連鎖反應」接在一起輸入至Text2Text模型，而模型生成了「重組DNA技術」、「電腦斷層掃描」與「組織培養」三個干擾選項，三個選項皆為醫療檢測方式且都不是單詞，符合如前言所述，我們也預計探索使用知識圖譜來強化生成干擾選項的能力。如Figure8的圖例，我們可以使用語言模型結合醫療知識圖譜，利用給定的文本從知識圖譜中萃取有包含這些topicentities以及相鄰的nodeentities的子圖，再分別將文本與子圖輸入至語言模型和GraphNeuralNetwork(GNN)，讓模型學習找出關鍵的資訊與強化network中重要entitynodes跟edges的權重，藉由GNN的額外資訊與語言模型能力生成更好的選項。Figure8為我們規劃做法的概觀。主要分兩階段進行：(1)關鍵字子圖與GNNEmbedding萃取與(2)GNNEmbedding與LanugageEmbedding間之FusionEncoder生成。具體而言，第一個階段是將從知識圖譜裡萃取相關的知識圖譜子圖。我們一開始先使用NERModel抓取題幹中的entities，再透過這些entities透過知識圖譜去尋找entitynodes以及所有跟entitynodes相鄰3-hop內的bridgenodes，將檢索出來的結果當成知識圖譜子圖，並在後續第二階段提供以Figure9的圖例，我們先將問題透過NERModel斷詞得到”白血病”、”發病”、”器官”與”發病原因”當作entities，再透過這些entities當作關鍵字透過Retriever從知識圖譜萃取Figure8:結合知識圖譜與語言模型生成之構想架構圖與這些詞相關的node和edge，如利用”白血病”與”器官”能找到”肝臟”、”骨髓”、”肺部”等等entities，我們能利用這些entities來當作後續的生成選項。第二個階段則將文本與第一階段得到的知識圖譜子圖分別輸入至語言模型與GNN進行訓練，訓練目標為將Input的[MASK]Token還原來產生干擾選項，先將語言模型的Input額外新增[MASK]Token，經過N層LMLayer做self-attention後，透過FusionEncoder再結合知識圖譜的資訊與原本的文本去生成選項，透過以上的手法，來強化干擾選項之生成。2.考量題幹可用性與干擾選項間關聯之Retrieve-and-Generate架構優化如同研究目的所述，本研究項目著眼於優化現有Retrieve-and-Generate生成架構。我們•文章檢索器過濾:在Retrieving-and-Generating方法中Retriever會檢索回傳含有主題字詞的文章，但有部分文章並不適合當作問句生成模型的輸入，那些文章含有太多的雜訊，我們想嘗試在這步驟先過濾一些文章。我們的想法是利用詞向量中的相似詞再次檢索Retriever回傳的文章，把過於不相關的文章濾掉，減少問句生成模型輸出過於複•生成題幹品質自動過濾：問句生成模型輸出的問句，會再交由使用者挑選，但為了更好的效果，我們想嘗試利用語言模型做初步的篩選，減少人工挑選時間。我們想嘗試利用ODQA模型來過濾問句，將問句生成模型輸出的問句輸入至問答模型後，根據問答模型的回答結果與信心分數來決定該問句是否合適當作MCQ。如Figure10所例，我們將生成之問句再次投入ODQA模型，透過答案選項之信心度來為問句生成過濾之基Figure10:Unanswerablequestionsolution•考量干擾選項間關聯之干擾選項挑選:如前言所述，一個可以提升選擇題生成品質￿考量干擾選項彼此之間之難易度。而在Retrieving-and-Generating方法中，生成干擾選項的方式為取出詞向量模型中與主題字詞相似之詞，使用相似度前三名當作干擾選項，因此有選項過於相近而造成問題太難的可能。為做改善，我們取出所有相似詞的向量與主題字詞向量計算相對距離，設定各個選項與答案的相似度範圍(距離範圍)，再從中隨機挑選相似詞，讓干擾選項彼此之間難易度有所區分以提升試題之鑑別度。•基於語言模型之干擾選項候選生成:雖然詞向量模型生成distractor過程迅速，但如果有主題字詞沒有被詞向量模型看過或有主題字詞一字多意，就無法生成適當的干擾選項。這個問題我們想嘗試使用MaskedLM來解決，我們將Retriever回傳的文章中含有主題字詞的段落記錄下來，移除主題字詞後輸入MaskedLM，再從MaskedLM輸出的候選字中選出distractor，使用這個方式生成的干擾選項不會被上述的缺點受限，也更Figure11:OptimizedRetrievingandGeneratingFramework3.基於Masked-LM之反事實干擾選項生成研究本研究項目為基於Masked-LM技巧來生成反事實干擾選項。我們需要先查找與事實相符之敘述，再修改成反事實干擾選項。我們預計之作法為：首先讓使用者先輸入考題關鍵字，再使用retriever查找相關文章，其目的為從相關文章中選取與事實相符之敘述。接著，將retriever檢索到的文章回傳給使用者，由使用者選取與事實相符之敘述。並按照以下之步驟進行Masked-LM反事實干擾選項生成。為了抽換專有名詞，我們先使用命名實體與事實相符之敘述中的所有專有名詞，以下圖為例”腎小管藉壓力差方式再吸收葡萄糖回微血管”，透過命名實體模型會萃取出專透過命名實體模型，我們會萃取出多個與事實相符之敘述中的專有名詞，我們需要選擇一個專有名詞行抽換。我們認為該專有名詞在相關文章中的重要程度越高，越高機率是考題中的重點。為了確保抽換專有名詞的品質，我們使用BM25來查看各個專有Figure12:AutoEncoder模型與AutoRegressive模型做MaskLM3.使用Maskedlanguagemodel模型進行改寫Maskedlanguagemodel，簡稱為MaskLM，該任務的會將輸入中的部分內容遮擋起來，模型會根據其他未被遮擋的內容，預測出被遮擋的內容或與其擁有相似語義的內容，現有的MaskLM模型分為兩種，分別為AutoEncoder框架的MaskLM模型與AutoRe-gressive框架的MaskLM模型，經典模型如BERT與BART。針對MaskLM任務，兩種模型進行預測的方式也有所不同，AutoEncoder框架的模型會預測出字，而AutoRe-gressive框架的模型會預測出字、詞或是句子，詳細範例如Figure12。根據抽換專有名詞的需求，我們將使用Autoregressive模型做MaskLM任務來抽換重要的專有名詞。然而模型預測出的內容眾多，有可能是原先遮擋的內容、遮擋內容的同義詞或是與遮擋內容擁有相似語義的內容。理想的情況下，模型預測的內容最高機率為原先遮擋的內容，若我們選擇原先遮擋的內容或是遮擋內容的同義詞進行抽換，會造成與事實相符之敘述未改變，便無法生成反事實干擾選項。為了避免此狀況發生，我們使用NLI任務中的換句話說(MRPC)任務來檢測其是否為干擾選項。換句話說任務主要在判斷兩句話的語意是否相同，為了避免MaskLM模型選用同義詞進行抽換，即模型生成的干擾選項仍是正確的，我們將與事實相符之敘述與生成之干擾選項作為模型的輸入，若模型認為兩句話具有相同語義，我們會讓MaskLM模型預4.基於知識圖譜強化之反事實干擾選項生成於本研究項目中，我們嘗試利用知識圖譜提供語言模型額外資訊達成反事實干擾選項生成。我們的構想如Figure4所示，包含以下步驟。首先我們會讓使用者先輸入考題內文，再將內文使用斷詞或NER模型萃取entity，利用這些entity於知識圖譜中查找相關子圖，子圖必須包含內文中所有的entitynodes以及所有跟entitynodes相鄰節點，以此為知識生成之基礎。接著我們將檢索到的子圖內的nodes與edges萃取成三元組，每筆三元組的結構是以node,edge,node組成。如同圖中所示，每一筆三元組描述一基本事實陳述，如導體可以3.將與事實相符之敘述修改為反事實干擾選項為了將與事實相符之敘述更改為反事實干擾選項，我們將前一步驟所萃取的三元組與考題內文當作作語言模型的輸入來生成反事實干擾選項。我們預計透過生成語言模型與輸入給定的問句和三元組來生成與問句情境相符合的反事實干擾選項，以FigurejQuery36306984126397040076_1676210426716為例，生成語言模型在給定關於與導體相關的題目敘述以及從知識圖譜萃取與導體相關的三元組後，模型藉由這些三元組推斷其中的關係，比如”導體可以導電->石墨是導體->筆芯是由石墨製成”，從上述的三元組推斷出”筆芯可以導電”的事實，也可以改寫成”筆芯不能導電”的反事實干擾選項。5.使用Open-DomainQuestionAnswering錯誤結果之干擾選項挑選研究如前言所述，我們預計使用ODQA模型所回答之錯誤結果來當作干擾選項(如Figure5)。以我們先前曾開發https://agri.nlpnchu.org/之農業問答系統為例，我們發現以上想法所面臨的挑於ODQA的架構下，可能會遇到ODQA正確回答所下達之問題，其回傳結果皆為可行之答案，也因此無法被使用來當干擾選項，對此，我們探索兩種作法來進行調整。–1.使用BM25，且刪除尋找出來文章的正確答案在使用BM25的情境下，因為是使用關鍵字比對，所以會找出許多跟正確答案相關的文章，我們將找到文章的正確答案刪除，這樣這個文章會變成絕對回答不出正確答案的文章，可是因為Reader對文字有一定的理解能力，並且具有關鍵字的文章中也很可能會有，他會從剩下可能的答案中，尋找一個最相近的選項，這樣就可以較容易生出干擾選項。如Figure13所示，我們將答案”2,4-D”從文章中直接刪除後，再交給Reader進行閱讀，藉此確保正確答案不會被回傳。Figure13:基於ODQA架構之Retriever弱化方法–2.使用DensePassageRetrieverDensePassageRetriever[16]，DensePassageRetriever可以將資料庫的文章轉為向量的方式表示，在搜尋的時候也將Query轉為向量，並且比對兩個向量的相似度，這樣就可以找出相似的文章，可是我們想說，可以利用DPR找尋文章相似度的這一點，理論上相似的文章跟Query算出來的相似度會是最高的，我們可以將相似度高的文章拿出來，並且查看有沒有答案出現在上面，如果沒有答案，我們就將他丟給Reader，因為我們覺得如我相似度高，就代表Retriever認為裡面有答案，可是裡面並沒有含有我們的正解，所以這篇文章就含Retriever認為是答案，但是實際上不是答案的文字，所以這時候我們將這篇文章傳給Reader，就會有比較高在我們使用baseline的情境中，有時候也會出現不相關的答案，例如在??中，問模型關於殺草劑的問題，會出現的人力除草，人力除草跟題目所問的殺草劑名稱是不相關的，所以我們需要用一個方法把他過濾掉。在這種情況下，我們如果加入正確答案當作Query一起輸入Retriever，就會增加模型找到正確passage的機率，對我們想找跟正確答案不相關但是相似度較高的Passage會產生反效果，所以我們採用的是從Reader輸出之後把他過濾掉的方法。當Reader從Retriever拿到Passage之後，我們會將正確答案去除，之後再將剩餘的答案丟入一個LanguageModel作判斷，如果他跟答案的距離過於遙遠，就放入NegativeList，如果是相關的就放入PositiveList，之後要挑相關答案的時候可以從PositiveList裡面挑選。6.基於對比式學習之干擾選項生成模型強化ContrastiveLearning的設計上需要考量如何去設定PositiveSample以及NegativeSample。在我們的目標中，因為不希望模型會重複的生成出相同意思的句子，所以我們預計使用同義的句子當作NegativeSample，而資料集中的干擾選項可以被當作PositiveSample。然而，由於我們的資料集中並沒有NagativeSample的選項，也就是同義於模型生成的選項。對此，我們的想法為透過迭代生成來產生我們需要的NegativeSample。具體而言，如Figure14所舉例，在DistractorGenerator的第一次生成中，我們會輸入Question以及Answer使模型生成出干擾選項。而在第二次迭代中，我們除了會輸入QuestionAnswerPair外，還會再加上在上一次迭代中生成的干擾選項，以提示模型先前已生成的Distractor的資訊。對於第二次迭代的輸出來說，第一次迭代輸出的Distractor，即為它的NegativeSample，因為DistractorGenerator必須要生成出與上一次迭代生成的Distractor意義相異的句子，所以它們之間的SentenceEmbedding不能太過接近。Figure14:IterationGenerater示意，在第一次的迭代中生成的Distractor會作為第二次迭代的輸入，以提示模型不要生成出類似的DistractorFigure15:對比式學習於干擾選項生成之預想架構我們所規劃之整體框架，如Figure15所示。我們使用多次迭代的方式來生成Distractor，對於第一次迭代來說，我們會從其他in-batch的data中取得Sentence當作NegativeSample。而在第二次以後的迭代來說，除了其他in-batch的Sentence以外，我們還會將此次迭代前所生成的Distractor加入我們的NegativeSample中。透過NegativeSample和模型輸出的Distractor的dotproduct可以計算出ContrastiveLoss，此外我們也會加入Distractor和Label計算的CrossEntropyLoss，來計算出我們的TotalLoss15。ContrastiveLoss定義如下:x為生成的distractor，p為資料集中的Distractor(Label)，N為NegativeSampleSet，N={n1,n2,...nj}，其中包括先前迭代生成的Distractor以及其他in-batch的選項。7.基於NodeCentrality考量主題涵蓋性之選擇題題組生成如同前言所述，本研究議題之目標為考量主題涵蓋性之選擇題題組生成。因此，我們預計根據使用者所指定的主題，按Figure16架構規劃來進行題組生成。1.透過知識圖譜子圖萃取來涵蓋所指定主題之內容：藉由知識圖譜上NodeCentrality方式來萃取關鍵字，以為題幹之生成。我們的基本想法為在知識圖譜上我們定位出與使用者輸入之主題關鍵字相關之結點後，我們可以試者套入PageRank或者其他圖形上計算NodeCentrality重要節點挑選，並以重要節點為題幹生成之主軸。2.透過上一步取得之重要答案片段後，進行問題之生成。並生成較多的問題數量以為CandidateQuestionPool，而後將問題將題組生成問題設定一成為一選擇最佳化議題(Optimization)：根據主題生成大量的單一題目，而後定義UtilityScore來計算個別題目組合後之優劣，來挑選最佳化的題組以為輸出。於本研究項目中，我們預計探索以下兩種生成難度控制方式。•調整Masked-LM比例之反事實選項難度控制：現有的Masked-LM模型會將重要的專有名詞進行遮擋，模型會透過未遮擋的內容對該專有名詞進行預測。理想的情況下，當我們僅對該專有名詞進行遮擋時，模型可成功地預測出遮擋的專有名詞或語義相近的專有名詞，這不僅是因為MaskLM模型夠強大，也是因為未遮擋的內容提供模型足夠多的資訊讓模型預測，若未遮擋的內容提供的資訊不夠多，則模型預測出的內容有可能會與遮擋的內容語義相差很多。我們根據Masked-LM模型的此特性對題目的難易度進行控制，若我們將多個專有名詞進行遮擋，再讓模型預測時，模型會因為提供的資訊不足，而預測出語義較不相近的專有名詞，即較不可能的答案。以下表為例，我們可以觀察到當[Mask]越多時，我們要判斷該事實是否正確就越形容易。[Mask]借壓力差吸收[Mask]回微血管[Mask]借壓力差吸收[Mask]回[Mask]•調整ODQA檢索文章之難度控制：由於我們的基礎架構為基於文章檢索(DocumentRetrieving)來取得包含使用者所給定的關鍵字之文章，並使用該文章與關鍵字為問題生成模型之輸入，來生成選擇題問句。我們若挑選較為冷門之文章進行生成，則預期可以生成較為困難之題幹。而就干擾選項而言，我們可藉由調整輸入給閱讀理解模型之文章來產生難度較高的錯誤。具體而言，給定一有包含正確答案A之文章C，我們先將答案片段A於文章D中刪除，而後將其輸入至閱讀理解模型，藉此產生錯誤之閱讀理解結果，以這樣的錯誤結果當Distractor預期將有較高之誘導能力。相類似的想法也可以套用為挑選Retriever認為相關度比較不高的文章以為閱讀理解模型之輸入。此外，就生成題目之廣度與深度而言，我們可藉由控制取得之文章來調配題組之廣度與深度。例如，若我們以單篇或少數篇文章進行生成，我們所生成的主題題組針對測驗內容而言，應該可產生知識性較為深入之試題。而若針對較多篇文章，則應可產生範圍較廣本研究計畫將以三年為期，共八項研究議題進行，各研究產出如上文所示，並整合各技術成為一選擇題自動生成系統，預期的展示情境為：我們可於系統直接輸入一知識範圍，系統將利用我們所開發之技術先進行自動關鍵語萃取，而後進行各種類型的選擇題出題並考量題組關聯性。我們也將彙整各研究議題中所發展之技術為一個成為公開原始碼專案，以為後續相關研究之用。並將計畫執行過程所累積的資料，公開希望能吸引更多研究者之投入。對於學術研究、國家發展及其他應用方面預期之貢獻自然語言生成已是未來深度學習領域之熱門研究重點。透過本研究計畫之投入我們將可進行先驅之探索，建立學生團隊與自然語言處理與生成之能力，並持續過去之閱讀測驗自動出題研究成果，以知識類型選擇題自動出題為主要系統開發目標為例，希冀能為國內外探索不同的自然語言生成應用服務。此外若可順利完成也可以成為一成功範例利用人工智慧輔助教師進行測驗的客製化與個人化學習。對比於題庫類型的方式，透過我們技術我們將可動態產生各式各樣不同之考題為每個人量身定做難度適中之考題。就此一觀點出發，我們預期本計畫除學術研究價值外，同時也兼具商業價值，也應可成為EdTech(教育科技)領域之一環。再者，疫情改變了學習與教學的模式。同步與非同步之線上教學已成為教學模式常態之一。線上教學帶來了好處，但也帶來了顧慮。其中，學習的專注度應是諸多顧慮中主要令人擔憂的一環。隔著螢幕學生學習是否專注？目前的教學內容學生是否已跟上進度？等等問題是線上教學學習專注度所在意的問題。對此，我們認為適時與即時地檢測學生學習成效，來提醒學生與教師目前之學習成效，應能適度緩解線上學習專注度之顧慮。因此，我們也常見於MOOCs課程於一個授課單元結束後適時地給予測驗(多數以選擇題測驗)，來確認學生學習成效。然而，人工設計試題耗費心力，一成不變全班同學皆相同試題也可能透過即時訊息(instantmessage)之分享而取得答案，造成檢測之無效。對此，我們認為隨著深度學習技術之快速演進，結合自然語言(NaturalLanguageGeneration)生成技術來進行自動出題技術已成為可能；透過自然語言生成技術自動化出題，產生多樣化的試題，來進行個人化的檢測，將有除了可以增進研讀論文、撰寫程式、架設系統以及撰寫論文的基本能力之外，參與本計畫之研究人員，將可以學習到深度學習與自然語言生成技術之基本觀念、實際應用以及相關的核心技術，在這些基礎之下，透過積極的研究與討論，將可以提升參與計畫人員思考問題與解決困難的重要能力。我們預期可獲得具體技能如下:(1)設計與建立一個深度學習模型之能力，(2)整合不同先進技術之能力，(3)自然語言生成技術之瞭解，(4)資訊檢索相關知識與軟體套件之暸解，(5)佈建深度學習模型於WebScale之能力。"}, "1676210648.7068481": {"單位": "", "標題": "(ㄧ)、研究計畫之背景與目的", "作者": "", "發布日": 0, "摘要": "", "全文": "自然語言生成(NaturalLanguageGeneration)將是未來深度學習領域之熱門研究議題。2022年年末ChatGPT語言模型所帶來的示範性與展示效果預期將吸引更多研究目光。而個人於中興大學之研究團隊近年於自然語言生成之耕耘已建立初步研究成果。在過去朝著自然語言生成方向前進，我們探索自動生成試題之研究方向前進。於前期研究計畫「基於深度學習語言模型之閱讀測驗自動化生成研究109-2221-E-005-058-MY3」中，我們著眼於基於深度學習模型進行閱讀測驗(ReadingComprehensionTesting)題型之自動出題。在計畫執行過程中，我們•Ying-HongChan,Yao-ChungFan:KeywordProvisionQuestionGenerationforFacilitatingEducationalReadingComprehensionPreparation.INLG(InternationalNaturalLanguageGen-erationConference)2022.•Ho-LamChung,Ying-HongChan,Yao-ChungFan:ABERT-basedDistractorGenerationSchemewithMulti-taskingandNegativeAnswerTrainingStrategies.EMNLP(EmpiricalMethodsinNaturalLanguageProcessing)Findings2020.•Shang-HsuanChiang,Ssu-ChengWang,Yao-ChungFan:CDGP:AutomaticClozeDistractorGenerationbasedonPre-trainedLanguageModel.EMNLP(EmpiricalMethodsinNaturalLanguageProcessing)Findings2022.•Hsien-YungPeng,Ho-LamChung,Ying-HongChan,Yao-ChungFan:MisleadingInferenceGenerationviaProximalPolicyOptimization.PAKDD(AdvancesinKnowledgeDiscoveryandDataMining-26thPacific-AsiaConference)2022此外我們也將前期計畫之自動出題研究成果進行實作，可於以下兩個網址進行實測。•問句生成QuestionGeneration：給定文章、指定片段，透過自然語言生成模型進行問句生成。目前系統可於https://classic.queratorai.com/進行實測。•克漏字干擾選項生成DistractorGeneration：此一部分針對給定之文章進行克漏字誘導選項生成。目前系統可於：https://cdgp-demo.nlpnchu.org/進行實測。基於過去之研究成果累積，本計劃延續自然語言生成技術於自動出題之研究成果，擬定知識類型選擇題(MultipleChoiceQuestion,MCQ)自動生成為目標。以四技二專統一測驗考試為例(如Table2所示)，知識類型選擇題將包含題幹與四個選項(其中包含正確答案與干擾選項)，主要針對某一知識範圍進行測驗。我們認為知識範圍選擇題之自動生成可分為兩部分：(1)問題題幹(Stem)生成與(2)干擾選項(Distractor)生成。題幹生成主要目標為選擇題問題本身之生成，而干擾選項則著眼於誘導選項之生成。知識類型選擇題為學習測驗最常見的測驗模式之一。對比於閱讀測驗生成，知識類型選擇題的自動生成，將可減輕教師準備個人化教學測驗之負擔，也為學生提供更有效率之學習檢測方式。以現有我們成果為基礎，一個基本之自動選擇題出題系統作法為(如圖1)所示：我們可利用資訊檢索技術(如BM25Retriever)，從給定之文集資料庫(目前使用2021中文維基百科所有中文文章)中檢索主題字詞相關文章後，結合前期計畫所開發之問句生成模型(QuestionGeneration)[4]生成題幹，而後再利用詞向量模型(例如word2vec模型)找出相近字詞以為干擾選項。我們將這樣的構想先行實作一系統(稱之為Retrieve-and-GeneratePipeline)，目前此系統可於https://mcq-demo.nlpnchu.org/使用。使用者可藉由輸入主題相關字詞，選擇題生成系統將自動生成輸入字詞相關之選擇題結果。舉例而言，我們以「骨髓、白血病、淋巴」為主題字詞輸入至系統。我們挑選較佳之成果顯示於Table1。自動生成選擇題AutomaticGeneratedMCQQ1:哪一種癌症種類的統稱中可以看出它會造成不正常白血球的大量增生?(a)白血病、(b)乳腺癌、(c)腎病、(d)帕金森氏症Q1:白血病的發病原因通常是發生在哪一個器官上?(a)肝臟、(b)淋巴、(c)甲狀腺、(d)骨髓Q:白血病是最常見的兒童癌症，其中80%的病例是慢性的哪一種性病變?(a)淋巴結、(b)微血管、(c)淋巴、(d)黏膜Table1:目前選擇題自動生成結果範例。目前系統可於https://mcq-demo.nlpnchu.org/。目前我們挑選較佳之生成結果顯示於此Table中。建議審查委員自行輸入關鍵字詞組，來觀察我們Figure1:Retrieve-and-Generate基本架構。本例中我們以”骨髓、白血病、淋巴”為關鍵詞進行文章檢索，而再使用”骨髓”為答案，分別生成問句與干擾選項。從展示之結果中，我們發現自動選擇題出題之可能性。但仔細觀察會發現現有系統離實際應用仍有空間須待努力。也因此觸發本期研究計畫之提出。具體而言，我們認為選擇題自•句子等級之干擾選項生成(Sentence-LevelDistractorGeneration)首先，關於干擾選項的部分，若仰賴詞向量模型來產生干擾選項，我們所能取得的干擾選項便僅限於單詞(Word-Level)等級。從統測試題中，我們發現許多題目之干擾選項屬於句子等級(Sentence-Level)。以Table2中Q3與Q4選擇題為例，我們可以看到選項皆為句子等級之描述，此為目前基於詞向量來產生干擾選項的做法所不能。•反事實干擾選項生成(Counter-FactualSentenceGeneration)我們觀察到選擇題題型中，有著下列何者為錯誤或正確類型之考題。其干擾選項是一個較長的描述句。以Table2為例，Q5與Q6為分別詢問下列選項何者正確/錯誤。此題型，為變像之是非題。除了句子等級生成外，核心技術牽涉的是反事實(Counter-Factual)句子之生成。如何生成此類型之句子以為干擾選項也將是研究重點所在。•選擇題生成品質提升(MCQQuestionQualityImprovement)此外，我們也看到retreive-and-generatepipeline的做法，所產生的題幹本身有時是一個過於空泛之問題(例如，下列何種疾病為兒童常見之癌症？)或者並非語意正確之語句(例如，中樞性的什麼症狀是缺乏抗利尿激素的?)。如何有效判斷生成不良之語句，減低人工進一步過濾/挑選題目之負擔也是需進行研究之議題。此外，基於詞向量之干擾選項作法也有待商榷;隨機從相近詞向量挑選distractor，是否能確保distractor之誘導品類型I:單詞等級的干擾選項Word-LevelDistractorsQ1：淋巴器官是人體防禦系統中重要的一環，可提供淋巴細胞發育或活化所需要的條件與狀態。下列何者是淋巴球進行防禦與活化的器官？(a)骨髓(b)胸腺(c)肝臟(d)脾臟Q2：健康人體感覺神經傳入大腦的訊息大部分經過轉接，(a)視丘(b)下視丘(c)中腦(d)橋腦類型II:句子等級的干擾選項Sentence-LevelDistractors(a)無就巢性(b)體型大(c)著肉多(d)耐粗食Q4：下列何者為「聯合國國際糧農組織」(FAO)成立最重要的目標及首要任務？(a)消除及戰勝飢餓(b)聯絡及交流農業資訊(c)自然環境的永續及保護(d)實現國際間的公平貿易類型III:句子等級且與事實相違背的干擾選項Counter-FactualDistractorsQ5：有關人類上皮細胞的細胞週期之敘述，下列何者正確？(c)後期，分離的染色分體往細胞中央移動(d)末期，細胞膜內凹且細胞質開始一分為二Q6：有關一健康成人泌尿系統功能的敘述，下列何者錯誤？(a)腎臟每日約產生濾液180公升及尿液1.5公升(b)腎小管藉壓力差方式再吸收葡萄糖回微血管(c)腎臟藉排尿量多寡來維持體內水分的恆定(d)若出現尿毒症狀時，可使用血液透析方式進行治療Table2:四技二專統一測驗選擇題類型概觀。Q1與Q2擷取自110學年度護理基礎生物考科，Q3與Q4110學年度農業群農業概論考科，Q5與Q6取自110學年度農業群基礎生物考科質？也許所挑選的distractor並不具誘導性，使得受測者很輕易地可挑選出正確答案，或者挑選到答案的同義詞，造成一題有多個正確答案。根據先前計畫執行訪談教師所知，於測驗時為考量受測全體鑑別度，教師出題時會考量干擾選項彼此之間之難易度；通常三個干擾選項中，會有一個離正確答案較遠之干擾選項(誘導性較低)，一個次之，一個最為困難最具有誘導性。藉此來有適當的鑑別度。目前我們直接使用詞向量之作法並未考量此設計。•主題題組生成(MCQQuestionGroupGeneration)目前我們的系統僅為單一關鍵字詞之出題。我們認為一個良好的自動出題應該是以題組為單位進行出題；使用者指定一主題知識，系統根據此一主題知識範圍進行5-10題之選擇題自動生成，題目之間也應該彼此考量。目前Retreive-and-Generatepipeline的做法，我們使用使用者給定的關鍵字為答案，進行題目之生成。然而比較好的做法應為將使用者給定的關鍵字集合為知識範圍之指定，系統根據該範圍自動挑選重要概念關鍵字進行答案挑選，而後再進行選擇題題組之生成。針對上述之研究挑戰，於本研究規劃中我們擬定下列八項研究議題進行探討。•研究項目ㄧ：基於語言模型與知識圖譜之Text2Text干擾選項生成模型(DistractorGenerationbasedonText2TextLanguageModelandKnowledgeGraph)Figure2:Text2Text干擾選項生成架構Figure3:基於Masked-LM之反事實干擾選項現有之Distractor生成研究多是基於CandicateGenerating-and-Ranking架構，先透過知識庫(如Wordnet)或者透過MaskedLM的方式取得候選字後，再適當進行候選詞特徵排序以為最終干擾選項。對此我們有不同之想法，我們預計嘗試把干擾選項生成設定為一個Text2Text任務；如圖2所例，預期的輸入為題幹Q與答案A之串接字串，透過ModelEncoder與Decoder轉化輸出為一連串之干擾選項字串“重組DNA技術[SEP]電腦斷層掃描[SEP]組織培養”。我們透過語言生成模型直接將輸入對比轉換為輸出。此外，透過此一Text2Text方式，生成句子等級的干擾選項也將為可能。再者，近年來語言模型研究趨勢除了將模型參數量與訓練資料持續增加外，引入知識圖譜來強化生成模型於常識理解與推理能力也成為研究社群的關注所在[5][13]。也因此，探索整合知識圖譜於選擇題自動生成也因此成為我們規劃之研究項目。希冀藉由整合知識圖譜來提升干擾選項之誘導性與題幹生成之正確性。•研究項目二：基於Masked-LM之反事實干擾選項生成研究(Counter-FactualSentenceGenerationbasedonMasked-LM)如同前言所述，“下列何者正確/錯誤”題目為常見之類型選擇題題型。對此，我們預計嘗試以下之作法。首先，我們假定可取得正確敘述之語句S。我們先針對語句S中的字詞進行命名實體辨識(NamedEntityRecognition,NER)，其後再利用語言模型針對所取得之命名實體進行WholeWordMasking(WWM)，填入機率較低之填空結果，藉此生成反事實描述之語句。如Figure4所例，我們將所給定之事實描述，藉由NER方式挑選”葡萄糖”一次進行Masked，再藉由語言模型推測該Masked字詞，並挑選機率相對較低之預測填入。藉此創造出反事實之陳述句。然而，如何確保不挑選到同義詞？要進行多少個字詞之Masked？如何自動從主題文章中挑選事實陳述句以進行改寫？等等•研究項目三：基於知識圖譜強化之反事實干擾選項生成(Counter-FactualSentenceGen-erationbasedonKnowledge-GraphAugmentedLanguageModel)針對反事實干擾選項生成之議題，除了研究項目二之Masked-LM作法外，我們另一個構想為利用知識圖譜來強化Counter-Factual語句之生成。如Figure4所示，我們基本想法為先萃取與題幹相關聯之知識圖譜子圖(LocalSubgraph)並取得知識三元組，以此輸入至語言模型中，訓練語言模型反向改寫三元組語句，以此為反事實干擾選項。然而，此模式之訓練資料該如何取得？如何從知識圖譜上萃取子圖，並生成合適三元組？如何引入GraphNeuralNetwork來萃取結構？為此研究項目需進行探討之工作內容。•研究項目四：基於對比式學習之干擾選項生成模型強化(DistractorGenerationModelbasedonContrastiveLearning)我們過去的研究經驗[8]與相關研究[35]皆指出當使用Generative語言模型來生成句子Figure4:基於知識圖譜強化之反事實干擾選項生成構想層級之輸出時，模型經常會產生語意之相同，但字面的結果。如Table3所示，選項A跟C雖然字面敘述上不同，但實際上都是描述同樣的意思。生成重複選項將導致選擇副交感神經興奮會導致使心跳加速，血壓上升Table3:模型生成的誤導選項重複出現的範例對此，我們預計探索對比式學習(ContrastiveLearning)[7][35]來提升生成品質。對比式學習於自然語言之應用中主要想法為透過選取PositiveSample以及NegativSample使得模型去學習到生成句子跟同義句子的相同以及跟異類句子的不同之處，這樣我們可以使用重複選項當作我們的NegativeSample，然後資料集中的干擾選項當作PostiveSample，期望模型可以學習避免生成出重複的誤導選項。因此，我們預計探索對比式•研究項目五：使用Open-DomainQuestionAnswering錯誤結果之干擾選項挑選研究(Us-ingIncorrectAnswersfromODQAasDistractors)而除了利用生成模型產生Distractors外，我們發現利用Open-DomainQuestionAnswer-ing(ODQA)模型所產生的錯誤結果是一個值得探討的干擾選項的取得方式。具體而言，現有的ODQA模型主要包含一個Retriever與一個Reader，Retriever負責檢索相關的文章，而Reader利用檢索出來的文章利用閱讀理解模型(ReadingComprehensionModel)來找尋答案片段。由於ODQA並非完美，因此也會有錯誤結果之回報。但我們發現錯誤結果與正確答案並非天差地遠，閱讀理解模型所回報之結果通常也與答案接以圖5為例，當我們下達Q：「下列何者為目前世界上應用最廣的殺草劑，其具備有生長素之特性，常用於闊葉雜草防治，亦為選擇性殺草劑的一種？」，ODQA系統將取得圖5中右半部之答案：「萌前除草劑、2,4-D、非選擇性除草劑、10殺丹粒劑」。其中，2,4-D為正確答案，餘者為錯誤答案。然而我們覺得這些錯誤答案，可以當作Q之干擾選項來使用。有別於基於生成模型研究項目，利用ODQA模型之錯誤答案來當作干擾選項應可提供不同思路進行干擾選項生成。這樣的想法乍看可行，但仍有若干技術點需進行探討。諸如，如何控制ODQA模型之錯誤？我們需要在Reader部分弱化Reader的能力，或者弱化Retriever能力，來製造ODQA的錯誤？如何從錯誤回報結果挑選干擾選項？這些皆是尚待深入研究之議題。也因此觸發本研究項目之規劃。Figure5:利用ODQA模型之錯誤答案來當作Distractors•研究項目六：基於NodeCentrality考量主題涵蓋性之選擇題題組生成(MCQGroupGenerationconsideringTopicCoveragebasedGraphNodeCentrality)上述所規劃的選擇題生成做法，主要針對單題進行生成。若要產生多題/題組考題，就是重複相同流程，獨立產生個別考題後，整合為一題組。然而，題組生成做法應當考慮題與題之間之關聯性與互補性。因此，我們認為比較好的做法應當為根據使用者所指定的主題，透過(1)知識圖譜子圖萃取來涵蓋所指定主題之內容與(2)透過主題重要概念萃取：藉由知識圖譜上NodeCentrality方式來萃取關鍵字，以為題幹之生成。此外，如何根據主題互補性，來挑選個別生成之題組，用以組成選擇題題組也是需進行•研究項目七：考量題幹可用性與干擾選項間關聯之Retrieve-and-Generate架構優化(Op-timizingRetrieve-and-GenerateFrameworkconsideringStemUtilityandDistractorCorrela-tion)本研究項目著眼於優化現有Retrieve-and-Generate架構設定。Retrieve-and-Generate架構雖然簡單，但我們可以發現其於單字等級的干擾選項生成已有一定的可用性。我們預計可優化之處為：(1)我們發現引用現有系統於題幹生成時，會有題幹生成結果不佳的狀況。對此，我們發現其中的一個原因為Retriever所檢索之文章品質不佳(文章過短或者與主題不相關)，導致不良之題幹生成。因此如何改善檢索機制來挑選較佳的文章用以進行生成，便是值得探討的課題。再者，題幹生成不佳也有部分原因是來自於問句生成模型本身之生成錯誤，如何有效過濾生成不佳的題幹也因此是一個可需進行研究之項目。(2)如同前言所述，干擾選項之挑選應該考量彼此間之關聯，而非獨立挑選。如何納入此觀點來進行干擾選項之生成也因此成為預計關注之議題。又再者，我們基於word2vec方式之字詞等級干擾選項挑選是否可以引入語言模型之詞向量來求取•研究項目八：生成難度可控之選擇題生成研究Difficulty-ControlledMCQGeneration發展自動化生成考題之可能願景為：針對個人化學習自對應生成難度適中之測驗，以增加學習與檢測之效率。因此，如何針對現有技術探討生成結果難度控制也因此成為一可探索之研究議題。由於我們的基礎架構為基於文章檢索來取得包含使用者所給定的關鍵字之文章，並使用該文章與關鍵字為問題生成模型之輸入，來生成選擇題問句。我們若挑選較為冷門之文章進行生成，則預期可以生成較為困難之題幹。而就干擾選項而言，若我們基於研究項目五「使用Open-DomainQuestionAnswering錯誤結果之干擾選項挑選研究」，利用ODQA來取得干擾選項之作法。我們可藉由調整輸入給閱讀理解模型之文章來產生難度較高的錯誤。具體而言，給定一有包含正確答案A之文章C，我們先將答案片段A於文章D中刪除，而後將其輸入至閱讀理解模型，藉此產生錯誤之閱讀理解結果，以這樣的錯誤結果當干擾選項預期將有較高之誘導能力。相類似的想法也可以套用為挑選Retriever認為相關度比較不高的文章以為閱讀理解模型之輸入。此外，就生成題目之廣度與深度而言，我們可藉由控制取得之文章來調配題組之廣度與深度。例如，若我們以單篇或少數篇文章進行生成，我們所生成的主題題組針對測驗內容而言，應該可產生知識性較為深入之試題。而若針對較多篇文章，則應可產生範圍較廣之測驗試題。上述之研究議題，乍看可行但仍有許多細節需進行探索，如上述研究背景目的所述，本研究計畫擬探索基於現有Retrieve-and-GeneratePipeline之選擇題自動生成架構進行強化。我們以三年為期，規劃在此目標下擬定八項關鍵技術進行研究探討。如Figure6所示，八個工作項目又可區分為三項主軸：「選擇題生成品質提升」、「反事實選項生成」與「群組生成與難度控制」。其中，於「選擇題生成品質提升」研究主軸中，我們探索利用語言模型與知識圖譜架構來生成，將選擇題選項生成問題設定成為一Text2Text任務，輸入為問題題幹與正確答案所串接之TextSequence，而輸出為所生成之distractor串接之TextSequence。此外，我們也嘗試引入對比式學習來改善Text2Text生成模式之品質。我們也預期規劃探索利用ODQA方式來取得干擾選項。而於「反事實選項生成」研究主軸中，我們探索反事實陳述句之生成。我們預計嘗試基於MaskedLM取代重要名詞之反事實語句生成作法與基於知識圖譜知識三元組萃取改寫之作法。於第三個研究主軸中，我們則嘗試針對題組生成與難度可控制之干擾選項生成進行研究。MultipleChoice,Multi-此資料集特色在於為Multi-hop形式，答案需從複數文檔中推理而成；資料來自維基百科以及PubMed，PubMed為生物醫學論文的搜尋引擎，在兩個來源各自提供51,318(Wiki)和2,508(PubMed)筆問題。資料來自國高中英文考試，提供超過10萬筆問題以及2.8萬篇文本，題型上為英文閱讀測驗，會需要閱讀故事或對話紀錄，而針對該文本會有複數題目，每個題目此資料集的挑戰在於，內容來自於7個不同領域(新聞、小學自然科學、旅遊、小說故事等等)，為文本與題目提供多樣性，總計約1萬筆問題與800篇文本。MultipleChoice,Open-為Open-domain形式，並不提供文本，只有問題與答案；包含12,247筆問題、一個正確答案和四個干擾選項；題目類型為知識與常識題，來自於ConceptNet與同時包含兩種形式資料，填空題與Open-domain；填空題形式命名為QUASAR-S，其中資料來源自StackOverflow，提供約37K筆問題。Open-domain形式命名為QUASAR-T，提供約43K筆問題。來自StanfordUniversity的閱讀理解資料集，包含了超過100K筆問題和536篇對應的維基百科文本；延續了SQuAD1.1的題目外，[31]另外加入約50K筆無答案的題目，進一步測試機器能否在無對應答案時回答否，而MedQA是基於美國醫師執照考試(USMLE)所產生的資料集，其中問題皆為較為困難的專業醫學相關的知識，且包含了英文、簡體中文、繁體中文分別有約12K、34K、14K個問題集，這種專業的知識能使機器針對單一領域產生更具參考價值的distractor，目前此任務排名上最佳為ValentinLiévin團隊做出的Codex5-shot訓練機器學習模型進行閱讀理解(MachineReadingComprehensionTask)為最常見的深度學習自然語言任務。諸如，SQuAD[32]、RACE[21]、DROP[10]與HotPotQA[41]等資料集，皆是測驗/訓練機器學習模型於答案找尋、邏輯推理與摘要之能力。其中，許多機器閱讀理解任務採用多選題方式進行測驗。選擇題的自動答題與選擇題的自動生成為一體兩面之應用。因此具有選擇題樣態的資料集為我們選擇題自動生成研究之根本資料集，我們將目前所探知之相選擇題與干擾選項生成之相關研究可按應用類型可區隔為英文學習測驗與知識學習測驗。干擾選項生成研究於英文學習測驗應用，主要以自動生成閱讀理解與克漏字類型的考題為目標。閱讀理解之干擾選項生成與本研究計畫之知識類型選擇題之自動出題目標不同。因此本研究回顧中，我們不做此分類每一研究細部說明。我們將此類型之相關研究按不同象限做分類：(1)干擾選項層級之差異(生成字詞等級與句子等級之差異)象限，(2)題目類型(閱讀理解與克漏字測驗)，(3)生成方式(CandidateRanking與Generative)。我們按此三象限，將目前之相關研究摘要整理於Table5中。而知識學習測驗類型的選擇題生成，則於這一兩年逐漸吸引相關研究投入。我們認為與本研究計畫較為相關之研究有[22][40][3]，以下我們針對個別文獻討論如下。首先，與我們有相同目標者為[22]，也是針對特定知識主題生成選擇題。其主要作法Table5:現有閱讀測驗與克漏字干擾選項生成研究整理表為：先由人工給定一個主題，程式使用該主題名稱的維基百科頁面作為起始點(例如給定“體育”後搜尋體育相關維基百科)，保留熱門文章並取出文章的第一段擷取摘要，由此產生大量句子。並將這些句子依單純英文文法規則生成簡單5W問句與答案，例如who問句是將第一個動詞前的名詞作為正確答案並從句子中移除，what問句是將第一個夾在動詞間的名詞移除作為正確答案並從句子中移除，移除答案後的句子再帶入問題模板生成制式問句。生出五種問句後再依英文文法的詞與詞的相關性去決定哪個問句最適合。在干擾選項生成的部分，先隨機選取其他句子所產生的答案，再藉由語言模型去排序與問題的相關性，取前三高之預測為干擾選項。但本論文並未使用問句生成模型，採取之作法也與本計畫所規劃有所不同。於論文中[40]，則探討如何讓干擾選項更加多樣且與正確答案相差不遠(以具誘導性)。對此，創造了一款監督式分類任務來找出文章中屬於關鍵句子(及對產生干擾選項生成有用的句子)，作者在訓練之前會將訓練用的干擾選項和文章中的每個句子做比對，若該句子和干擾選項相似度達到一定程度，則該句子將成為文章中的關鍵句，透過這個方式在文章中能被修改成為新的干擾選項的句子將會達到多樣性的效果，且透過相似度對比，干擾選項也不會跟原始文章相差太遠從而提升干擾選項的品質。本研究之作法可供我們借鏡參考對關鍵文章/句選擇設計，用以提升干擾選項之多樣性。而與本計畫研究項目五「使用Open-DomainQuestionAnswer錯誤結果之干擾選項」走非生成想法相似之研究為[3]。其主要想法為作者發現於干擾選項資料集中，有許多干擾選項一再被使用。促使論文作者想為新的問題檢索以重複使用干擾選項。作者提出一個Transformer-baseddistractorretrievalmodels，該模型通過考慮問題的上下文來選擇干擾選項模型包含三個部分，分別為D-SIM模型、Q-SIM模型以及DQ-SIM模型。在D-SIM模型中使用mBERT計算同一個MCQ中問題和答案以及對應的干擾選項的向量，並計算他們之間的相似度。而在Q-SIM模型中作者假設不同問題共享一個或多個干擾選項或答案，代表問題可能在語意上相關，因此他們相關的干擾選項可以用作彼此的良好候選干擾選項，因此作者將訓練資料重新排序，使這些共享至少一個干擾選項或答案的問題聚集在一起，接著將資料送進Q-SIM模型計算不同的MCQ中問題之間的相似度。在DQ-SIM模型線性結合前兩個模型，從各自的優勢中獲益，透過基於分數和基於排名的合併策略計算來自D-SIM模型和Q-SIM模型的分數，並設計演算法使得高排名之干擾選項有較大的權重。重複使用現有資料集中之干擾選項為一創新想法，但仍與我們之ODQA做法有所不同。[OpenDomainQuestionAnswering相關研究]本計畫的選擇題生成架構主要基於兩元件:(1)文章萃取與(2)問句生成。目前我們所使用的文章萃取機制為傳統之BM25檢索。而近年來由於ODQA研究領域之發展，基於深度學習之DenseRetrieval技巧也紛紛於ODQA研究領域中所提出。我們預期DenseRetrieval技術也將於我們知識類型選擇題自動出題中擔任關鍵角色。茲簡要回顧現有ODQA之研究如下。ODQA架構中Retriver之重要性不言可喻。不同層級的語意檢索也影響著ODQA的效能。於[28]研究中，作者提出一個簡單而有效的pipeline系統，研究段落和句子級別語義檢索及其對下游任務的潛在影響，作法上是先檢索段落級別的段落，然後從上一步檢索後取得的段落中再去檢索句子級別的段落。該系統在事實驗證和Multi-hopQA方面進行評估，實驗結果顯示QA任務與事實驗證皆仰來上游檢索之結果。於[23]論文中，作者在ODQA架構的retreiver端提出了一個有效率的弱監督式學習的方法(InverseClozeTask)來對retreiver進行預訓練，給予模型完全無人工標記的維基百科原始資料，藉由論文中的方法將資料切分成固定長度的段落，並隨機選取其中的一個句子當作標籤，讓模型去學習如何從給定的句子去資料庫中找尋與之最相關的文章。在實驗數據中此方法訓練出的ODQA模型分數高於傳統的BM25檢索方法。而於DPR[16]論文中，作者為解決傳統BM25無法檢索語意相似段落的問題，提出了基於深度學習模型之DensePassageRetrieval，並提出使用in-batchnegative的技術，在密集向量的檢索取得極佳的效果改進。我們也預計引入中文DRP模型來提升目前題幹生成所需之而於GAR[27]研究中，作者們探討傳統基於BM25的檢索無法檢索沒有單詞重疊或語意相似的段落的問題，而基於密集向量的檢索方式在計算上又過於複雜，且會有資訊損失的問題。該論文透過預訓練語言模型自動生成與查詢語句相關的內容，可增加查詢語句的資訊量，隨後透過BM25進行檢索，該方法比起傳統BM25能檢索到更精確的段落，且執行效率也快於基於密集向量的檢索方式。透過GAR中的PLM可對查詢語句的資訊進行補充，原論文中是補充答案、相關段落及段落標題，可基於該手法讓PLM生成問題與原始答案的關聯，對這些關聯進行組合或刪除即可檢索與原答案有相似關係的查詢語句，可檢索到適合製作干於[19]，作者觀察到基於語言模型之檢索技術雖然效果很好，但比以往的方法需要極高的計算成本，特別是在計算每個query與document間的relevancescore時。本篇論文作者提出了ColBERT，引入lateinteraction架構，對query與document進行獨立的encoding，於模型輸出最終階段才進行相似度運算。而[18]中，則使用近一步基於ColBERT當基礎模型來進行retreiver端的訓練，使用了Relevance-guidedSupervision(RGS)的方法來訓練retreiver的模型。相較於傳統的retreiver訓練資料，傳統方法需人工標註的訓練資料，但作者認為人工標住資料往往存在偏見，進而影響模型的效能，所以在論文中提出了用訓練好的模型來找尋最適合的訓練資料，並把這些資料用於訓練下一個模型，使用這個方式來進行後續的訓練，最後得出最佳的模型。此方法訓練出的模型分數明顯高於目前opendomain的其他方式。在找尋訓練資料時，模型會先搜尋前k筆資料，並把前20筆資料中含有shortanswer的資料當成possitiveexample，而前20筆以外剩餘的資料中不含有shortanswer的資料當成negativeexample，此方法尋找的負面訓練資料交由reader進行預測能得到與正確答案相關的錯誤答案，如能改良ranking的方式，應用於干擾選項生成，預期將能提升干擾選項之品質。[ContrastiveLearning於自然語言生成相關研究]對比式學習始見於電腦視覺領域，其於影像應用上有著顯著的效果提升。因此，許多自然語言處理之研究者也將其引用至自然語言生成領域，諸如[11][7][35]。於[11]研究中，作者們提出了使用ContrastiveLearning的作法來學習SentenceEmbedding，他們使用BERT作為基礎模型，透過不同的dropoutmask使模型能用同樣的輸入產生不同的SentenceEmbedding，但它們之間的距離會是近的，也就可作為PositiveSample。然後再使用其他in-batch的句子作為NegativeSample，以達到非監督式訓練。而在有訓練資料的情況下，如NLI資料集，則可以將Eentailment視為PositiveSample，Contradiction以及其他in-batch的句子視為Negative而再基於[11]研究後，於[7]研究中，作者們提出加上了Discriminator來加強SentenceEmbedding的學習效果。具體而言，對每筆輸入來說，會透過SentenceEncoder產生一個SentenceEmbedding，同時也會隨機的Masking輸入句子其中的幾個Token，並透過一個fixed的Generator來還原MaskToken，而Discriminator要試著透過沒被修改過的SentenceEmbedding的資訊去辨認修改後的句子中的哪一個Token有被修改過，整個框架會透過SentenceEmbedding的ContrastiveLearningLoss以及Discriminator的ReplacedTokenDetection而與我們預計引入對比式學習來提升句子等級的干擾選項生成效果想法接近的研究也可見於[35]。該研究指出發現了以往autoregressive模型使用BeamSearch去決定要生成的文本時，會產生語意不自然以及沒意義的重複文字，儘管後續有人使用取樣以及修改ObjectiveFunction來嘗試解決這個問題，但這個方法時常會產生語意不連貫的問題。因此作者提出了使用ContrastiveLearningLoss加上原先的MaximumLikelihoodEstimationLoss來訓練模型，並且使用ContrastiveSearch來作為decodingmethod。[Counter-FactualSentenceGeneration相關研究]於Counter-FactualGeneration研究中，[44,33]為相關研究之先驅。主要想法是使用神經網路來生成反事實敘述，這個概念背後的想法是同時使用語言模型和各種採樣的策略來達成生成反事實敘述的效果。接續之前的成果，[39]提出了Polyjuice架構，可以控制輸出的反事實敘述的類型。另外一種生成干擾選項的研究是使用干擾選項生成(DistractorGeneration)的手法，干擾選項生成(DistractorGeneration)的目標是給予了一篇文章、問題和正確答案後，模型能夠生成似是而非的干擾選項。於研究[24]中，作者最先提出使用了神經網路來生成干擾選項，而研究[8]則延續了前者的成果，把重點放在生成複數的干擾選項並提出了一個負樣本學習策略(negativetrainingstrategy)來加強干擾選項生成。但這上述兩個研究主要都著眼於閱讀理解題型中干擾選項之生成，與本研究知識類型選擇題生成目標所有不同。而在關於AbductiveInferenceGeneration的相關研究中，於[30]中作者提出了故事改寫的任務，此任務目標是要改寫一段故事的敘述並且導致產生與原先不同的結局，與顯永的論文的方式的差異為故事改寫任務是根據給定的誤導推論來生成不同的結局，在MIG任務中是給定敘述句與其前後文來生成誤導推論。[2]研究中是讓模型根據上下文來推論中間適合加入的句子，此研究主要是產生正確的推論敘述，而在MIG任務[29]主要是產生相反的誤導推論敘述。於我們先前的研究[29]中，我們提出一個自然語言的誤導推論生成任務，任務步驟是當給定了一個陳述事實的句子並給定其上下文，將其定義為(x,y,z)與誤導推論ˆy，模型會生成與該事實相反的推論句子，由此可以讓模型產生出多選題似是而非的干擾選項。然而這樣任務與我們所設定之反事實出題仍有所不同。而與我們生成Counter-Factual陳述句想法接近的研究為：[46]。於該研究中，作者探討有別於過往的是非題，將文章中原始句子作為True的問句並通過反義詞替換關鍵字生成False問句，本篇作者通過使用一種新穎的masking-and-infilling策略來生成更靈活與複雜的問句。要被mask的關鍵字找法有很多種，像是SRL結果中的predicate，句子中連接詞(”that”、”when”、”since”)之後的部分或是數字等等，來找出值得提問的句子。Infilling的作法則是以BART為基底作為生成模型，將原始句子的關鍵字mask後，提供前後句子作為BART的上下文後產生句子填入。作者為了找值得提問的句子所提出的方法亦可用來作為多選題產生題目的答案選擇方法，而infilling的想法如果使用得宜，在提供出題範圍的前後文後，也能將欲出題句子挖空讓BART模型填補後產生選項。雖然與我們目標相似，但仍與我整合知識圖譜與語言模型為近年語言模型訓練熱門關注議題。於[42]研究中，作者們提出一CounterfactualGenerationControllableCounterfactualGenerationDistractorGenerationMisleadingInferenceGeneration種能將文本和知識圖譜的資訊進行深度雙向、自我監督的預訓練的方法，透過作者的方法可以將一些特定領域的KnowledgeBase資料集結合相關文獻去訓練語言知識模型，讓語言模型和知識圖譜之間的資訊互相交換，並且在回答特定領域問答的任務上，表現優於現有的語言模型和知識圖譜增強模型。我們想透過作者提供的預訓練模型，來做多選題選項生成，透過知識圖譜所額外提供的資訊，讓在考慮生成選項時，除了題目和答案的文字外，用這些文字從知識圖譜萃取相關的知識圖譜子圖幫助生成選項有更多參考資訊。知識圖譜於問句生成之研究也見於[5][13]。於[5]研究中，作者之想法為使用知識圖譜進行問題生成任務時，通常只會取出資料與關係的三元組合，並不能完整利用知識圖譜中豐富的資訊，因此作者基於GraphNeuralNetworks提出了BidirectionalGatedGraphNeuralNetwork(BiGGNN)，將知識圖譜中的node及edge編碼成向量，使得每個node都包含了周圍其他node的資訊，並對答案的node進行標註，隨後透過RNN進行問句生成，生成過程中RNN會決定生成新文字或直接複製某個node的資訊放進問題。我們應可借鏡此研究論文的想法，可將知識圖譜及答案的資訊完整編碼，可基於此特性，在生成問題時同時考慮答案附近的node，讓模型在建立問題的同時也能根據附近的node產生干擾選項。而於[13]研究中，則提出的ISEEQ架構，主要目的是希望從簡短的敘述句中去尋找更多訊息來反問使用者是否符合關於敘述句背後未提及的情況，在使用時，使用者只需輸入簡短的敘述句，模型會先從知識圖譜擴展並增加query的關鍵字，透過這些關鍵字模型就能retrieve更多相關的文章進行ranking，在從這些文章中使用QG模型產生一些原始query中未提及但可能相關背景知識，例如當使用者說道:“我感到疲勞且渾身無力”，ISEEQ能回覆:“請問病徵持續了多久?”，或是“你最近14天有出國過嗎?”這類可能與其他知識相關的問句。這種方式最大的貢獻及是讓機器學習到”反問”的能力，而非一味的透過query生成出結果。這樣的想法，或許可引用來提升題幹生成的精確度，舉例來說:有位教師想出關於日治時期的歷史題目，我們就能先藉由ISEEQ來詢問是要出與日治時期戰爭相關的題目或是與日治時期建設相關的題目，以此來取得更精確的題幹生成需求。[1]J.Araki,D.Rajagopal,S.Sankaranarayanan,S.Holm,Y.Yamakawa,andT.Mitamura.Gen-eratingquestionsandmultiple-choiceanswersusingsemanticanalysisoftexts.InProceedingsofCOLING2016,the26thInternationalConferenceonComputationalLinguistics:TechnicalPapers,pages1125–1136,2016.[2]C.Bhagavatula,R.L.Bras,C.Malaviya,K.Sakaguchi,A.Holtzman,H.Rashkin,D.Downey,S.W.-t.Yih,andY.Choi.Abductivecommonsensereasoning.arXivpreprintarXiv:1908.05739,[3]S.K.Bitew,A.Hadifar,L.Sterckx,J.Deleu,C.Develder,andT.Demeester.Learningtoreusedistractorstosupportmultiplechoicequestiongenerationineducation.IEEETransactionsonLearningTechnologies,2022.[4]Y.-H.ChanandY.-C.Fan.Arecurrentbert-basedmodelforquestiongeneration.InProceedingsofthe2ndWorkshoponMachineReadingforQuestionAnswering,pages154–162,2019.[5]Y.Chen,L.Wu,andM.J.Zaki.Towardsubgraphguidedknowledgegraphquestiongenerationwithgraphneuralnetworks.arXivpreprintarXiv:2004.06015,2020.[6]S.-H.Chiang,S.-C.Wang,andY.-C.Fan.Cdgp:Automaticclozedistractorgenerationbasedonpre-trainedlanugagemodel.InFindingsoftheAssociationforComputationalLinguistics:[7]Y.-S.Chuang,R.Dangovski,H.Luo,Y.Zhang,S.Chang,M.Soljac￿ic￿,S.-W.Li,W.-t.Yih,Y.Kim,andJ.Glass.Diffcse:Difference-basedcontrastivelearningforsentenceembeddings.arXivpreprintarXiv:2204.10298,2022.[8]H.-L.Chung,Y.-H.Chan,andY.-C.Fan.Abert-baseddistractorgenerationschemewithmulti-taskingandnegativeanswertrainingstrategies.InFindingsoftheAssociationforComputationalLinguistics:EMNLP2020,pages4390–4400,2020.[9]B.Dhingra,K.Mazaitis,andW.W.Cohen.Quasar:Datasetsforquestionansweringbysearchandreading.arXivpreprintarXiv:1707.03904,2017.[10]D.Dua,Y.Wang,P.Dasigi,G.Stanovsky,S.Singh,andM.Gardner.ingcomprehensionbenchmarkrequiringdiscretereasoningoverparagraphs.arXiv:1903.00161,2019.[11]T.Gao,X.Yao,andD.Chen.Simcse:Simplecontrastivelearningofsentenceembeddings.arXivpreprintarXiv:2104.08821,2021.[12]Y.Gao,L.Bing,P.Li,I.King,andM.R.Lyu.Generatingdistractorsforreadingcomprehen-sionquestionsfromrealexaminations.InProceedingsoftheAAAIConferenceonArtificialIntelligence,volume33,pages6423–6430,2019.[13]M.Gaur,K.Gunaratna,V.Srinivasan,andH.Jin.Iseeq:Informationseekingquestiongenerationusingdynamicmeta-informationretrievalandknowledgegraphs.InProceedingsoftheAAAIConferenceonArtificialIntelligence,volume36,pages10672–10680,2022.[14]Q.Guo,C.Kulkarni,A.Kittur,J.P.Bigham,andE.Brunskill.Questimator:Generatingknowl-edgeassessmentsforarbitrarytopics.InIJCAI-16:ProceedingsoftheAAAITwenty-FifthInternationalJointConferenceonArtificialIntelligence,2016.[15]D.Jin,E.Pan,N.Oufattole,W.-H.Weng,H.Fang,andP.Szolovits.Whatdiseasedoesthispa-tienthave?alarge-scaleopendomainquestionansweringdatasetfrommedicalexams.AppliedSciences,11(14):6421,2021.[16]V.Karpukhin,B.Og￿uz,S.Min,P.Lewis,L.Wu,S.Edunov,D.Chen,andW.-t.Yih.Densepassageretrievalforopen-domainquestionanswering.arXivpreprintarXiv:2004.04906,2020.[17]D.Khashabi,S.Chaturvedi,M.Roth,S.Upadhyay,andD.Roth.Lookingbeyondthesurface:Achallengesetforreadingcomprehensionovermultiplesentences.InProceedingsofthe2018ConferenceoftheNorthAmericanChapteroftheAssociationforComputationalLinguistics:HumanLanguageTechnologies,Volume1(LongPapers),pages252–262,2018.[18]O.Khattab,C.Potts,andM.Zaharia.Relevance-guidedsupervisionforopenqawithcolbert.TransactionsoftheAssociationforComputationalLinguistics,9:929–944,2021.[19]O.KhattabandM.Zaharia.Colbert:Efficientandeffectivepassagesearchviacontextualizedlateinteractionoverbert.InProceedingsofthe43rdInternationalACMSIGIRconferenceonresearchanddevelopmentinInformationRetrieval,pages39–48,2020.[20]G.Kumar,R.E.Banchs,andL.F.D’Haro.Revup:Automaticgap-fillquestiongenerationfromeducationaltexts.InProceedingsoftheTenthWorkshoponInnovativeUseofNLPforBuildingEducationalApplications,pages154–161,2015.[21]G.Lai,Q.Xie,H.Liu,Y.Yang,andE.Hovy.Race:Large-scalereadingcomprehensiondatasetfromexaminations.arXivpreprintarXiv:1704.04683,2017.[22]G.LeBerre,C.Cerisara,P.Langlais,andG.Lapalme.Unsupervisedmultiple-choicequestiongenerationforout-of-domainq&afine-tuning.InProceedingsofthe60thAnnualMeetingoftheAssociationforComputationalLinguistics(Volume2:ShortPapers),pages732–738,2022.[23]K.Lee,M.-W.Chang,andK.Toutanova.Latentretrievalforweaklysupervisedopendomainquestionanswering.arXivpreprintarXiv:1906.00300,2019.[24]C.Liang,X.Yang,N.Dave,D.Wham,B.Pursel,andC.L.Giles.Distractorgenerationformultiplechoicequestionsusinglearningtorank.InProceedingsofthethirteenthworkshoponinnovativeuseofNLPforbuildingeducationalapplications,pages284–290,2018.[25]C.Liang,X.Yang,D.Wham,B.Pursel,R.Passonneaur,andC.L.Giles.Distractorgenera-tionwithgenerativeadversarialnetsforautomaticallycreatingfill-in-the-blankquestions.InProceedingsoftheKnowledgeCaptureConference,pages1–4,2017.[26]V.Liévin,C.E.Hother,andO.Winther.Canlargelanguagemodelsreasonaboutmedicalquestions?arXivpreprintarXiv:2207.08143,2022.[27]Y.Mao,P.He,X.Liu,Y.Shen,J.Gao,J.Han,andW.Chen.Generation-augmentedretrievalforopen-domainquestionanswering.arXivpreprintarXiv:2009.08553,2020.[28]Y.Nie,S.Wang,andM.Bansal.Revealingtheimportanceofsemanticretrievalformachinereadingatscale.arXivpreprintarXiv:1909.08041,2019.[29]H.-Y.Peng,H.-L.Chung,Y.-H.Chan,andY.-C.Fan.Misleadinginferencegenerationviaproxi-malpolicyoptimization.InPacific-AsiaConferenceonKnowledgeDiscoveryandDataMining,pages497–509.Springer,2022.[30]L.Qin,A.Bosselut,A.Holtzman,C.Bhagavatula,E.Clark,andY.Choi.Counterfactualstoryreasoningandgeneration.arXivpreprintarXiv:1909.04076,2019.[31]P.Rajpurkar,R.Jia,andP.Liang.Knowwhatyoudon’tknow:Unanswerablequestionsforsquad.arXivpreprintarXiv:1806.03822,2018.[32]P.Rajpurkar,J.Zhang,K.Lopyrev,andP.Liang.Squad:100,000+questionsformachinecomprehensionoftext.arXivpreprintarXiv:1606.05250,2016.[33]S.Ren,Y.Deng,K.He,andW.Che.Generatingnaturallanguageadversarialexamplesthroughprobabilityweightedwordsaliency.InProceedingsofthe57thannualmeetingoftheassociationforcomputationallinguistics,pages1085–1097,2019.[34]S.RenandK.Q.Zhu.Knowledge-drivendistractorgenerationforcloze-stylemultiplechoicequestions.InProceedingsoftheAAAIConferenceonArtificialIntelligence,volume35,pages[35]Y.Su,T.Lan,Y.Wang,D.Yogatama,L.Kong,andN.Collier.Acontrastiveframeworkforneuraltextgeneration.arXivpreprintarXiv:2202.06417,2022.[36]A.Talmor,J.Herzig,N.Lourie,andJ.Berant.Commonsenseqa:Aquestionansweringchallengetargetingcommonsenseknowledge.arXivpreprintarXiv:1811.00937,2018.[37]J.Welbl,N.F.Liu,andM.Gardner.Crowdsourcingmultiplechoicesciencequestions.arXivpreprintarXiv:1707.06209,2017.[38]J.Welbl,P.Stenetorp,andS.Riedel.Constructingdatasetsformulti-hopreadingcomprehensionacrossdocuments.TransactionsoftheAssociationforComputationalLinguistics,6:287–302,[39]T.Wu,M.T.Ribeiro,J.Heer,andD.S.Weld.Polyjuice:Automated,general-purposecounter-factualgeneration.arXivpreprintarXiv:2101.00288,1(2),2021.[40]J.Xie,N.Peng,Y.Cai,T.Wang,andQ.Huang.Diversedistractorgenerationforconstruct-inghigh-qualitymultiplechoicequestions.IEEE/ACMTransactionsonAudio,Speech,andLanguageProcessing,30:280–291,2021.[41]Z.Yang,P.Qi,S.Zhang,Y.Bengio,W.W.Cohen,R.Salakhutdinov,andC.D.Manning.Hotpotqa:Adatasetfordiverse,explainablemulti-hopquestionanswering.arXiv:1809.09600,2018.[42]M.Yasunaga,A.Bosselut,H.Ren,X.Zhang,C.D.Manning,P.Liang,andJ.Leskovec.Deepbidirectionallanguage-knowledgegraphpretraining.arXivpreprintarXiv:2210.09338,2022.[43]C.Y.Yeung,J.S.Lee,andB.K.Tsou.Difficulty-awaredistractorgenerationforgap-fillitems.InProceedingsoftheThe17thAnnualWorkshopoftheAustralasianLanguageTechnologyAssociation,pages159–164,2019.[44]C.Zhang,Y.Sun,H.Chen,andJ.Wang.Generatingadequatedistractorsformultiple-choicequestions.arXivpreprintarXiv:2010.12658,2020.[45]X.Zhou,S.Luo,andY.Wu.Co-attentionhierarchicalnetwork:Generatingcoherentlongdistractorsforreadingcomprehension.InProceedingsoftheAAAIConferenceonArtificialIntelligence,volume34,pages9725–9732,2020.[46]B.Zou,P.Li,L.Pan,andA.Aw.Automatictrue/falsequestiongenerationforeducationalpur-pose.InProceedingsofthe17thWorkshoponInnovativeUseofNLPforBuildingEducationalApplications(BEA2022),pages61–70,2022.於本研究計畫中，我們目標基於自然語言生成技術進行選擇題自動出題研究。我們預期的展示情境為：給定一知識範圍(如農業文集)，我們可於系統直接輸入一組主題關鍵詞，而後自動生成關於該主題相關之選擇題題組(含字詞等級、句子等級、挑選是非類型之選擇題)。然而，如同前言背景中所言，現有技術離目標仍有許多待強化之處，對此我們擬定三大出題目標：字詞等級干擾選項、句子等級干擾選項與反事實干擾選項進行研究。針對此三大目標，我們擬定(1)基於知識圖譜之強化與(2)基於生成語言模型與對比學習之強化三個核心技術進行探討。研究技術項目與項目間之關係與進行規劃如Figure7所示。於第一年度中，我們規劃「考量題幹可用性與干擾選項間關聯之Retrieve-and-Generate架構優化」、「基於語言模型與知識圖譜之Text2Text干擾選項生成」與「基於Masked-LM之反事實干擾選項生成研究」進行研究。「考量題幹可用性與干擾選項間關聯之Retrieve-and-Generate架構優化」著眼於優化目前https://mcq-demo.nlpnchu.org之WordLevel之選擇題生成系統。希冀自動過濾相似或語意不正確題幹，並且考量干擾選項彼此間之關聯進行最終干擾選項集合之綜合挑選。「基於生成語言模型之Text2Text干擾選項生成模型」則考量Text2Text模式利用生成模型直接生成干擾選項序列，藉此嘗試生成句子等級之干擾選項。並進一步地探索結合知識圖譜與語言模型之干擾選項生成，希冀透過知識圖譜之整合來生成更有誘導性且合理之字詞等級干擾選項。此外，於「基於Masked-LM之反事實干擾選項生成研究」項目中，我們預計著手挑戰反事實干擾選項生成，我們嘗試利用透過NER擷取給定事實陳述句中之關鍵字並Masked-LM該關鍵字來填入機率較低之字詞，以為反事實語句生成之作法。我們預期第一年度工作完成後，將可優化現有單詞等級干擾選項之選擇題自動生成系統，並於第二年度中，我們則規劃「基於知識圖譜強化之反事實干擾選項生成」、「基於對比式學習之干擾選項生成模型強化」與「利用Open-DomainQuestionAnswering錯誤結果之干擾選項挑選」。於此年度中，進一步嘗試利用知識圖譜萃取三元組知識後進行反事實語句之生成，基本想法為透過萃取三元組後再進一步輸入語言模型，並希望語言模型適當地進行反事實語句的改寫。此一部分預計使用第一年度之「基於Masked-LM之反事實干擾選項生成研究」之基礎以為訓練資料。對於字詞等級的干擾選項，我們則探索利用Open-DomainQuestionAnswering模型所錯誤回報之答案當作干擾選項。我們希望透過此不同之想法來取得更多樣的干擾選項。再者，如前言所述，生成模型生成序列時常出現生成結果重複的狀況，對此我們嘗試利用對比式來改善生成模型這方面的缺點，以優化干擾選項之生成。第二年度工作項目完成後，我們可初步提供反事實干擾選項之選擇題生成、優化後之句子等級干擾選項生成與基於ODQA方式取得之字詞等級干擾選項生成。於第三年度中，我們則規劃「基於NodeCentrality考量主題涵蓋性之選擇題題組生成」與「生成難度可控之干擾選項生成研究」。如同前言所述，一個理想的選擇題出題系統應該考慮題組生成，整體題目之主題涵蓋性，對此我們也將嘗試基於知識圖譜涵蓋的方式考慮節點centrality重要度方式來進行題組之生成。同時我們也將嘗試題目難易度之控制生成研究。我們的想法是當自動生成成為可能後未來的出題應該是個人化模式；針對個人的學習狀況進行測驗，因此我們規劃「個人化與生成難度可控之干擾選項生成研究」進行探討。第三年度，我們也預計投入人力進行系統的實作開發與整合。以下茲針對擬訂之研究議題其問題挑戰與1.基於語言模型與知識圖譜之Text2Text干擾選項生成模型近年來，語言模型除了使用CandicateGeneratingandRanking架構的MaskedLM之外，使用Encoder-Decoder架構的Text2Text模型於生成任務上也有很好的成果，因此我們的想法是把干擾選項生成Model視為一個Text2Text任務，嘗試讓模型直接生成干擾選項。且使用Text2Text模型生成Distractor有別於用MaskedLM生成Distractor，我們希望模型能生成的Distractor有以下特性(1)非單詞之Distractor:目前在Distractor生成上多為單詞生成，要生成一段敘述/句子的干擾選項還沒有很好的做法。(2)特定domain之MCQ:目前的MCQ多為英文測驗，若想針對特定領域出選擇題，使用MaskedLM生成的Distractor會因語料庫限制而為達到上述的兩種目標需求，我們設Q為輸入之問句，A為輸入問句之答案，並將Q與A字串接字串當作輸入，訓練模型輸出一連串問句Q的干擾選項字串D=(d1,d2,d3)。模型架構如圖2所示，我們將「哪一種技術最適用於嚴重特殊傳染性肺炎之病原快速篩檢?」與「聚合￿連鎖反應」接在一起輸入至Text2Text模型，而模型生成了「重組DNA技術」、「電腦斷層掃描」與「組織培養」三個干擾選項，三個選項皆為醫療檢測方式且都不是單詞，符合如前言所述，我們也預計探索使用知識圖譜來強化生成干擾選項的能力。如Figure8的圖例，我們可以使用語言模型結合醫療知識圖譜，利用給定的文本從知識圖譜中萃取有包含這些topicentities以及相鄰的nodeentities的子圖，再分別將文本與子圖輸入至語言模型和GraphNeuralNetwork(GNN)，讓模型學習找出關鍵的資訊與強化network中重要entitynodes跟edges的權重，藉由GNN的額外資訊與語言模型能力生成更好的選項。Figure8為我們規劃做法的概觀。主要分兩階段進行：(1)關鍵字子圖與GNNEmbedding萃取與(2)GNNEmbedding與LanugageEmbedding間之FusionEncoder生成。具體而言，第一個階段是將從知識圖譜裡萃取相關的知識圖譜子圖。我們一開始先使用NERModel抓取題幹中的entities，再透過這些entities透過知識圖譜去尋找entitynodes以及所有跟entitynodes相鄰3-hop內的bridgenodes，將檢索出來的結果當成知識圖譜子圖，並在後續第二階段提供以Figure9的圖例，我們先將問題透過NERModel斷詞得到”白血病”、”發病”、”器官”與”發病原因”當作entities，再透過這些entities當作關鍵字透過Retriever從知識圖譜萃取Figure8:結合知識圖譜與語言模型生成之構想架構圖與這些詞相關的node和edge，如利用”白血病”與”器官”能找到”肝臟”、”骨髓”、”肺部”等等entities，我們能利用這些entities來當作後續的生成選項。第二個階段則將文本與第一階段得到的知識圖譜子圖分別輸入至語言模型與GNN進行訓練，訓練目標為將Input的[MASK]Token還原來產生干擾選項，先將語言模型的Input額外新增[MASK]Token，經過N層LMLayer做self-attention後，透過FusionEncoder再結合知識圖譜的資訊與原本的文本去生成選項，透過以上的手法，來強化干擾選項之生成。2.考量題幹可用性與干擾選項間關聯之Retrieve-and-Generate架構優化如同研究目的所述，本研究項目著眼於優化現有Retrieve-and-Generate生成架構。我們•文章檢索器過濾:在Retrieving-and-Generating方法中Retriever會檢索回傳含有主題字詞的文章，但有部分文章並不適合當作問句生成模型的輸入，那些文章含有太多的雜訊，我們想嘗試在這步驟先過濾一些文章。我們的想法是利用詞向量中的相似詞再次檢索Retriever回傳的文章，把過於不相關的文章濾掉，減少問句生成模型輸出過於複•生成題幹品質自動過濾：問句生成模型輸出的問句，會再交由使用者挑選，但為了更好的效果，我們想嘗試利用語言模型做初步的篩選，減少人工挑選時間。我們想嘗試利用ODQA模型來過濾問句，將問句生成模型輸出的問句輸入至問答模型後，根據問答模型的回答結果與信心分數來決定該問句是否合適當作MCQ。如Figure10所例，我們將生成之問句再次投入ODQA模型，透過答案選項之信心度來為問句生成過濾之基Figure10:Unanswerablequestionsolution•考量干擾選項間關聯之干擾選項挑選:如前言所述，一個可以提升選擇題生成品質￿考量干擾選項彼此之間之難易度。而在Retrieving-and-Generating方法中，生成干擾選項的方式為取出詞向量模型中與主題字詞相似之詞，使用相似度前三名當作干擾選項，因此有選項過於相近而造成問題太難的可能。為做改善，我們取出所有相似詞的向量與主題字詞向量計算相對距離，設定各個選項與答案的相似度範圍(距離範圍)，再從中隨機挑選相似詞，讓干擾選項彼此之間難易度有所區分以提升試題之鑑別度。•基於語言模型之干擾選項候選生成:雖然詞向量模型生成distractor過程迅速，但如果有主題字詞沒有被詞向量模型看過或有主題字詞一字多意，就無法生成適當的干擾選項。這個問題我們想嘗試使用MaskedLM來解決，我們將Retriever回傳的文章中含有主題字詞的段落記錄下來，移除主題字詞後輸入MaskedLM，再從MaskedLM輸出的候選字中選出distractor，使用這個方式生成的干擾選項不會被上述的缺點受限，也更Figure11:OptimizedRetrievingandGeneratingFramework3.基於Masked-LM之反事實干擾選項生成研究本研究項目為基於Masked-LM技巧來生成反事實干擾選項。我們需要先查找與事實相符之敘述，再修改成反事實干擾選項。我們預計之作法為：首先讓使用者先輸入考題關鍵字，再使用retriever查找相關文章，其目的為從相關文章中選取與事實相符之敘述。接著，將retriever檢索到的文章回傳給使用者，由使用者選取與事實相符之敘述。並按照以下之步驟進行Masked-LM反事實干擾選項生成。為了抽換專有名詞，我們先使用命名實體與事實相符之敘述中的所有專有名詞，以下圖為例”腎小管藉壓力差方式再吸收葡萄糖回微血管”，透過命名實體模型會萃取出專透過命名實體模型，我們會萃取出多個與事實相符之敘述中的專有名詞，我們需要選擇一個專有名詞行抽換。我們認為該專有名詞在相關文章中的重要程度越高，越高機率是考題中的重點。為了確保抽換專有名詞的品質，我們使用BM25來查看各個專有Figure12:AutoEncoder模型與AutoRegressive模型做MaskLM3.使用Maskedlanguagemodel模型進行改寫Maskedlanguagemodel，簡稱為MaskLM，該任務的會將輸入中的部分內容遮擋起來，模型會根據其他未被遮擋的內容，預測出被遮擋的內容或與其擁有相似語義的內容，現有的MaskLM模型分為兩種，分別為AutoEncoder框架的MaskLM模型與AutoRe-gressive框架的MaskLM模型，經典模型如BERT與BART。針對MaskLM任務，兩種模型進行預測的方式也有所不同，AutoEncoder框架的模型會預測出字，而AutoRe-gressive框架的模型會預測出字、詞或是句子，詳細範例如Figure12。根據抽換專有名詞的需求，我們將使用Autoregressive模型做MaskLM任務來抽換重要的專有名詞。然而模型預測出的內容眾多，有可能是原先遮擋的內容、遮擋內容的同義詞或是與遮擋內容擁有相似語義的內容。理想的情況下，模型預測的內容最高機率為原先遮擋的內容，若我們選擇原先遮擋的內容或是遮擋內容的同義詞進行抽換，會造成與事實相符之敘述未改變，便無法生成反事實干擾選項。為了避免此狀況發生，我們使用NLI任務中的換句話說(MRPC)任務來檢測其是否為干擾選項。換句話說任務主要在判斷兩句話的語意是否相同，為了避免MaskLM模型選用同義詞進行抽換，即模型生成的干擾選項仍是正確的，我們將與事實相符之敘述與生成之干擾選項作為模型的輸入，若模型認為兩句話具有相同語義，我們會讓MaskLM模型預4.基於知識圖譜強化之反事實干擾選項生成於本研究項目中，我們嘗試利用知識圖譜提供語言模型額外資訊達成反事實干擾選項生成。我們的構想如Figure4所示，包含以下步驟。首先我們會讓使用者先輸入考題內文，再將內文使用斷詞或NER模型萃取entity，利用這些entity於知識圖譜中查找相關子圖，子圖必須包含內文中所有的entitynodes以及所有跟entitynodes相鄰節點，以此為知識生成之基礎。接著我們將檢索到的子圖內的nodes與edges萃取成三元組，每筆三元組的結構是以node,edge,node組成。如同圖中所示，每一筆三元組描述一基本事實陳述，如導體可以3.將與事實相符之敘述修改為反事實干擾選項為了將與事實相符之敘述更改為反事實干擾選項，我們將前一步驟所萃取的三元組與考題內文當作作語言模型的輸入來生成反事實干擾選項。我們預計透過生成語言模型與輸入給定的問句和三元組來生成與問句情境相符合的反事實干擾選項，以FigurejQuery36306984126397040076_1676210426716為例，生成語言模型在給定關於與導體相關的題目敘述以及從知識圖譜萃取與導體相關的三元組後，模型藉由這些三元組推斷其中的關係，比如”導體可以導電->石墨是導體->筆芯是由石墨製成”，從上述的三元組推斷出”筆芯可以導電”的事實，也可以改寫成”筆芯不能導電”的反事實干擾選項。5.使用Open-DomainQuestionAnswering錯誤結果之干擾選項挑選研究如前言所述，我們預計使用ODQA模型所回答之錯誤結果來當作干擾選項(如Figure5)。以我們先前曾開發https://agri.nlpnchu.org/之農業問答系統為例，我們發現以上想法所面臨的挑於ODQA的架構下，可能會遇到ODQA正確回答所下達之問題，其回傳結果皆為可行之答案，也因此無法被使用來當干擾選項，對此，我們探索兩種作法來進行調整。–1.使用BM25，且刪除尋找出來文章的正確答案在使用BM25的情境下，因為是使用關鍵字比對，所以會找出許多跟正確答案相關的文章，我們將找到文章的正確答案刪除，這樣這個文章會變成絕對回答不出正確答案的文章，可是因為Reader對文字有一定的理解能力，並且具有關鍵字的文章中也很可能會有，他會從剩下可能的答案中，尋找一個最相近的選項，這樣就可以較容易生出干擾選項。如Figure13所示，我們將答案”2,4-D”從文章中直接刪除後，再交給Reader進行閱讀，藉此確保正確答案不會被回傳。Figure13:基於ODQA架構之Retriever弱化方法–2.使用DensePassageRetrieverDensePassageRetriever[16]，DensePassageRetriever可以將資料庫的文章轉為向量的方式表示，在搜尋的時候也將Query轉為向量，並且比對兩個向量的相似度，這樣就可以找出相似的文章，可是我們想說，可以利用DPR找尋文章相似度的這一點，理論上相似的文章跟Query算出來的相似度會是最高的，我們可以將相似度高的文章拿出來，並且查看有沒有答案出現在上面，如果沒有答案，我們就將他丟給Reader，因為我們覺得如我相似度高，就代表Retriever認為裡面有答案，可是裡面並沒有含有我們的正解，所以這篇文章就含Retriever認為是答案，但是實際上不是答案的文字，所以這時候我們將這篇文章傳給Reader，就會有比較高在我們使用baseline的情境中，有時候也會出現不相關的答案，例如在??中，問模型關於殺草劑的問題，會出現的人力除草，人力除草跟題目所問的殺草劑名稱是不相關的，所以我們需要用一個方法把他過濾掉。在這種情況下，我們如果加入正確答案當作Query一起輸入Retriever，就會增加模型找到正確passage的機率，對我們想找跟正確答案不相關但是相似度較高的Passage會產生反效果，所以我們採用的是從Reader輸出之後把他過濾掉的方法。當Reader從Retriever拿到Passage之後，我們會將正確答案去除，之後再將剩餘的答案丟入一個LanguageModel作判斷，如果他跟答案的距離過於遙遠，就放入NegativeList，如果是相關的就放入PositiveList，之後要挑相關答案的時候可以從PositiveList裡面挑選。6.基於對比式學習之干擾選項生成模型強化ContrastiveLearning的設計上需要考量如何去設定PositiveSample以及NegativeSample。在我們的目標中，因為不希望模型會重複的生成出相同意思的句子，所以我們預計使用同義的句子當作NegativeSample，而資料集中的干擾選項可以被當作PositiveSample。然而，由於我們的資料集中並沒有NagativeSample的選項，也就是同義於模型生成的選項。對此，我們的想法為透過迭代生成來產生我們需要的NegativeSample。具體而言，如Figure14所舉例，在DistractorGenerator的第一次生成中，我們會輸入Question以及Answer使模型生成出干擾選項。而在第二次迭代中，我們除了會輸入QuestionAnswerPair外，還會再加上在上一次迭代中生成的干擾選項，以提示模型先前已生成的Distractor的資訊。對於第二次迭代的輸出來說，第一次迭代輸出的Distractor，即為它的NegativeSample，因為DistractorGenerator必須要生成出與上一次迭代生成的Distractor意義相異的句子，所以它們之間的SentenceEmbedding不能太過接近。Figure14:IterationGenerater示意，在第一次的迭代中生成的Distractor會作為第二次迭代的輸入，以提示模型不要生成出類似的DistractorFigure15:對比式學習於干擾選項生成之預想架構我們所規劃之整體框架，如Figure15所示。我們使用多次迭代的方式來生成Distractor，對於第一次迭代來說，我們會從其他in-batch的data中取得Sentence當作NegativeSample。而在第二次以後的迭代來說，除了其他in-batch的Sentence以外，我們還會將此次迭代前所生成的Distractor加入我們的NegativeSample中。透過NegativeSample和模型輸出的Distractor的dotproduct可以計算出ContrastiveLoss，此外我們也會加入Distractor和Label計算的CrossEntropyLoss，來計算出我們的TotalLoss15。ContrastiveLoss定義如下:x為生成的distractor，p為資料集中的Distractor(Label)，N為NegativeSampleSet，N={n1,n2,...nj}，其中包括先前迭代生成的Distractor以及其他in-batch的選項。7.基於NodeCentrality考量主題涵蓋性之選擇題題組生成如同前言所述，本研究議題之目標為考量主題涵蓋性之選擇題題組生成。因此，我們預計根據使用者所指定的主題，按Figure16架構規劃來進行題組生成。1.透過知識圖譜子圖萃取來涵蓋所指定主題之內容：藉由知識圖譜上NodeCentrality方式來萃取關鍵字，以為題幹之生成。我們的基本想法為在知識圖譜上我們定位出與使用者輸入之主題關鍵字相關之結點後，我們可以試者套入PageRank或者其他圖形上計算NodeCentrality重要節點挑選，並以重要節點為題幹生成之主軸。2.透過上一步取得之重要答案片段後，進行問題之生成。並生成較多的問題數量以為CandidateQuestionPool，而後將問題將題組生成問題設定一成為一選擇最佳化議題(Optimization)：根據主題生成大量的單一題目，而後定義UtilityScore來計算個別題目組合後之優劣，來挑選最佳化的題組以為輸出。於本研究項目中，我們預計探索以下兩種生成難度控制方式。•調整Masked-LM比例之反事實選項難度控制：現有的Masked-LM模型會將重要的專有名詞進行遮擋，模型會透過未遮擋的內容對該專有名詞進行預測。理想的情況下，當我們僅對該專有名詞進行遮擋時，模型可成功地預測出遮擋的專有名詞或語義相近的專有名詞，這不僅是因為MaskLM模型夠強大，也是因為未遮擋的內容提供模型足夠多的資訊讓模型預測，若未遮擋的內容提供的資訊不夠多，則模型預測出的內容有可能會與遮擋的內容語義相差很多。我們根據Masked-LM模型的此特性對題目的難易度進行控制，若我們將多個專有名詞進行遮擋，再讓模型預測時，模型會因為提供的資訊不足，而預測出語義較不相近的專有名詞，即較不可能的答案。以下表為例，我們可以觀察到當[Mask]越多時，我們要判斷該事實是否正確就越形容易。[Mask]借壓力差吸收[Mask]回微血管[Mask]借壓力差吸收[Mask]回[Mask]•調整ODQA檢索文章之難度控制：由於我們的基礎架構為基於文章檢索(DocumentRetrieving)來取得包含使用者所給定的關鍵字之文章，並使用該文章與關鍵字為問題生成模型之輸入，來生成選擇題問句。我們若挑選較為冷門之文章進行生成，則預期可以生成較為困難之題幹。而就干擾選項而言，我們可藉由調整輸入給閱讀理解模型之文章來產生難度較高的錯誤。具體而言，給定一有包含正確答案A之文章C，我們先將答案片段A於文章D中刪除，而後將其輸入至閱讀理解模型，藉此產生錯誤之閱讀理解結果，以這樣的錯誤結果當Distractor預期將有較高之誘導能力。相類似的想法也可以套用為挑選Retriever認為相關度比較不高的文章以為閱讀理解模型之輸入。此外，就生成題目之廣度與深度而言，我們可藉由控制取得之文章來調配題組之廣度與深度。例如，若我們以單篇或少數篇文章進行生成，我們所生成的主題題組針對測驗內容而言，應該可產生知識性較為深入之試題。而若針對較多篇文章，則應可產生範圍較廣本研究計畫將以三年為期，共八項研究議題進行，各研究產出如上文所示，並整合各技術成為一選擇題自動生成系統，預期的展示情境為：我們可於系統直接輸入一知識範圍，系統將利用我們所開發之技術先進行自動關鍵語萃取，而後進行各種類型的選擇題出題並考量題組關聯性。我們也將彙整各研究議題中所發展之技術為一個成為公開原始碼專案，以為後續相關研究之用。並將計畫執行過程所累積的資料，公開希望能吸引更多研究者之投入。對於學術研究、國家發展及其他應用方面預期之貢獻自然語言生成已是未來深度學習領域之熱門研究重點。透過本研究計畫之投入我們將可進行先驅之探索，建立學生團隊與自然語言處理與生成之能力，並持續過去之閱讀測驗自動出題研究成果，以知識類型選擇題自動出題為主要系統開發目標為例，希冀能為國內外探索不同的自然語言生成應用服務。此外若可順利完成也可以成為一成功範例利用人工智慧輔助教師進行測驗的客製化與個人化學習。對比於題庫類型的方式，透過我們技術我們將可動態產生各式各樣不同之考題為每個人量身定做難度適中之考題。就此一觀點出發，我們預期本計畫除學術研究價值外，同時也兼具商業價值，也應可成為EdTech(教育科技)領域之一環。再者，疫情改變了學習與教學的模式。同步與非同步之線上教學已成為教學模式常態之一。線上教學帶來了好處，但也帶來了顧慮。其中，學習的專注度應是諸多顧慮中主要令人擔憂的一環。隔著螢幕學生學習是否專注？目前的教學內容學生是否已跟上進度？等等問題是線上教學學習專注度所在意的問題。對此，我們認為適時與即時地檢測學生學習成效，來提醒學生與教師目前之學習成效，應能適度緩解線上學習專注度之顧慮。因此，我們也常見於MOOCs課程於一個授課單元結束後適時地給予測驗(多數以選擇題測驗)，來確認學生學習成效。然而，人工設計試題耗費心力，一成不變全班同學皆相同試題也可能透過即時訊息(instantmessage)之分享而取得答案，造成檢測之無效。對此，我們認為隨著深度學習技術之快速演進，結合自然語言(NaturalLanguageGeneration)生成技術來進行自動出題技術已成為可能；透過自然語言生成技術自動化出題，產生多樣化的試題，來進行個人化的檢測，將有除了可以增進研讀論文、撰寫程式、架設系統以及撰寫論文的基本能力之外，參與本計畫之研究人員，將可以學習到深度學習與自然語言生成技術之基本觀念、實際應用以及相關的核心技術，在這些基礎之下，透過積極的研究與討論，將可以提升參與計畫人員思考問題與解決困難的重要能力。我們預期可獲得具體技能如下:(1)設計與建立一個深度學習模型之能力，(2)整合不同先進技術之能力，(3)自然語言生成技術之瞭解，(4)資訊檢索相關知識與軟體套件之暸解，(5)佈建深度學習模型於WebScale之能力。"}, "1676210674.4444792": {"單位": "", "標題": "伍動螞蟻-", "作者": "", "發布日": 0, "摘要": "", "全文": "螞蟻會對我們造成很大的影響。入侵紅火蟻在澳洲、台灣肆虐，攻擊任何在牠們巢穴附近的生物；黃瘋蟻在耶誕島獵捕紅地蟹，造成耶誕島紅地蟹的浩劫；熱帶大頭家蟻在澳洲是最強勢的入侵物種，牠們會保護介殼蟲及蚜蟲，造成農民的損失……螞蟻至少有260屬，18000種，所以我會藉由相關文獻及實地研究探討螞蟻的不同習性與生活型態，希望能讓大家了解，螞蟻對我們此次有關螞蟻的獨立研究，我主要是以文獻探討中內容分析的方式呈現簡報。並且加入問卷測試小學生對於螞蟻知識的了解程度。以下是我針對文獻探討、前測問卷、講座推廣和後側(KahootQ&A、google問卷編1.\t前測問卷發放-小學生的螞蟻知識知多少?\t(1)分析問卷中的答對率，並從中了解小學生對螞蟻知識了解的的程度，針對錯比較多的題目，進行內容的蒐集與整理，製作成專題講及臭巨山蟻來更了解螞蟻的日常習性，並藉此螞蟻知識推廣-(kahoot機智問答)我於民國一一○年三月三十日星期二舉辦了一場《伍動螞蟻－螞蟻知識知多少》的螞蟻知識推廣的更懂螞蟻，所以我在專題講座後請參語的同學在我煩惱要做什麼當我的獨立研究時，忽然看到了螞蟻相關報告，讓我彷彿撥雲見日，我決定要以螞蟻當作我的獨立研究主題。在文獻探討中，我將其分為八個章節，在網路上搜尋有關螞蟻的知識。並在閒暇的時間進行有關螞蟻的小實驗，目的就是要讓我的報告更完整，且更有價值。經過了一段艱辛的路程後，我完成了我的獨立研究。希望能讓大家更了解螞蟻，且更尊重牠們。螞蟻會對我們造成很大的影響。入侵紅火蟻在澳洲、台灣肆虐，攻擊任何在牠們巢穴附近的生物；黃瘋蟻在耶誕島獵捕紅地蟹，造成耶誕島紅地蟹的浩劫；熱帶大頭家蟻在澳洲是最強勢的入侵物種，牠們會保護介殼蟲及蚜蟲，造成農民的損失……螞蟻至少有260屬，18000種，所以我會藉由相關文獻及實地研究探討螞蟻的不同習性與生活型態，希望能讓大家了解，螞蟻對我們的(二)讓大家了解螞蟻之餘，能更尊重牠們。為蟻窩中具有生殖能力的雌性螞蟻，整個蟻群都為蟻后的後代。有時不需要雄蟻介入也能進行孤雌生殖，不過在這種情況下所產出來的卵為未受精的卵，只會孵化出雄蟻。大多數的蟻后和雄蟻都會進行婚飛，也就是在特定時間讓蟻后與型蟻同時飛出巢外，並且進行交配，而不同螞蟻進行婚飛的時間也不盡相同。依照種類的不同，有些種一個蟻窩中只會有一隻蟻后，也有些種類的螞蟻在一個蟻窩中同時會有上百隻具有生殖能力的蟻后。蟻后可生存長達30年，紀錄中由德國昆蟲學家HermannAppel所飼養的黑褐毛山蟻蟻后活了28¾年。雖然蟻后被稱為「蟻后」，但不能將人類社會當中上下階級和統治的觀念給帶入螞蟻社會中。事實上，蟻后在蟻巢中只是個產卵機器，在部分情況下甚至會被推翻。由此可證蟻后不一定在蟻巢中絕對的權勢。雄蟻的主要工作是與蟻后交配。由未受精的卵細胞發育而來，性染色體為X(人類男性的性染色體為XY)，大部分有翅，交配完不久後便會死去。工蟻的主要工作為負責搜尋食物、照顧蟻卵、幼蟲等大部分的工作，體型比兵蟻小。是由受精的卵細胞發育而來，染色體雙套（2n），大部分在一出生就就受到蟻后分泌的費洛蒙影響，所以不會產卵，在蟻后死亡後可能會產下未受精卵。未受精卵有營養卵或是雄蟻卵兩種，營養卵是食物，雄蟻卵則會孵化成雄蟻；而某些物種的工蟻卵巢能夠發育，交配後能夠產下能發育的受精卵。部分物種的工蟻產卵管特化為螫針，可以用來制服獵物或是防衛蟻窩。在螞蟻巢中，較年輕的工蟻會負責照顧蟻后；而幾乎所有的螞蟻都具有年齡多形性（Agepolytheism）的分工機制，除了兩個物種以外，分別是Amblyoponepallipes和大眼擬鬥牛犬蟻（Nothomyrmeciamacrops）。1.和工蟻一起保衛蟻巢。如熱帶火蟻、收穫蟻屬。2.在工蟻出外覓食的時候，幫忙擔任清除雜物的「推土機」並非每一種螞蟻都有兵蟻，沒有兵蟻的物種包括針蟻亞科、琉璃蟻亞科、擬家蟻亞科等。兵蟻是由受精的卵細胞發育而來，染色體雙套，為沒有生殖能力的雌蟲。某些物種的兵蟻頭部及大顎高度骨化且發達。一般來說頭部異速生長的大型個體稱作兵蟻，而頭部沒有異速生長的大型個體則稱作大工蟻，但這兩個詞彙之間還是有難以區分的灰色地帶。具有兵蟻的物種如大頭家蟻屬，具在蟻巢中，除生殖階級外，所有個體無翅。觸角為膝狀，某些物種觸角末端膨大，稱作「垂節」，牠們的觸角，牠們也用觸角判斷彼此身分。大部分物種胸部和腹部之間隘縮，形成腰節，實則屬於胸部的延伸，或稱腹柄節，具有三個腹節。胸節與前伸腹節合稱「中軀」），腹垂節稱作「後軀」。螞蟻的體型範圍平均大約為0.75~52mm，目前已知體型最大的螞蟻為已滅絕的古巨蟻亞科，蟻后體長可達6cm，翅膀長度可達15cm。螞蟻可以透過費洛蒙溝通，一隻螞蟻如果發現了食物，它就會在回家的路上留下一路的氣味，其他的螞蟻就會沿著這條氣味路線去找食物，並不斷地留下氣味加強氣味。如果這裡的食物被採集完了，沒有螞蟻再來，氣味就會逐漸消散。如果一隻螞蟻被碾碎，就會散發出強烈的警戒費洛蒙(註一)引起其他螞蟻進入緊戒狀態；如果一隻螞蟻散發出死亡費洛蒙，不管那隻螞蟻是活的還是螞蟻和其他昆蟲一樣透過觸角辨識氣味，觸角的末幾節膨大，呈膝狀彎曲，非常靈活。由於觸角是一對，因此既能辨别氣味的強度，也能辨識氣味來源的方向距離。但是也造就了蟻客(專門寄生於螞蟻巢內的生物，部分蟻客對螞蟻沒有害處，不過有一部分的蟻客對螞蟻有明顯危害)成蟲互相交哺並通過其氣味了解對方的健康狀況，對方發現的食物等資訊。同時也能區別對方屬於從事哪個分工的階級。如負責餵養蟻后及幼蟲的螞蟻，或是負責搜集食物的螞蟻后會不斷地分泌費洛蒙，這種費洛蒙能抑制工蟻的卵巢發育並讓工蟻知道蟻后還在巢內，一旦以這種氣味消失，有些物種會出現新的蟻后，有些物種的工蟻則會開始產卵，填補蟻后的功能。某些螞蟻用大顎啃咬以攻擊或自衛，山蟻亞科的物種能從腹部末端分泌蟻酸（甲酸），刺激被叮咬的傷口紅腫疼痛；部分物種腹部末端具有螫針。螞蟻亦是全世界力氣最大的昆蟲之一，牠的負重能力相當驚人，能拖動比牠體重還重1400倍的物品，也能背負自身體重註一:警戒費洛蒙分成己醛、己醇、正十一酮和丁基辛酮。己醛會最先出現，提醒工蟻提高警覺；己醇是與酒精(乙醇)類似的物質，會促使工蟻尋找問題來源；正十一酮會吸引工蟻接近問題來源，並在碰到外來物時張口就咬；丁基辛酮最後才會出現，會讓工蟻展開攻擊與咬噬的行動。蠻蟻亞科Agroecomyrmecinae鈍針蟻亞科Amblyoponinae（包含新蟻亞科Apomyrminae）†布朗長蟻亞科†Brownimeciinae異針蟻亞科Heteroponerinae鬥牛犬蟻亞科Myrmeciinae（包含：擬鬥牛犬蟻亞擬家蟻亞科Pseudomyrmecinae†蜂蟻亞科†Sphecomyrminae†幽冥蟻亞科†Haidomyrmecini†阿爾曼蟻亞科†Armaniinae，有些文獻將牠列入蟻科，有些則獨立一科，但無論如何，該亞科都在蟻總科（Formicoidea）之下目前已確定有21亞科283屬，11700種。茲舉幾種以說明:(一)行軍蟻:牠們會一直移居，並且吞食路上遇到的獵物。在移居過程中，1.又名弓背蟻屬或木匠蟻屬，是蟻科山蟻亞科的一個屬。2.牠們不喜歡築巢在乾燥的地方，而巢穴可能建於房屋的橫樑、地板或巢穴中。牠們是雜食性的螞蟻，以食物碎屑或其他昆蟲為食，也會食3.巨山蟻屬的成員眾多，台灣目前本屬有十七種，巨山蟻屬以牠們獨特的習性聞名。牠們會在木材裡挖洞，並以這些通道為生活重心，因為牠們這種特殊的生活習性，所以規模較大的巨山蟻群落會造成房屋結構性毀損，但通常不像白蟻破壞過的那麼嚴重。4.一個成熟的巨山蟻群落一般會有一隻蟻后，許多待孵化的卵，超過兩千隻以上的工蟻，和一個只有工蟻的附屬群落。5.目前發現的蟻后體型最大者為1.91公分。而屋裡的巨山蟻可能是來自主群落或附屬群落，例如：巨山蟻可能來自戶外已枯萎的大樹，陸6.本屬的膨咕巨山蟻跟蜜瓶蟻一樣有貯蜜蟻階級(蜜瓶蟻中的貯蜜蟻又稱蜜缽階級工蟻)它們會收集糖蜜並儲存它們。而在鬧飢荒時，這些貯蜜蟻就派上用場了:牠們會把這些花蜜回哺給同伴食用。7.被巨山蟻在裡面築巢的木材會有木屑遺落在附近，而被白蟻破壞過的木材則會有含泥類物質，當發現附近有上述的木屑(通常會包含已死亡的螞蟻和部分被巨山蟻吃掉的昆蟲殘骸)，就代表附近可能有巨山蟻的巢穴。巨山蟻喜歡築巢在較潮濕的木頭中。(三)針蟻:這種螞蟻包含世界最大的螞蟻:巨人恐針蟻。牠們是少數沒有蟻后的螞蟻，交配過的工蟻會取代蟻后，可以透過解剖牠(四)蜜瓶蟻:牠們的部分大型工蟻會吸飽蜜，在鬧飢荒的時候回吐花蜜，因此被稱為蜜缽階級工蟻。牠們也是澳洲居民的美食，澳洲居民會從牠們的屁股吸取花蜜，但是被人類吸食的蜜瓶蟻也會因為人類破壞式的取(五)火蟻:火蟻屬中有惡名昭彰的入侵紅火蟻，但是火蟻屬中，在台灣已發現1.獵食紅火蟻：生活在森林邊緣，都市綠地、海邊、草地、農地等。在石頭下或土中常常可以發現牠們的蟻巢。牠們是多后制體系的螞蟻，成熟蟻巢由數隻具生殖能力的蟻后和數百隻工蟻組成。牠們分布在台灣中低海拔地區，牠們的食性是雜食偏素食性，主要食物是花蜜、蜜腺分泌和小型昆蟲屍體。牠們除了。會讓工蟻組成小組出外覓食，還會用蟻賊的方式，偷取附近蟻巢的資源。目前未發現有2.知本火蟻：牠們是一種分布在廣西、湖南、湖北天堂寨和台灣。牠們跟原生於南美洲、中美洲、墨西哥和部分美國南部地區的切葉蟻有親戚關係。牠們原生於美洲的熱帶與亞熱帶地區，後來隨著交通工具入侵印度、非洲、太平洋島嶼等地區。牠們攻擊性較強，會出現主動攻擊人畜與莊稼，甚至會咬破電線，破壞供電系統。牠們的毒腺中含有大量生物鹼，叮咬人時會造成劇烈疼痛。會危害當地的農林業生產、人體健康、公共安全及生態環境。3.入侵紅火蟻：牠們的英文簡稱為RIFA，是Redimportedfireant的縮寫，原分布在巴拉那河流域，由於生活在河邊，所以發展出「蟻筏」，也就是讓工蟻互相咬住對方，變成一個蟻筏，並且把蟻后、卵、幼蟲及蛹放在上面，載牠們渡河。由於牠們體表防水，所以不會淹死。紅火蟻在二十世紀初入侵美國南部，造成美國十二個州超過一億公畝(一百億平方公尺)被入侵紅火蟻佔領，每年損失約數十億美元以上。且目前未發現能夠有效防治入侵紅火蟻的生物。火蟻屬成員的毒液大多含有大量生物鹼，叮咬人時會產生如火燒般的劇痛，這也是牠們被取名為「火蟻」的原因。紅火蟻的成熟群落約有二十萬隻至五十萬隻，所以入侵者往往會被大量的火蟻叮咬。被火蟻叮咬後，大部分的人除了會感到劇痛，還會產生水泡，若把它抓破，則會造成二次性的感染。少部分的人甚至會產生嚴重的過螞蟻是靠氣味溝通的。所以蟻客就是掌握了螞蟻的費洛蒙，才能滲透進蟻巢。有些蟻客寄生在蟻巢內純粹是為了尋求庇護，如寄生家蟻；有些蟻客則是為了取得食物，如:黑隱翅蟲的幼蟲會寄生在蟻巢內，並吞食正牌的螞蟻幼蟲。牠們沒有消滅整個蟻巢，是因為牠們也會自相殘殺。甚至有一種蜘蛛會偽有些植物也跟螞蟻有共生關係。如:有一種植物會分泌蜜露，供螞蟻食用，和讓螞蟻築巢在裡面。而螞蟻則會趕走要吃牠們的動物，並且清除雜草。而這有助於植物生長。形成互利共生的關係。螞蟻可以簡單分為頭胸腹三部分，但是更細微的部分呢?以下是我從網路抓(KahootQ&A、google問卷編1.前測問卷發放-小學生的螞蟻知識知多少?(1)分析問卷中的答對率，並從中了解小學生對螞蟻知識了解的的程度，針對錯比較多的題目，進行內容的蒐集與整理，製作成專題2.螞蟻習性觀察:我透過飼養高雄巨山蟻、希氏巨山蟻，以及臭巨山蟻來更了解螞蟻的日常習性，並藉此驗證我所查到的文獻內容是否正3.校內演講:螞蟻知識推廣-(kahoot機智問答)我於民國一一○年三月三十日星期二舉辦了一場《伍動螞蟻－螞蟻4.後測問卷發放-我了解舉辦一場專題講座是否能夠讓同學更懂螞蟻，所以我在專題講座後請參語的同學幫我填寫回饋問卷，也給我一些一、前測問卷分析-小學生的螞蟻知識知多少?我為了想要了解小學生對螞蟻的認識，因此特地發了這份問卷，並請高年級的電腦老師鍾佑聆老師於電腦課的時候花幾分鐘的時間請大家填寫。下表顯示我於二零二零年十二月二十一日所發出，並於二零二零年十二月三十日回收的問卷共527筆的結果如下，請詳細閱讀。由此表可知，在527人中，有480個人回答了正確答案(正確答案:六隻腳)，21人回答螞蟻有四隻腳，26人回答螞蟻有八隻腳。難度:低由此表可知，在527人中，有491個人回答了正確答案(正確答案:蟻后)，6個人回答兵蟻，9個人回答雄蟻，17個人回答工蟻。難度:低由此表可知，在527人中，有十三人回答蟻后、四百八十一人回答工蟻、四十人回答雄蟻及一百八十二人回答兵蟻(正確答案:工蟻和兵蟻)。難度:低由此表可知，在527人中，有475個人回答了正確答案(正確答案:兵蟻)，7人回答蟻后，36人回答工蟻，8人回答雄蟻。難度:低由此表可知，在527人中，有489個人回答了正確答案(正確答案:雄蟻)，5人回答兵蟻，14人回答蟻后，19人回答工蟻。難度:低由此表可知，在525人中，有192個人回答了正確答案(正確答案:亞全山蟻)，125人回答狂蟻(小黑蟻)，53人回答亞絲山蟻，154人回答都不是。難度:高由此表可知，在523人中，有168個人回答了正確答案(正確答案:亞絲山蟻)，170人回答狂蟻(小黑蟻)，99人回答都不是，85人回答亞全山蟻。難度:高由此表可知，在523人中，有317個人回答了正確答案(正確答案:蜜瓶蟻)，78人回答狂蟻(小黑蟻)，43人回答亞絲山蟻，84人回答都不是，難度:中由此表可知，在523人中，有211個人回答了正確答案(正確答案:火家蟻)，130人回答黃瘋蟻，35人回答蜜瓶蟻，83人回答都不是，63人回答白霜前琉璃由此表可知，在523人中，有110人回答了正確答案(正確答案:蜈蚣)，67人回答青蛙，154人回答都不是，120人回答蜘蛛，72人回答蠍子。難度:高由此表可知，在523人中，有181個人回答了正確答案(正確答案:不是)，342由此表可知，在518人中，有179人回答了正確答案(正確答案:黃瘋蟻)，196人回答白霜前琉璃蟻，62人回答蜜瓶蟻，83人回答狂蟻。難度:高由此表可知，在518人中，有324人回答了正確答案(正確答案:鋸針蟻)，61人回答黃瘋蟻，100人回答狂蟻，33人回答蜜瓶蟻。難度:中1.螞蟻遇到石膏粉與滑石粉時，會因為摩擦力降低而無法爬上斜坡。2.不同種或同種不同窩的螞蟻在相遇時有可能會打起來，也可能會互相忽略，但是在其中一方確定自己有百分之百的勝算時，便會征服另一個群落。(三)專題講座後的搶答活動:後測kahoot問答:螞蟻冷知識知多少?在我進行完螞蟻專題講座後，我用kahoot進行機智問答。以下是我的題1.螞蟻有幾隻腳(選項:四隻腳、六隻腳、八隻腳、十隻腳)2.一般來說，什麼螞蟻會產卵(選項:雄蟻、蟻后、工蟻、兵蟻)3.請問何種螞蟻是由非受精卵孵化而成?(選項:工蟻、蟻后、雄蟻、兵4.以下何者是「蟻客」?(選項:獨角仙、鍬形蟲、黑隱翅蟲(幼蟲)、蟑5.下列何者會捕食螞蟻(選項:草齡幼蟲、蝴蝶、蜜蜂、以上皆非)6.下列敘述何者正確?(選項:螞蟻為群落服務並不為任何好處、所有的螞蟻都具有兵蟻階級、螞蟻群都是雜食性的昆蟲、螞蟻是一種真社會7.螞蟻的一生為何?(選項:卵-幼蟲-蛹-成蟲、卵-若蟲-成蟲、卵-若蟲8.古代有一種「蚳醢」，為帝王食補。請問蚳醢為下列何者?(選項:螞9.螞蟻和下列何者有親戚關係?(選項:蟑螂、白蟻、螳螂、胡蜂)10.白蟻和下列何者有親戚關係?選項:蟑螂、獨角仙、天蛾、胡蜂)11.請問螞蟻透過甚麼溝通?(選項:費洛蒙、聲音、震動、以上皆是)12.請問下列何種螞蟻會使用蟻酸攻擊?(選項:蠻蟻亞科、家蟻亞科、此為我於民國一一○年三月三十號螞蟻講座後發出的滿意度調查問卷:由此表可知，有四個人對於我在台上的表現給三分、二個人給我四分、五個人由此表可知，有四個人對於我在台上的表現給三分、三個人給我四分、四個人由此表可知，有十一個人在這次專題演講後，對於螞蟻知識有更深的了解、零由此表可知，有七個人在這次專題演講後，不會想要進一步了解螞蟻的相關知識或主動閱讀相關書籍、四個人會想要進一步了解螞蟻的相關知識或主動閱讀此為我對同學們發出的專題講座滿意度調查問卷:簡報得稍微有點多，我還沒看完就換下一頁。可以講得再慢一點搶答部分應先設定好平板、電腦，以免讓大家等太久由此表可知，有六個人認為不用改進、兩個人認為可以先把平板和電腦設定好、一個人認為我講太快了、一個人認為簡報上面的字有點多、一個人認為簡根據我發出的表格，有五題難度為低，三題難度為中，五題難度為高，平均難度為中。卻有很多題目連一半的答對率都不到，我覺得是因為我的問卷題目太刁鑽而且太難。下次可以考慮把題目的難度降低且不要出那麼刁鑽，會更我將研究結果統一整理成一個表格，來表達我的結果與討論，還有結論與此次有關螞蟻的獨立研究，我主要是以文獻探討中內容分析的方式呈現簡報。並且加入問卷測試小學生對於螞蟻知識的了解程度。以下是我針對文獻探討、前測問卷、講座推廣和後側問答的結果、結論和建議製作的表格:一、維基百科https://reurl.cc/6yV2xO二、超罕見的詭異螞蟻！澳洲原住民最愛的甜食「蜜罐蟻」，號稱行走食物櫃https://reurl.cc/R1Dao6三、臺灣生命大百科:獵食火家蟻https://reurl.cc/7oLnKN四、知本火蟻https://reurl.cc/r84Ga4五、認識火蟻https://reurl.cc/e87lGb六、巨山蟻屬https://reurl.cc/Q3xlpZ七、下課花路米第749集-小小螞蟻大世界(上)https://reurl.cc/VXVoW6八、下課花路米第750集-小小螞蟻大世界(下)https://reurl.cc/OqWom9九、下課花路米第1181集-台灣還有紅火蟻?https://reurl.cc/odG3K3十、流言追追追第51集-螞蟻剋星https://reurl.cc/9X8A5j流言追追追第4集https://reurl.cc/KjZg3e螞蟻的構造https://reurl.cc/pmndVZ螞蟻亞科列表https://reurl.cc/5ox9qv"}, "1676210716.6615682": {"單位": "", "標題": "(ㄧ)、研究計畫之背景與目的", "作者": "", "發布日": 0, "摘要": "", "全文": "自然語言生成(NaturalLanguageGeneration)將是未來深度學習領域之熱門研究議題。2022年年末ChatGPT語言模型所帶來的示範性與展示效果預期將吸引更多研究目光。而個人於中興大學之研究團隊近年於自然語言生成之耕耘已建立初步研究成果。在過去朝著自然語言生成方向前進，我們探索自動生成試題之研究方向前進。於前期研究計畫「基於深度學習語言模型之閱讀測驗自動化生成研究109-2221-E-005-058-MY3」中，我們著眼於基於深度學習模型進行閱讀測驗(ReadingComprehensionTesting)題型之自動出題。在計畫執行過程中，我們•Ying-HongChan,Yao-ChungFan:KeywordProvisionQuestionGenerationforFacilitatingEducationalReadingComprehensionPreparation.INLG(InternationalNaturalLanguageGen-erationConference)2022.•Ho-LamChung,Ying-HongChan,Yao-ChungFan:ABERT-basedDistractorGenerationSchemewithMulti-taskingandNegativeAnswerTrainingStrategies.EMNLP(EmpiricalMethodsinNaturalLanguageProcessing)Findings2020.•Shang-HsuanChiang,Ssu-ChengWang,Yao-ChungFan:CDGP:AutomaticClozeDistractorGenerationbasedonPre-trainedLanguageModel.EMNLP(EmpiricalMethodsinNaturalLanguageProcessing)Findings2022.•Hsien-YungPeng,Ho-LamChung,Ying-HongChan,Yao-ChungFan:MisleadingInferenceGenerationviaProximalPolicyOptimization.PAKDD(AdvancesinKnowledgeDiscoveryandDataMining-26thPacific-AsiaConference)2022此外我們也將前期計畫之自動出題研究成果進行實作，可於以下兩個網址進行實測。•問句生成QuestionGeneration：給定文章、指定片段，透過自然語言生成模型進行問句生成。目前系統可於https://classic.queratorai.com/進行實測。•克漏字干擾選項生成DistractorGeneration：此一部分針對給定之文章進行克漏字誘導選項生成。目前系統可於：https://cdgp-demo.nlpnchu.org/進行實測。基於過去之研究成果累積，本計劃延續自然語言生成技術於自動出題之研究成果，擬定知識類型選擇題(MultipleChoiceQuestion,MCQ)自動生成為目標。以四技二專統一測驗考試為例(如Table2所示)，知識類型選擇題將包含題幹與四個選項(其中包含正確答案與干擾選項)，主要針對某一知識範圍進行測驗。我們認為知識範圍選擇題之自動生成可分為兩部分：(1)問題題幹(Stem)生成與(2)干擾選項(Distractor)生成。題幹生成主要目標為選擇題問題本身之生成，而干擾選項則著眼於誘導選項之生成。知識類型選擇題為學習測驗最常見的測驗模式之一。對比於閱讀測驗生成，知識類型選擇題的自動生成，將可減輕教師準備個人化教學測驗之負擔，也為學生提供更有效率之學習檢測方式。以現有我們成果為基礎，一個基本之自動選擇題出題系統作法為(如圖1)所示：我們可利用資訊檢索技術(如BM25Retriever)，從給定之文集資料庫(目前使用2021中文維基百科所有中文文章)中檢索主題字詞相關文章後，結合前期計畫所開發之問句生成模型(QuestionGeneration)[4]生成題幹，而後再利用詞向量模型(例如word2vec模型)找出相近字詞以為干擾選項。我們將這樣的構想先行實作一系統(稱之為Retrieve-and-GeneratePipeline)，目前此系統可於https://mcq-demo.nlpnchu.org/使用。使用者可藉由輸入主題相關字詞，選擇題生成系統將自動生成輸入字詞相關之選擇題結果。舉例而言，我們以「骨髓、白血病、淋巴」為主題字詞輸入至系統。我們挑選較佳之成果顯示於Table1。自動生成選擇題AutomaticGeneratedMCQQ1:哪一種癌症種類的統稱中可以看出它會造成不正常白血球的大量增生?(a)白血病、(b)乳腺癌、(c)腎病、(d)帕金森氏症Q1:白血病的發病原因通常是發生在哪一個器官上?(a)肝臟、(b)淋巴、(c)甲狀腺、(d)骨髓Q:白血病是最常見的兒童癌症，其中80%的病例是慢性的哪一種性病變?(a)淋巴結、(b)微血管、(c)淋巴、(d)黏膜Table1:目前選擇題自動生成結果範例。目前系統可於https://mcq-demo.nlpnchu.org/。目前我們挑選較佳之生成結果顯示於此Table中。建議審查委員自行輸入關鍵字詞組，來觀察我們Figure1:Retrieve-and-Generate基本架構。本例中我們以”骨髓、白血病、淋巴”為關鍵詞進行文章檢索，而再使用”骨髓”為答案，分別生成問句與干擾選項。從展示之結果中，我們發現自動選擇題出題之可能性。但仔細觀察會發現現有系統離實際應用仍有空間須待努力。也因此觸發本期研究計畫之提出。具體而言，我們認為選擇題自•句子等級之干擾選項生成(Sentence-LevelDistractorGeneration)首先，關於干擾選項的部分，若仰賴詞向量模型來產生干擾選項，我們所能取得的干擾選項便僅限於單詞(Word-Level)等級。從統測試題中，我們發現許多題目之干擾選項屬於句子等級(Sentence-Level)。以Table2中Q3與Q4選擇題為例，我們可以看到選項皆為句子等級之描述，此為目前基於詞向量來產生干擾選項的做法所不能。•反事實干擾選項生成(Counter-FactualSentenceGeneration)我們觀察到選擇題題型中，有著下列何者為錯誤或正確類型之考題。其干擾選項是一個較長的描述句。以Table2為例，Q5與Q6為分別詢問下列選項何者正確/錯誤。此題型，為變像之是非題。除了句子等級生成外，核心技術牽涉的是反事實(Counter-Factual)句子之生成。如何生成此類型之句子以為干擾選項也將是研究重點所在。•選擇題生成品質提升(MCQQuestionQualityImprovement)此外，我們也看到retreive-and-generatepipeline的做法，所產生的題幹本身有時是一個過於空泛之問題(例如，下列何種疾病為兒童常見之癌症？)或者並非語意正確之語句(例如，中樞性的什麼症狀是缺乏抗利尿激素的?)。如何有效判斷生成不良之語句，減低人工進一步過濾/挑選題目之負擔也是需進行研究之議題。此外，基於詞向量之干擾選項作法也有待商榷;隨機從相近詞向量挑選distractor，是否能確保distractor之誘導品類型I:單詞等級的干擾選項Word-LevelDistractorsQ1：淋巴器官是人體防禦系統中重要的一環，可提供淋巴細胞發育或活化所需要的條件與狀態。下列何者是淋巴球進行防禦與活化的器官？(a)骨髓(b)胸腺(c)肝臟(d)脾臟Q2：健康人體感覺神經傳入大腦的訊息大部分經過轉接，(a)視丘(b)下視丘(c)中腦(d)橋腦類型II:句子等級的干擾選項Sentence-LevelDistractors(a)無就巢性(b)體型大(c)著肉多(d)耐粗食Q4：下列何者為「聯合國國際糧農組織」(FAO)成立最重要的目標及首要任務？(a)消除及戰勝飢餓(b)聯絡及交流農業資訊(c)自然環境的永續及保護(d)實現國際間的公平貿易類型III:句子等級且與事實相違背的干擾選項Counter-FactualDistractorsQ5：有關人類上皮細胞的細胞週期之敘述，下列何者正確？(c)後期，分離的染色分體往細胞中央移動(d)末期，細胞膜內凹且細胞質開始一分為二Q6：有關一健康成人泌尿系統功能的敘述，下列何者錯誤？(a)腎臟每日約產生濾液180公升及尿液1.5公升(b)腎小管藉壓力差方式再吸收葡萄糖回微血管(c)腎臟藉排尿量多寡來維持體內水分的恆定(d)若出現尿毒症狀時，可使用血液透析方式進行治療Table2:四技二專統一測驗選擇題類型概觀。Q1與Q2擷取自110學年度護理基礎生物考科，Q3與Q4110學年度農業群農業概論考科，Q5與Q6取自110學年度農業群基礎生物考科質？也許所挑選的distractor並不具誘導性，使得受測者很輕易地可挑選出正確答案，或者挑選到答案的同義詞，造成一題有多個正確答案。根據先前計畫執行訪談教師所知，於測驗時為考量受測全體鑑別度，教師出題時會考量干擾選項彼此之間之難易度；通常三個干擾選項中，會有一個離正確答案較遠之干擾選項(誘導性較低)，一個次之，一個最為困難最具有誘導性。藉此來有適當的鑑別度。目前我們直接使用詞向量之作法並未考量此設計。•主題題組生成(MCQQuestionGroupGeneration)目前我們的系統僅為單一關鍵字詞之出題。我們認為一個良好的自動出題應該是以題組為單位進行出題；使用者指定一主題知識，系統根據此一主題知識範圍進行5-10題之選擇題自動生成，題目之間也應該彼此考量。目前Retreive-and-Generatepipeline的做法，我們使用使用者給定的關鍵字為答案，進行題目之生成。然而比較好的做法應為將使用者給定的關鍵字集合為知識範圍之指定，系統根據該範圍自動挑選重要概念關鍵字進行答案挑選，而後再進行選擇題題組之生成。針對上述之研究挑戰，於本研究規劃中我們擬定下列八項研究議題進行探討。•研究項目ㄧ：基於語言模型與知識圖譜之Text2Text干擾選項生成模型(DistractorGenerationbasedonText2TextLanguageModelandKnowledgeGraph)Figure2:Text2Text干擾選項生成架構Figure3:基於Masked-LM之反事實干擾選項現有之Distractor生成研究多是基於CandicateGenerating-and-Ranking架構，先透過知識庫(如Wordnet)或者透過MaskedLM的方式取得候選字後，再適當進行候選詞特徵排序以為最終干擾選項。對此我們有不同之想法，我們預計嘗試把干擾選項生成設定為一個Text2Text任務；如圖2所例，預期的輸入為題幹Q與答案A之串接字串，透過ModelEncoder與Decoder轉化輸出為一連串之干擾選項字串“重組DNA技術[SEP]電腦斷層掃描[SEP]組織培養”。我們透過語言生成模型直接將輸入對比轉換為輸出。此外，透過此一Text2Text方式，生成句子等級的干擾選項也將為可能。再者，近年來語言模型研究趨勢除了將模型參數量與訓練資料持續增加外，引入知識圖譜來強化生成模型於常識理解與推理能力也成為研究社群的關注所在[5][13]。也因此，探索整合知識圖譜於選擇題自動生成也因此成為我們規劃之研究項目。希冀藉由整合知識圖譜來提升干擾選項之誘導性與題幹生成之正確性。•研究項目二：基於Masked-LM之反事實干擾選項生成研究(Counter-FactualSentenceGenerationbasedonMasked-LM)如同前言所述，“下列何者正確/錯誤”題目為常見之類型選擇題題型。對此，我們預計嘗試以下之作法。首先，我們假定可取得正確敘述之語句S。我們先針對語句S中的字詞進行命名實體辨識(NamedEntityRecognition,NER)，其後再利用語言模型針對所取得之命名實體進行WholeWordMasking(WWM)，填入機率較低之填空結果，藉此生成反事實描述之語句。如Figure4所例，我們將所給定之事實描述，藉由NER方式挑選”葡萄糖”一次進行Masked，再藉由語言模型推測該Masked字詞，並挑選機率相對較低之預測填入。藉此創造出反事實之陳述句。然而，如何確保不挑選到同義詞？要進行多少個字詞之Masked？如何自動從主題文章中挑選事實陳述句以進行改寫？等等•研究項目三：基於知識圖譜強化之反事實干擾選項生成(Counter-FactualSentenceGen-erationbasedonKnowledge-GraphAugmentedLanguageModel)針對反事實干擾選項生成之議題，除了研究項目二之Masked-LM作法外，我們另一個構想為利用知識圖譜來強化Counter-Factual語句之生成。如Figure4所示，我們基本想法為先萃取與題幹相關聯之知識圖譜子圖(LocalSubgraph)並取得知識三元組，以此輸入至語言模型中，訓練語言模型反向改寫三元組語句，以此為反事實干擾選項。然而，此模式之訓練資料該如何取得？如何從知識圖譜上萃取子圖，並生成合適三元組？如何引入GraphNeuralNetwork來萃取結構？為此研究項目需進行探討之工作內容。•研究項目四：基於對比式學習之干擾選項生成模型強化(DistractorGenerationModelbasedonContrastiveLearning)我們過去的研究經驗[8]與相關研究[35]皆指出當使用Generative語言模型來生成句子Figure4:基於知識圖譜強化之反事實干擾選項生成構想層級之輸出時，模型經常會產生語意之相同，但字面的結果。如Table3所示，選項A跟C雖然字面敘述上不同，但實際上都是描述同樣的意思。生成重複選項將導致選擇副交感神經興奮會導致使心跳加速，血壓上升Table3:模型生成的誤導選項重複出現的範例對此，我們預計探索對比式學習(ContrastiveLearning)[7][35]來提升生成品質。對比式學習於自然語言之應用中主要想法為透過選取PositiveSample以及NegativSample使得模型去學習到生成句子跟同義句子的相同以及跟異類句子的不同之處，這樣我們可以使用重複選項當作我們的NegativeSample，然後資料集中的干擾選項當作PostiveSample，期望模型可以學習避免生成出重複的誤導選項。因此，我們預計探索對比式•研究項目五：使用Open-DomainQuestionAnswering錯誤結果之干擾選項挑選研究(Us-ingIncorrectAnswersfromODQAasDistractors)而除了利用生成模型產生Distractors外，我們發現利用Open-DomainQuestionAnswer-ing(ODQA)模型所產生的錯誤結果是一個值得探討的干擾選項的取得方式。具體而言，現有的ODQA模型主要包含一個Retriever與一個Reader，Retriever負責檢索相關的文章，而Reader利用檢索出來的文章利用閱讀理解模型(ReadingComprehensionModel)來找尋答案片段。由於ODQA並非完美，因此也會有錯誤結果之回報。但我們發現錯誤結果與正確答案並非天差地遠，閱讀理解模型所回報之結果通常也與答案接以圖5為例，當我們下達Q：「下列何者為目前世界上應用最廣的殺草劑，其具備有生長素之特性，常用於闊葉雜草防治，亦為選擇性殺草劑的一種？」，ODQA系統將取得圖5中右半部之答案：「萌前除草劑、2,4-D、非選擇性除草劑、10殺丹粒劑」。其中，2,4-D為正確答案，餘者為錯誤答案。然而我們覺得這些錯誤答案，可以當作Q之干擾選項來使用。有別於基於生成模型研究項目，利用ODQA模型之錯誤答案來當作干擾選項應可提供不同思路進行干擾選項生成。這樣的想法乍看可行，但仍有若干技術點需進行探討。諸如，如何控制ODQA模型之錯誤？我們需要在Reader部分弱化Reader的能力，或者弱化Retriever能力，來製造ODQA的錯誤？如何從錯誤回報結果挑選干擾選項？這些皆是尚待深入研究之議題。也因此觸發本研究項目之規劃。Figure5:利用ODQA模型之錯誤答案來當作Distractors•研究項目六：基於NodeCentrality考量主題涵蓋性之選擇題題組生成(MCQGroupGenerationconsideringTopicCoveragebasedGraphNodeCentrality)上述所規劃的選擇題生成做法，主要針對單題進行生成。若要產生多題/題組考題，就是重複相同流程，獨立產生個別考題後，整合為一題組。然而，題組生成做法應當考慮題與題之間之關聯性與互補性。因此，我們認為比較好的做法應當為根據使用者所指定的主題，透過(1)知識圖譜子圖萃取來涵蓋所指定主題之內容與(2)透過主題重要概念萃取：藉由知識圖譜上NodeCentrality方式來萃取關鍵字，以為題幹之生成。此外，如何根據主題互補性，來挑選個別生成之題組，用以組成選擇題題組也是需進行•研究項目七：考量題幹可用性與干擾選項間關聯之Retrieve-and-Generate架構優化(Op-timizingRetrieve-and-GenerateFrameworkconsideringStemUtilityandDistractorCorrela-tion)本研究項目著眼於優化現有Retrieve-and-Generate架構設定。Retrieve-and-Generate架構雖然簡單，但我們可以發現其於單字等級的干擾選項生成已有一定的可用性。我們預計可優化之處為：(1)我們發現引用現有系統於題幹生成時，會有題幹生成結果不佳的狀況。對此，我們發現其中的一個原因為Retriever所檢索之文章品質不佳(文章過短或者與主題不相關)，導致不良之題幹生成。因此如何改善檢索機制來挑選較佳的文章用以進行生成，便是值得探討的課題。再者，題幹生成不佳也有部分原因是來自於問句生成模型本身之生成錯誤，如何有效過濾生成不佳的題幹也因此是一個可需進行研究之項目。(2)如同前言所述，干擾選項之挑選應該考量彼此間之關聯，而非獨立挑選。如何納入此觀點來進行干擾選項之生成也因此成為預計關注之議題。又再者，我們基於word2vec方式之字詞等級干擾選項挑選是否可以引入語言模型之詞向量來求取•研究項目八：生成難度可控之選擇題生成研究Difficulty-ControlledMCQGeneration發展自動化生成考題之可能願景為：針對個人化學習自對應生成難度適中之測驗，以增加學習與檢測之效率。因此，如何針對現有技術探討生成結果難度控制也因此成為一可探索之研究議題。由於我們的基礎架構為基於文章檢索來取得包含使用者所給定的關鍵字之文章，並使用該文章與關鍵字為問題生成模型之輸入，來生成選擇題問句。我們若挑選較為冷門之文章進行生成，則預期可以生成較為困難之題幹。而就干擾選項而言，若我們基於研究項目五「使用Open-DomainQuestionAnswering錯誤結果之干擾選項挑選研究」，利用ODQA來取得干擾選項之作法。我們可藉由調整輸入給閱讀理解模型之文章來產生難度較高的錯誤。具體而言，給定一有包含正確答案A之文章C，我們先將答案片段A於文章D中刪除，而後將其輸入至閱讀理解模型，藉此產生錯誤之閱讀理解結果，以這樣的錯誤結果當干擾選項預期將有較高之誘導能力。相類似的想法也可以套用為挑選Retriever認為相關度比較不高的文章以為閱讀理解模型之輸入。此外，就生成題目之廣度與深度而言，我們可藉由控制取得之文章來調配題組之廣度與深度。例如，若我們以單篇或少數篇文章進行生成，我們所生成的主題題組針對測驗內容而言，應該可產生知識性較為深入之試題。而若針對較多篇文章，則應可產生範圍較廣之測驗試題。上述之研究議題，乍看可行但仍有許多細節需進行探索，如上述研究背景目的所述，本研究計畫擬探索基於現有Retrieve-and-GeneratePipeline之選擇題自動生成架構進行強化。我們以三年為期，規劃在此目標下擬定八項關鍵技術進行研究探討。如Figure6所示，八個工作項目又可區分為三項主軸：「選擇題生成品質提升」、「反事實選項生成」與「群組生成與難度控制」。其中，於「選擇題生成品質提升」研究主軸中，我們探索利用語言模型與知識圖譜架構來生成，將選擇題選項生成問題設定成為一Text2Text任務，輸入為問題題幹與正確答案所串接之TextSequence，而輸出為所生成之distractor串接之TextSequence。此外，我們也嘗試引入對比式學習來改善Text2Text生成模式之品質。我們也預期規劃探索利用ODQA方式來取得干擾選項。而於「反事實選項生成」研究主軸中，我們探索反事實陳述句之生成。我們預計嘗試基於MaskedLM取代重要名詞之反事實語句生成作法與基於知識圖譜知識三元組萃取改寫之作法。於第三個研究主軸中，我們則嘗試針對題組生成與難度可控制之干擾選項生成進行研究。MultipleChoice,Multi-此資料集特色在於為Multi-hop形式，答案需從複數文檔中推理而成；資料來自維基百科以及PubMed，PubMed為生物醫學論文的搜尋引擎，在兩個來源各自提供51,318(Wiki)和2,508(PubMed)筆問題。資料來自國高中英文考試，提供超過10萬筆問題以及2.8萬篇文本，題型上為英文閱讀測驗，會需要閱讀故事或對話紀錄，而針對該文本會有複數題目，每個題目此資料集的挑戰在於，內容來自於7個不同領域(新聞、小學自然科學、旅遊、小說故事等等)，為文本與題目提供多樣性，總計約1萬筆問題與800篇文本。MultipleChoice,Open-為Open-domain形式，並不提供文本，只有問題與答案；包含12,247筆問題、一個正確答案和四個干擾選項；題目類型為知識與常識題，來自於ConceptNet與同時包含兩種形式資料，填空題與Open-domain；填空題形式命名為QUASAR-S，其中資料來源自StackOverflow，提供約37K筆問題。Open-domain形式命名為QUASAR-T，提供約43K筆問題。來自StanfordUniversity的閱讀理解資料集，包含了超過100K筆問題和536篇對應的維基百科文本；延續了SQuAD1.1的題目外，[31]另外加入約50K筆無答案的題目，進一步測試機器能否在無對應答案時回答否，而MedQA是基於美國醫師執照考試(USMLE)所產生的資料集，其中問題皆為較為困難的專業醫學相關的知識，且包含了英文、簡體中文、繁體中文分別有約12K、34K、14K個問題集，這種專業的知識能使機器針對單一領域產生更具參考價值的distractor，目前此任務排名上最佳為ValentinLiévin團隊做出的Codex5-shot訓練機器學習模型進行閱讀理解(MachineReadingComprehensionTask)為最常見的深度學習自然語言任務。諸如，SQuAD[32]、RACE[21]、DROP[10]與HotPotQA[41]等資料集，皆是測驗/訓練機器學習模型於答案找尋、邏輯推理與摘要之能力。其中，許多機器閱讀理解任務採用多選題方式進行測驗。選擇題的自動答題與選擇題的自動生成為一體兩面之應用。因此具有選擇題樣態的資料集為我們選擇題自動生成研究之根本資料集，我們將目前所探知之相選擇題與干擾選項生成之相關研究可按應用類型可區隔為英文學習測驗與知識學習測驗。干擾選項生成研究於英文學習測驗應用，主要以自動生成閱讀理解與克漏字類型的考題為目標。閱讀理解之干擾選項生成與本研究計畫之知識類型選擇題之自動出題目標不同。因此本研究回顧中，我們不做此分類每一研究細部說明。我們將此類型之相關研究按不同象限做分類：(1)干擾選項層級之差異(生成字詞等級與句子等級之差異)象限，(2)題目類型(閱讀理解與克漏字測驗)，(3)生成方式(CandidateRanking與Generative)。我們按此三象限，將目前之相關研究摘要整理於Table5中。而知識學習測驗類型的選擇題生成，則於這一兩年逐漸吸引相關研究投入。我們認為與本研究計畫較為相關之研究有[22][40][3]，以下我們針對個別文獻討論如下。首先，與我們有相同目標者為[22]，也是針對特定知識主題生成選擇題。其主要作法Table5:現有閱讀測驗與克漏字干擾選項生成研究整理表為：先由人工給定一個主題，程式使用該主題名稱的維基百科頁面作為起始點(例如給定“體育”後搜尋體育相關維基百科)，保留熱門文章並取出文章的第一段擷取摘要，由此產生大量句子。並將這些句子依單純英文文法規則生成簡單5W問句與答案，例如who問句是將第一個動詞前的名詞作為正確答案並從句子中移除，what問句是將第一個夾在動詞間的名詞移除作為正確答案並從句子中移除，移除答案後的句子再帶入問題模板生成制式問句。生出五種問句後再依英文文法的詞與詞的相關性去決定哪個問句最適合。在干擾選項生成的部分，先隨機選取其他句子所產生的答案，再藉由語言模型去排序與問題的相關性，取前三高之預測為干擾選項。但本論文並未使用問句生成模型，採取之作法也與本計畫所規劃有所不同。於論文中[40]，則探討如何讓干擾選項更加多樣且與正確答案相差不遠(以具誘導性)。對此，創造了一款監督式分類任務來找出文章中屬於關鍵句子(及對產生干擾選項生成有用的句子)，作者在訓練之前會將訓練用的干擾選項和文章中的每個句子做比對，若該句子和干擾選項相似度達到一定程度，則該句子將成為文章中的關鍵句，透過這個方式在文章中能被修改成為新的干擾選項的句子將會達到多樣性的效果，且透過相似度對比，干擾選項也不會跟原始文章相差太遠從而提升干擾選項的品質。本研究之作法可供我們借鏡參考對關鍵文章/句選擇設計，用以提升干擾選項之多樣性。而與本計畫研究項目五「使用Open-DomainQuestionAnswer錯誤結果之干擾選項」走非生成想法相似之研究為[3]。其主要想法為作者發現於干擾選項資料集中，有許多干擾選項一再被使用。促使論文作者想為新的問題檢索以重複使用干擾選項。作者提出一個Transformer-baseddistractorretrievalmodels，該模型通過考慮問題的上下文來選擇干擾選項模型包含三個部分，分別為D-SIM模型、Q-SIM模型以及DQ-SIM模型。在D-SIM模型中使用mBERT計算同一個MCQ中問題和答案以及對應的干擾選項的向量，並計算他們之間的相似度。而在Q-SIM模型中作者假設不同問題共享一個或多個干擾選項或答案，代表問題可能在語意上相關，因此他們相關的干擾選項可以用作彼此的良好候選干擾選項，因此作者將訓練資料重新排序，使這些共享至少一個干擾選項或答案的問題聚集在一起，接著將資料送進Q-SIM模型計算不同的MCQ中問題之間的相似度。在DQ-SIM模型線性結合前兩個模型，從各自的優勢中獲益，透過基於分數和基於排名的合併策略計算來自D-SIM模型和Q-SIM模型的分數，並設計演算法使得高排名之干擾選項有較大的權重。重複使用現有資料集中之干擾選項為一創新想法，但仍與我們之ODQA做法有所不同。[OpenDomainQuestionAnswering相關研究]本計畫的選擇題生成架構主要基於兩元件:(1)文章萃取與(2)問句生成。目前我們所使用的文章萃取機制為傳統之BM25檢索。而近年來由於ODQA研究領域之發展，基於深度學習之DenseRetrieval技巧也紛紛於ODQA研究領域中所提出。我們預期DenseRetrieval技術也將於我們知識類型選擇題自動出題中擔任關鍵角色。茲簡要回顧現有ODQA之研究如下。ODQA架構中Retriver之重要性不言可喻。不同層級的語意檢索也影響著ODQA的效能。於[28]研究中，作者提出一個簡單而有效的pipeline系統，研究段落和句子級別語義檢索及其對下游任務的潛在影響，作法上是先檢索段落級別的段落，然後從上一步檢索後取得的段落中再去檢索句子級別的段落。該系統在事實驗證和Multi-hopQA方面進行評估，實驗結果顯示QA任務與事實驗證皆仰來上游檢索之結果。於[23]論文中，作者在ODQA架構的retreiver端提出了一個有效率的弱監督式學習的方法(InverseClozeTask)來對retreiver進行預訓練，給予模型完全無人工標記的維基百科原始資料，藉由論文中的方法將資料切分成固定長度的段落，並隨機選取其中的一個句子當作標籤，讓模型去學習如何從給定的句子去資料庫中找尋與之最相關的文章。在實驗數據中此方法訓練出的ODQA模型分數高於傳統的BM25檢索方法。而於DPR[16]論文中，作者為解決傳統BM25無法檢索語意相似段落的問題，提出了基於深度學習模型之DensePassageRetrieval，並提出使用in-batchnegative的技術，在密集向量的檢索取得極佳的效果改進。我們也預計引入中文DRP模型來提升目前題幹生成所需之而於GAR[27]研究中，作者們探討傳統基於BM25的檢索無法檢索沒有單詞重疊或語意相似的段落的問題，而基於密集向量的檢索方式在計算上又過於複雜，且會有資訊損失的問題。該論文透過預訓練語言模型自動生成與查詢語句相關的內容，可增加查詢語句的資訊量，隨後透過BM25進行檢索，該方法比起傳統BM25能檢索到更精確的段落，且執行效率也快於基於密集向量的檢索方式。透過GAR中的PLM可對查詢語句的資訊進行補充，原論文中是補充答案、相關段落及段落標題，可基於該手法讓PLM生成問題與原始答案的關聯，對這些關聯進行組合或刪除即可檢索與原答案有相似關係的查詢語句，可檢索到適合製作干於[19]，作者觀察到基於語言模型之檢索技術雖然效果很好，但比以往的方法需要極高的計算成本，特別是在計算每個query與document間的relevancescore時。本篇論文作者提出了ColBERT，引入lateinteraction架構，對query與document進行獨立的encoding，於模型輸出最終階段才進行相似度運算。而[18]中，則使用近一步基於ColBERT當基礎模型來進行retreiver端的訓練，使用了Relevance-guidedSupervision(RGS)的方法來訓練retreiver的模型。相較於傳統的retreiver訓練資料，傳統方法需人工標註的訓練資料，但作者認為人工標住資料往往存在偏見，進而影響模型的效能，所以在論文中提出了用訓練好的模型來找尋最適合的訓練資料，並把這些資料用於訓練下一個模型，使用這個方式來進行後續的訓練，最後得出最佳的模型。此方法訓練出的模型分數明顯高於目前opendomain的其他方式。在找尋訓練資料時，模型會先搜尋前k筆資料，並把前20筆資料中含有shortanswer的資料當成possitiveexample，而前20筆以外剩餘的資料中不含有shortanswer的資料當成negativeexample，此方法尋找的負面訓練資料交由reader進行預測能得到與正確答案相關的錯誤答案，如能改良ranking的方式，應用於干擾選項生成，預期將能提升干擾選項之品質。[ContrastiveLearning於自然語言生成相關研究]對比式學習始見於電腦視覺領域，其於影像應用上有著顯著的效果提升。因此，許多自然語言處理之研究者也將其引用至自然語言生成領域，諸如[11][7][35]。於[11]研究中，作者們提出了使用ContrastiveLearning的作法來學習SentenceEmbedding，他們使用BERT作為基礎模型，透過不同的dropoutmask使模型能用同樣的輸入產生不同的SentenceEmbedding，但它們之間的距離會是近的，也就可作為PositiveSample。然後再使用其他in-batch的句子作為NegativeSample，以達到非監督式訓練。而在有訓練資料的情況下，如NLI資料集，則可以將Eentailment視為PositiveSample，Contradiction以及其他in-batch的句子視為Negative而再基於[11]研究後，於[7]研究中，作者們提出加上了Discriminator來加強SentenceEmbedding的學習效果。具體而言，對每筆輸入來說，會透過SentenceEncoder產生一個SentenceEmbedding，同時也會隨機的Masking輸入句子其中的幾個Token，並透過一個fixed的Generator來還原MaskToken，而Discriminator要試著透過沒被修改過的SentenceEmbedding的資訊去辨認修改後的句子中的哪一個Token有被修改過，整個框架會透過SentenceEmbedding的ContrastiveLearningLoss以及Discriminator的ReplacedTokenDetection而與我們預計引入對比式學習來提升句子等級的干擾選項生成效果想法接近的研究也可見於[35]。該研究指出發現了以往autoregressive模型使用BeamSearch去決定要生成的文本時，會產生語意不自然以及沒意義的重複文字，儘管後續有人使用取樣以及修改ObjectiveFunction來嘗試解決這個問題，但這個方法時常會產生語意不連貫的問題。因此作者提出了使用ContrastiveLearningLoss加上原先的MaximumLikelihoodEstimationLoss來訓練模型，並且使用ContrastiveSearch來作為decodingmethod。[Counter-FactualSentenceGeneration相關研究]於Counter-FactualGeneration研究中，[44,33]為相關研究之先驅。主要想法是使用神經網路來生成反事實敘述，這個概念背後的想法是同時使用語言模型和各種採樣的策略來達成生成反事實敘述的效果。接續之前的成果，[39]提出了Polyjuice架構，可以控制輸出的反事實敘述的類型。另外一種生成干擾選項的研究是使用干擾選項生成(DistractorGeneration)的手法，干擾選項生成(DistractorGeneration)的目標是給予了一篇文章、問題和正確答案後，模型能夠生成似是而非的干擾選項。於研究[24]中，作者最先提出使用了神經網路來生成干擾選項，而研究[8]則延續了前者的成果，把重點放在生成複數的干擾選項並提出了一個負樣本學習策略(negativetrainingstrategy)來加強干擾選項生成。但這上述兩個研究主要都著眼於閱讀理解題型中干擾選項之生成，與本研究知識類型選擇題生成目標所有不同。而在關於AbductiveInferenceGeneration的相關研究中，於[30]中作者提出了故事改寫的任務，此任務目標是要改寫一段故事的敘述並且導致產生與原先不同的結局，與顯永的論文的方式的差異為故事改寫任務是根據給定的誤導推論來生成不同的結局，在MIG任務中是給定敘述句與其前後文來生成誤導推論。[2]研究中是讓模型根據上下文來推論中間適合加入的句子，此研究主要是產生正確的推論敘述，而在MIG任務[29]主要是產生相反的誤導推論敘述。於我們先前的研究[29]中，我們提出一個自然語言的誤導推論生成任務，任務步驟是當給定了一個陳述事實的句子並給定其上下文，將其定義為(x,y,z)與誤導推論ˆy，模型會生成與該事實相反的推論句子，由此可以讓模型產生出多選題似是而非的干擾選項。然而這樣任務與我們所設定之反事實出題仍有所不同。而與我們生成Counter-Factual陳述句想法接近的研究為：[46]。於該研究中，作者探討有別於過往的是非題，將文章中原始句子作為True的問句並通過反義詞替換關鍵字生成False問句，本篇作者通過使用一種新穎的masking-and-infilling策略來生成更靈活與複雜的問句。要被mask的關鍵字找法有很多種，像是SRL結果中的predicate，句子中連接詞(”that”、”when”、”since”)之後的部分或是數字等等，來找出值得提問的句子。Infilling的作法則是以BART為基底作為生成模型，將原始句子的關鍵字mask後，提供前後句子作為BART的上下文後產生句子填入。作者為了找值得提問的句子所提出的方法亦可用來作為多選題產生題目的答案選擇方法，而infilling的想法如果使用得宜，在提供出題範圍的前後文後，也能將欲出題句子挖空讓BART模型填補後產生選項。雖然與我們目標相似，但仍與我整合知識圖譜與語言模型為近年語言模型訓練熱門關注議題。於[42]研究中，作者們提出一CounterfactualGenerationControllableCounterfactualGenerationDistractorGenerationMisleadingInferenceGeneration種能將文本和知識圖譜的資訊進行深度雙向、自我監督的預訓練的方法，透過作者的方法可以將一些特定領域的KnowledgeBase資料集結合相關文獻去訓練語言知識模型，讓語言模型和知識圖譜之間的資訊互相交換，並且在回答特定領域問答的任務上，表現優於現有的語言模型和知識圖譜增強模型。我們想透過作者提供的預訓練模型，來做多選題選項生成，透過知識圖譜所額外提供的資訊，讓在考慮生成選項時，除了題目和答案的文字外，用這些文字從知識圖譜萃取相關的知識圖譜子圖幫助生成選項有更多參考資訊。知識圖譜於問句生成之研究也見於[5][13]。於[5]研究中，作者之想法為使用知識圖譜進行問題生成任務時，通常只會取出資料與關係的三元組合，並不能完整利用知識圖譜中豐富的資訊，因此作者基於GraphNeuralNetworks提出了BidirectionalGatedGraphNeuralNetwork(BiGGNN)，將知識圖譜中的node及edge編碼成向量，使得每個node都包含了周圍其他node的資訊，並對答案的node進行標註，隨後透過RNN進行問句生成，生成過程中RNN會決定生成新文字或直接複製某個node的資訊放進問題。我們應可借鏡此研究論文的想法，可將知識圖譜及答案的資訊完整編碼，可基於此特性，在生成問題時同時考慮答案附近的node，讓模型在建立問題的同時也能根據附近的node產生干擾選項。而於[13]研究中，則提出的ISEEQ架構，主要目的是希望從簡短的敘述句中去尋找更多訊息來反問使用者是否符合關於敘述句背後未提及的情況，在使用時，使用者只需輸入簡短的敘述句，模型會先從知識圖譜擴展並增加query的關鍵字，透過這些關鍵字模型就能retrieve更多相關的文章進行ranking，在從這些文章中使用QG模型產生一些原始query中未提及但可能相關背景知識，例如當使用者說道:“我感到疲勞且渾身無力”，ISEEQ能回覆:“請問病徵持續了多久?”，或是“你最近14天有出國過嗎?”這類可能與其他知識相關的問句。這種方式最大的貢獻及是讓機器學習到”反問”的能力，而非一味的透過query生成出結果。這樣的想法，或許可引用來提升題幹生成的精確度，舉例來說:有位教師想出關於日治時期的歷史題目，我們就能先藉由ISEEQ來詢問是要出與日治時期戰爭相關的題目或是與日治時期建設相關的題目，以此來取得更精確的題幹生成需求。[1]J.Araki,D.Rajagopal,S.Sankaranarayanan,S.Holm,Y.Yamakawa,andT.Mitamura.Gen-eratingquestionsandmultiple-choiceanswersusingsemanticanalysisoftexts.InProceedingsofCOLING2016,the26thInternationalConferenceonComputationalLinguistics:TechnicalPapers,pages1125–1136,2016.[2]C.Bhagavatula,R.L.Bras,C.Malaviya,K.Sakaguchi,A.Holtzman,H.Rashkin,D.Downey,S.W.-t.Yih,andY.Choi.Abductivecommonsensereasoning.arXivpreprintarXiv:1908.05739,[3]S.K.Bitew,A.Hadifar,L.Sterckx,J.Deleu,C.Develder,andT.Demeester.Learningtoreusedistractorstosupportmultiplechoicequestiongenerationineducation.IEEETransactionsonLearningTechnologies,2022.[4]Y.-H.ChanandY.-C.Fan.Arecurrentbert-basedmodelforquestiongeneration.InProceedingsofthe2ndWorkshoponMachineReadingforQuestionAnswering,pages154–162,2019.[5]Y.Chen,L.Wu,andM.J.Zaki.Towardsubgraphguidedknowledgegraphquestiongenerationwithgraphneuralnetworks.arXivpreprintarXiv:2004.06015,2020.[6]S.-H.Chiang,S.-C.Wang,andY.-C.Fan.Cdgp:Automaticclozedistractorgenerationbasedonpre-trainedlanugagemodel.InFindingsoftheAssociationforComputationalLinguistics:[7]Y.-S.Chuang,R.Dangovski,H.Luo,Y.Zhang,S.Chang,M.Soljac￿ic￿,S.-W.Li,W.-t.Yih,Y.Kim,andJ.Glass.Diffcse:Difference-basedcontrastivelearningforsentenceembeddings.arXivpreprintarXiv:2204.10298,2022.[8]H.-L.Chung,Y.-H.Chan,andY.-C.Fan.Abert-baseddistractorgenerationschemewithmulti-taskingandnegativeanswertrainingstrategies.InFindingsoftheAssociationforComputationalLinguistics:EMNLP2020,pages4390–4400,2020.[9]B.Dhingra,K.Mazaitis,andW.W.Cohen.Quasar:Datasetsforquestionansweringbysearchandreading.arXivpreprintarXiv:1707.03904,2017.[10]D.Dua,Y.Wang,P.Dasigi,G.Stanovsky,S.Singh,andM.Gardner.ingcomprehensionbenchmarkrequiringdiscretereasoningoverparagraphs.arXiv:1903.00161,2019.[11]T.Gao,X.Yao,andD.Chen.Simcse:Simplecontrastivelearningofsentenceembeddings.arXivpreprintarXiv:2104.08821,2021.[12]Y.Gao,L.Bing,P.Li,I.King,andM.R.Lyu.Generatingdistractorsforreadingcomprehen-sionquestionsfromrealexaminations.InProceedingsoftheAAAIConferenceonArtificialIntelligence,volume33,pages6423–6430,2019.[13]M.Gaur,K.Gunaratna,V.Srinivasan,andH.Jin.Iseeq:Informationseekingquestiongenerationusingdynamicmeta-informationretrievalandknowledgegraphs.InProceedingsoftheAAAIConferenceonArtificialIntelligence,volume36,pages10672–10680,2022.[14]Q.Guo,C.Kulkarni,A.Kittur,J.P.Bigham,andE.Brunskill.Questimator:Generatingknowl-edgeassessmentsforarbitrarytopics.InIJCAI-16:ProceedingsoftheAAAITwenty-FifthInternationalJointConferenceonArtificialIntelligence,2016.[15]D.Jin,E.Pan,N.Oufattole,W.-H.Weng,H.Fang,andP.Szolovits.Whatdiseasedoesthispa-tienthave?alarge-scaleopendomainquestionansweringdatasetfrommedicalexams.AppliedSciences,11(14):6421,2021.[16]V.Karpukhin,B.Og￿uz,S.Min,P.Lewis,L.Wu,S.Edunov,D.Chen,andW.-t.Yih.Densepassageretrievalforopen-domainquestionanswering.arXivpreprintarXiv:2004.04906,2020.[17]D.Khashabi,S.Chaturvedi,M.Roth,S.Upadhyay,andD.Roth.Lookingbeyondthesurface:Achallengesetforreadingcomprehensionovermultiplesentences.InProceedingsofthe2018ConferenceoftheNorthAmericanChapteroftheAssociationforComputationalLinguistics:HumanLanguageTechnologies,Volume1(LongPapers),pages252–262,2018.[18]O.Khattab,C.Potts,andM.Zaharia.Relevance-guidedsupervisionforopenqawithcolbert.TransactionsoftheAssociationforComputationalLinguistics,9:929–944,2021.[19]O.KhattabandM.Zaharia.Colbert:Efficientandeffectivepassagesearchviacontextualizedlateinteractionoverbert.InProceedingsofthe43rdInternationalACMSIGIRconferenceonresearchanddevelopmentinInformationRetrieval,pages39–48,2020.[20]G.Kumar,R.E.Banchs,andL.F.D’Haro.Revup:Automaticgap-fillquestiongenerationfromeducationaltexts.InProceedingsoftheTenthWorkshoponInnovativeUseofNLPforBuildingEducationalApplications,pages154–161,2015.[21]G.Lai,Q.Xie,H.Liu,Y.Yang,andE.Hovy.Race:Large-scalereadingcomprehensiondatasetfromexaminations.arXivpreprintarXiv:1704.04683,2017.[22]G.LeBerre,C.Cerisara,P.Langlais,andG.Lapalme.Unsupervisedmultiple-choicequestiongenerationforout-of-domainq&afine-tuning.InProceedingsofthe60thAnnualMeetingoftheAssociationforComputationalLinguistics(Volume2:ShortPapers),pages732–738,2022.[23]K.Lee,M.-W.Chang,andK.Toutanova.Latentretrievalforweaklysupervisedopendomainquestionanswering.arXivpreprintarXiv:1906.00300,2019.[24]C.Liang,X.Yang,N.Dave,D.Wham,B.Pursel,andC.L.Giles.Distractorgenerationformultiplechoicequestionsusinglearningtorank.InProceedingsofthethirteenthworkshoponinnovativeuseofNLPforbuildingeducationalapplications,pages284–290,2018.[25]C.Liang,X.Yang,D.Wham,B.Pursel,R.Passonneaur,andC.L.Giles.Distractorgenera-tionwithgenerativeadversarialnetsforautomaticallycreatingfill-in-the-blankquestions.InProceedingsoftheKnowledgeCaptureConference,pages1–4,2017.[26]V.Liévin,C.E.Hother,andO.Winther.Canlargelanguagemodelsreasonaboutmedicalquestions?arXivpreprintarXiv:2207.08143,2022.[27]Y.Mao,P.He,X.Liu,Y.Shen,J.Gao,J.Han,andW.Chen.Generation-augmentedretrievalforopen-domainquestionanswering.arXivpreprintarXiv:2009.08553,2020.[28]Y.Nie,S.Wang,andM.Bansal.Revealingtheimportanceofsemanticretrievalformachinereadingatscale.arXivpreprintarXiv:1909.08041,2019.[29]H.-Y.Peng,H.-L.Chung,Y.-H.Chan,andY.-C.Fan.Misleadinginferencegenerationviaproxi-malpolicyoptimization.InPacific-AsiaConferenceonKnowledgeDiscoveryandDataMining,pages497–509.Springer,2022.[30]L.Qin,A.Bosselut,A.Holtzman,C.Bhagavatula,E.Clark,andY.Choi.Counterfactualstoryreasoningandgeneration.arXivpreprintarXiv:1909.04076,2019.[31]P.Rajpurkar,R.Jia,andP.Liang.Knowwhatyoudon’tknow:Unanswerablequestionsforsquad.arXivpreprintarXiv:1806.03822,2018.[32]P.Rajpurkar,J.Zhang,K.Lopyrev,andP.Liang.Squad:100,000+questionsformachinecomprehensionoftext.arXivpreprintarXiv:1606.05250,2016.[33]S.Ren,Y.Deng,K.He,andW.Che.Generatingnaturallanguageadversarialexamplesthroughprobabilityweightedwordsaliency.InProceedingsofthe57thannualmeetingoftheassociationforcomputationallinguistics,pages1085–1097,2019.[34]S.RenandK.Q.Zhu.Knowledge-drivendistractorgenerationforcloze-stylemultiplechoicequestions.InProceedingsoftheAAAIConferenceonArtificialIntelligence,volume35,pages[35]Y.Su,T.Lan,Y.Wang,D.Yogatama,L.Kong,andN.Collier.Acontrastiveframeworkforneuraltextgeneration.arXivpreprintarXiv:2202.06417,2022.[36]A.Talmor,J.Herzig,N.Lourie,andJ.Berant.Commonsenseqa:Aquestionansweringchallengetargetingcommonsenseknowledge.arXivpreprintarXiv:1811.00937,2018.[37]J.Welbl,N.F.Liu,andM.Gardner.Crowdsourcingmultiplechoicesciencequestions.arXivpreprintarXiv:1707.06209,2017.[38]J.Welbl,P.Stenetorp,andS.Riedel.Constructingdatasetsformulti-hopreadingcomprehensionacrossdocuments.TransactionsoftheAssociationforComputationalLinguistics,6:287–302,[39]T.Wu,M.T.Ribeiro,J.Heer,andD.S.Weld.Polyjuice:Automated,general-purposecounter-factualgeneration.arXivpreprintarXiv:2101.00288,1(2),2021.[40]J.Xie,N.Peng,Y.Cai,T.Wang,andQ.Huang.Diversedistractorgenerationforconstruct-inghigh-qualitymultiplechoicequestions.IEEE/ACMTransactionsonAudio,Speech,andLanguageProcessing,30:280–291,2021.[41]Z.Yang,P.Qi,S.Zhang,Y.Bengio,W.W.Cohen,R.Salakhutdinov,andC.D.Manning.Hotpotqa:Adatasetfordiverse,explainablemulti-hopquestionanswering.arXiv:1809.09600,2018.[42]M.Yasunaga,A.Bosselut,H.Ren,X.Zhang,C.D.Manning,P.Liang,andJ.Leskovec.Deepbidirectionallanguage-knowledgegraphpretraining.arXivpreprintarXiv:2210.09338,2022.[43]C.Y.Yeung,J.S.Lee,andB.K.Tsou.Difficulty-awaredistractorgenerationforgap-fillitems.InProceedingsoftheThe17thAnnualWorkshopoftheAustralasianLanguageTechnologyAssociation,pages159–164,2019.[44]C.Zhang,Y.Sun,H.Chen,andJ.Wang.Generatingadequatedistractorsformultiple-choicequestions.arXivpreprintarXiv:2010.12658,2020.[45]X.Zhou,S.Luo,andY.Wu.Co-attentionhierarchicalnetwork:Generatingcoherentlongdistractorsforreadingcomprehension.InProceedingsoftheAAAIConferenceonArtificialIntelligence,volume34,pages9725–9732,2020.[46]B.Zou,P.Li,L.Pan,andA.Aw.Automatictrue/falsequestiongenerationforeducationalpur-pose.InProceedingsofthe17thWorkshoponInnovativeUseofNLPforBuildingEducationalApplications(BEA2022),pages61–70,2022.於本研究計畫中，我們目標基於自然語言生成技術進行選擇題自動出題研究。我們預期的展示情境為：給定一知識範圍(如農業文集)，我們可於系統直接輸入一組主題關鍵詞，而後自動生成關於該主題相關之選擇題題組(含字詞等級、句子等級、挑選是非類型之選擇題)。然而，如同前言背景中所言，現有技術離目標仍有許多待強化之處，對此我們擬定三大出題目標：字詞等級干擾選項、句子等級干擾選項與反事實干擾選項進行研究。針對此三大目標，我們擬定(1)基於知識圖譜之強化與(2)基於生成語言模型與對比學習之強化三個核心技術進行探討。研究技術項目與項目間之關係與進行規劃如Figure7所示。於第一年度中，我們規劃「考量題幹可用性與干擾選項間關聯之Retrieve-and-Generate架構優化」、「基於語言模型與知識圖譜之Text2Text干擾選項生成」與「基於Masked-LM之反事實干擾選項生成研究」進行研究。「考量題幹可用性與干擾選項間關聯之Retrieve-and-Generate架構優化」著眼於優化目前https://mcq-demo.nlpnchu.org之WordLevel之選擇題生成系統。希冀自動過濾相似或語意不正確題幹，並且考量干擾選項彼此間之關聯進行最終干擾選項集合之綜合挑選。「基於生成語言模型之Text2Text干擾選項生成模型」則考量Text2Text模式利用生成模型直接生成干擾選項序列，藉此嘗試生成句子等級之干擾選項。並進一步地探索結合知識圖譜與語言模型之干擾選項生成，希冀透過知識圖譜之整合來生成更有誘導性且合理之字詞等級干擾選項。此外，於「基於Masked-LM之反事實干擾選項生成研究」項目中，我們預計著手挑戰反事實干擾選項生成，我們嘗試利用透過NER擷取給定事實陳述句中之關鍵字並Masked-LM該關鍵字來填入機率較低之字詞，以為反事實語句生成之作法。我們預期第一年度工作完成後，將可優化現有單詞等級干擾選項之選擇題自動生成系統，並於第二年度中，我們則規劃「基於知識圖譜強化之反事實干擾選項生成」、「基於對比式學習之干擾選項生成模型強化」與「利用Open-DomainQuestionAnswering錯誤結果之干擾選項挑選」。於此年度中，進一步嘗試利用知識圖譜萃取三元組知識後進行反事實語句之生成，基本想法為透過萃取三元組後再進一步輸入語言模型，並希望語言模型適當地進行反事實語句的改寫。此一部分預計使用第一年度之「基於Masked-LM之反事實干擾選項生成研究」之基礎以為訓練資料。對於字詞等級的干擾選項，我們則探索利用Open-DomainQuestionAnswering模型所錯誤回報之答案當作干擾選項。我們希望透過此不同之想法來取得更多樣的干擾選項。再者，如前言所述，生成模型生成序列時常出現生成結果重複的狀況，對此我們嘗試利用對比式來改善生成模型這方面的缺點，以優化干擾選項之生成。第二年度工作項目完成後，我們可初步提供反事實干擾選項之選擇題生成、優化後之句子等級干擾選項生成與基於ODQA方式取得之字詞等級干擾選項生成。於第三年度中，我們則規劃「基於NodeCentrality考量主題涵蓋性之選擇題題組生成」與「生成難度可控之干擾選項生成研究」。如同前言所述，一個理想的選擇題出題系統應該考慮題組生成，整體題目之主題涵蓋性，對此我們也將嘗試基於知識圖譜涵蓋的方式考慮節點centrality重要度方式來進行題組之生成。同時我們也將嘗試題目難易度之控制生成研究。我們的想法是當自動生成成為可能後未來的出題應該是個人化模式；針對個人的學習狀況進行測驗，因此我們規劃「個人化與生成難度可控之干擾選項生成研究」進行探討。第三年度，我們也預計投入人力進行系統的實作開發與整合。以下茲針對擬訂之研究議題其問題挑戰與1.基於語言模型與知識圖譜之Text2Text干擾選項生成模型近年來，語言模型除了使用CandicateGeneratingandRanking架構的MaskedLM之外，使用Encoder-Decoder架構的Text2Text模型於生成任務上也有很好的成果，因此我們的想法是把干擾選項生成Model視為一個Text2Text任務，嘗試讓模型直接生成干擾選項。且使用Text2Text模型生成Distractor有別於用MaskedLM生成Distractor，我們希望模型能生成的Distractor有以下特性(1)非單詞之Distractor:目前在Distractor生成上多為單詞生成，要生成一段敘述/句子的干擾選項還沒有很好的做法。(2)特定domain之MCQ:目前的MCQ多為英文測驗，若想針對特定領域出選擇題，使用MaskedLM生成的Distractor會因語料庫限制而為達到上述的兩種目標需求，我們設Q為輸入之問句，A為輸入問句之答案，並將Q與A字串接字串當作輸入，訓練模型輸出一連串問句Q的干擾選項字串D=(d1,d2,d3)。模型架構如圖2所示，我們將「哪一種技術最適用於嚴重特殊傳染性肺炎之病原快速篩檢?」與「聚合￿連鎖反應」接在一起輸入至Text2Text模型，而模型生成了「重組DNA技術」、「電腦斷層掃描」與「組織培養」三個干擾選項，三個選項皆為醫療檢測方式且都不是單詞，符合如前言所述，我們也預計探索使用知識圖譜來強化生成干擾選項的能力。如Figure8的圖例，我們可以使用語言模型結合醫療知識圖譜，利用給定的文本從知識圖譜中萃取有包含這些topicentities以及相鄰的nodeentities的子圖，再分別將文本與子圖輸入至語言模型和GraphNeuralNetwork(GNN)，讓模型學習找出關鍵的資訊與強化network中重要entitynodes跟edges的權重，藉由GNN的額外資訊與語言模型能力生成更好的選項。Figure8為我們規劃做法的概觀。主要分兩階段進行：(1)關鍵字子圖與GNNEmbedding萃取與(2)GNNEmbedding與LanugageEmbedding間之FusionEncoder生成。具體而言，第一個階段是將從知識圖譜裡萃取相關的知識圖譜子圖。我們一開始先使用NERModel抓取題幹中的entities，再透過這些entities透過知識圖譜去尋找entitynodes以及所有跟entitynodes相鄰3-hop內的bridgenodes，將檢索出來的結果當成知識圖譜子圖，並在後續第二階段提供以Figure9的圖例，我們先將問題透過NERModel斷詞得到”白血病”、”發病”、”器官”與”發病原因”當作entities，再透過這些entities當作關鍵字透過Retriever從知識圖譜萃取Figure8:結合知識圖譜與語言模型生成之構想架構圖與這些詞相關的node和edge，如利用”白血病”與”器官”能找到”肝臟”、”骨髓”、”肺部”等等entities，我們能利用這些entities來當作後續的生成選項。第二個階段則將文本與第一階段得到的知識圖譜子圖分別輸入至語言模型與GNN進行訓練，訓練目標為將Input的[MASK]Token還原來產生干擾選項，先將語言模型的Input額外新增[MASK]Token，經過N層LMLayer做self-attention後，透過FusionEncoder再結合知識圖譜的資訊與原本的文本去生成選項，透過以上的手法，來強化干擾選項之生成。2.考量題幹可用性與干擾選項間關聯之Retrieve-and-Generate架構優化如同研究目的所述，本研究項目著眼於優化現有Retrieve-and-Generate生成架構。我們•文章檢索器過濾:在Retrieving-and-Generating方法中Retriever會檢索回傳含有主題字詞的文章，但有部分文章並不適合當作問句生成模型的輸入，那些文章含有太多的雜訊，我們想嘗試在這步驟先過濾一些文章。我們的想法是利用詞向量中的相似詞再次檢索Retriever回傳的文章，把過於不相關的文章濾掉，減少問句生成模型輸出過於複•生成題幹品質自動過濾：問句生成模型輸出的問句，會再交由使用者挑選，但為了更好的效果，我們想嘗試利用語言模型做初步的篩選，減少人工挑選時間。我們想嘗試利用ODQA模型來過濾問句，將問句生成模型輸出的問句輸入至問答模型後，根據問答模型的回答結果與信心分數來決定該問句是否合適當作MCQ。如Figure10所例，我們將生成之問句再次投入ODQA模型，透過答案選項之信心度來為問句生成過濾之基Figure10:Unanswerablequestionsolution•考量干擾選項間關聯之干擾選項挑選:如前言所述，一個可以提升選擇題生成品質￿考量干擾選項彼此之間之難易度。而在Retrieving-and-Generating方法中，生成干擾選項的方式為取出詞向量模型中與主題字詞相似之詞，使用相似度前三名當作干擾選項，因此有選項過於相近而造成問題太難的可能。為做改善，我們取出所有相似詞的向量與主題字詞向量計算相對距離，設定各個選項與答案的相似度範圍(距離範圍)，再從中隨機挑選相似詞，讓干擾選項彼此之間難易度有所區分以提升試題之鑑別度。•基於語言模型之干擾選項候選生成:雖然詞向量模型生成distractor過程迅速，但如果有主題字詞沒有被詞向量模型看過或有主題字詞一字多意，就無法生成適當的干擾選項。這個問題我們想嘗試使用MaskedLM來解決，我們將Retriever回傳的文章中含有主題字詞的段落記錄下來，移除主題字詞後輸入MaskedLM，再從MaskedLM輸出的候選字中選出distractor，使用這個方式生成的干擾選項不會被上述的缺點受限，也更Figure11:OptimizedRetrievingandGeneratingFramework3.基於Masked-LM之反事實干擾選項生成研究本研究項目為基於Masked-LM技巧來生成反事實干擾選項。我們需要先查找與事實相符之敘述，再修改成反事實干擾選項。我們預計之作法為：首先讓使用者先輸入考題關鍵字，再使用retriever查找相關文章，其目的為從相關文章中選取與事實相符之敘述。接著，將retriever檢索到的文章回傳給使用者，由使用者選取與事實相符之敘述。並按照以下之步驟進行Masked-LM反事實干擾選項生成。為了抽換專有名詞，我們先使用命名實體與事實相符之敘述中的所有專有名詞，以下圖為例”腎小管藉壓力差方式再吸收葡萄糖回微血管”，透過命名實體模型會萃取出專透過命名實體模型，我們會萃取出多個與事實相符之敘述中的專有名詞，我們需要選擇一個專有名詞行抽換。我們認為該專有名詞在相關文章中的重要程度越高，越高機率是考題中的重點。為了確保抽換專有名詞的品質，我們使用BM25來查看各個專有Figure12:AutoEncoder模型與AutoRegressive模型做MaskLM3.使用Maskedlanguagemodel模型進行改寫Maskedlanguagemodel，簡稱為MaskLM，該任務的會將輸入中的部分內容遮擋起來，模型會根據其他未被遮擋的內容，預測出被遮擋的內容或與其擁有相似語義的內容，現有的MaskLM模型分為兩種，分別為AutoEncoder框架的MaskLM模型與AutoRe-gressive框架的MaskLM模型，經典模型如BERT與BART。針對MaskLM任務，兩種模型進行預測的方式也有所不同，AutoEncoder框架的模型會預測出字，而AutoRe-gressive框架的模型會預測出字、詞或是句子，詳細範例如Figure12。根據抽換專有名詞的需求，我們將使用Autoregressive模型做MaskLM任務來抽換重要的專有名詞。然而模型預測出的內容眾多，有可能是原先遮擋的內容、遮擋內容的同義詞或是與遮擋內容擁有相似語義的內容。理想的情況下，模型預測的內容最高機率為原先遮擋的內容，若我們選擇原先遮擋的內容或是遮擋內容的同義詞進行抽換，會造成與事實相符之敘述未改變，便無法生成反事實干擾選項。為了避免此狀況發生，我們使用NLI任務中的換句話說(MRPC)任務來檢測其是否為干擾選項。換句話說任務主要在判斷兩句話的語意是否相同，為了避免MaskLM模型選用同義詞進行抽換，即模型生成的干擾選項仍是正確的，我們將與事實相符之敘述與生成之干擾選項作為模型的輸入，若模型認為兩句話具有相同語義，我們會讓MaskLM模型預4.基於知識圖譜強化之反事實干擾選項生成於本研究項目中，我們嘗試利用知識圖譜提供語言模型額外資訊達成反事實干擾選項生成。我們的構想如Figure4所示，包含以下步驟。首先我們會讓使用者先輸入考題內文，再將內文使用斷詞或NER模型萃取entity，利用這些entity於知識圖譜中查找相關子圖，子圖必須包含內文中所有的entitynodes以及所有跟entitynodes相鄰節點，以此為知識生成之基礎。接著我們將檢索到的子圖內的nodes與edges萃取成三元組，每筆三元組的結構是以node,edge,node組成。如同圖中所示，每一筆三元組描述一基本事實陳述，如導體可以3.將與事實相符之敘述修改為反事實干擾選項為了將與事實相符之敘述更改為反事實干擾選項，我們將前一步驟所萃取的三元組與考題內文當作作語言模型的輸入來生成反事實干擾選項。我們預計透過生成語言模型與輸入給定的問句和三元組來生成與問句情境相符合的反事實干擾選項，以FigurejQuery36306984126397040076_1676210426716為例，生成語言模型在給定關於與導體相關的題目敘述以及從知識圖譜萃取與導體相關的三元組後，模型藉由這些三元組推斷其中的關係，比如”導體可以導電->石墨是導體->筆芯是由石墨製成”，從上述的三元組推斷出”筆芯可以導電”的事實，也可以改寫成”筆芯不能導電”的反事實干擾選項。5.使用Open-DomainQuestionAnswering錯誤結果之干擾選項挑選研究如前言所述，我們預計使用ODQA模型所回答之錯誤結果來當作干擾選項(如Figure5)。以我們先前曾開發https://agri.nlpnchu.org/之農業問答系統為例，我們發現以上想法所面臨的挑於ODQA的架構下，可能會遇到ODQA正確回答所下達之問題，其回傳結果皆為可行之答案，也因此無法被使用來當干擾選項，對此，我們探索兩種作法來進行調整。–1.使用BM25，且刪除尋找出來文章的正確答案在使用BM25的情境下，因為是使用關鍵字比對，所以會找出許多跟正確答案相關的文章，我們將找到文章的正確答案刪除，這樣這個文章會變成絕對回答不出正確答案的文章，可是因為Reader對文字有一定的理解能力，並且具有關鍵字的文章中也很可能會有，他會從剩下可能的答案中，尋找一個最相近的選項，這樣就可以較容易生出干擾選項。如Figure13所示，我們將答案”2,4-D”從文章中直接刪除後，再交給Reader進行閱讀，藉此確保正確答案不會被回傳。Figure13:基於ODQA架構之Retriever弱化方法–2.使用DensePassageRetrieverDensePassageRetriever[16]，DensePassageRetriever可以將資料庫的文章轉為向量的方式表示，在搜尋的時候也將Query轉為向量，並且比對兩個向量的相似度，這樣就可以找出相似的文章，可是我們想說，可以利用DPR找尋文章相似度的這一點，理論上相似的文章跟Query算出來的相似度會是最高的，我們可以將相似度高的文章拿出來，並且查看有沒有答案出現在上面，如果沒有答案，我們就將他丟給Reader，因為我們覺得如我相似度高，就代表Retriever認為裡面有答案，可是裡面並沒有含有我們的正解，所以這篇文章就含Retriever認為是答案，但是實際上不是答案的文字，所以這時候我們將這篇文章傳給Reader，就會有比較高在我們使用baseline的情境中，有時候也會出現不相關的答案，例如在??中，問模型關於殺草劑的問題，會出現的人力除草，人力除草跟題目所問的殺草劑名稱是不相關的，所以我們需要用一個方法把他過濾掉。在這種情況下，我們如果加入正確答案當作Query一起輸入Retriever，就會增加模型找到正確passage的機率，對我們想找跟正確答案不相關但是相似度較高的Passage會產生反效果，所以我們採用的是從Reader輸出之後把他過濾掉的方法。當Reader從Retriever拿到Passage之後，我們會將正確答案去除，之後再將剩餘的答案丟入一個LanguageModel作判斷，如果他跟答案的距離過於遙遠，就放入NegativeList，如果是相關的就放入PositiveList，之後要挑相關答案的時候可以從PositiveList裡面挑選。6.基於對比式學習之干擾選項生成模型強化ContrastiveLearning的設計上需要考量如何去設定PositiveSample以及NegativeSample。在我們的目標中，因為不希望模型會重複的生成出相同意思的句子，所以我們預計使用同義的句子當作NegativeSample，而資料集中的干擾選項可以被當作PositiveSample。然而，由於我們的資料集中並沒有NagativeSample的選項，也就是同義於模型生成的選項。對此，我們的想法為透過迭代生成來產生我們需要的NegativeSample。具體而言，如Figure14所舉例，在DistractorGenerator的第一次生成中，我們會輸入Question以及Answer使模型生成出干擾選項。而在第二次迭代中，我們除了會輸入QuestionAnswerPair外，還會再加上在上一次迭代中生成的干擾選項，以提示模型先前已生成的Distractor的資訊。對於第二次迭代的輸出來說，第一次迭代輸出的Distractor，即為它的NegativeSample，因為DistractorGenerator必須要生成出與上一次迭代生成的Distractor意義相異的句子，所以它們之間的SentenceEmbedding不能太過接近。Figure14:IterationGenerater示意，在第一次的迭代中生成的Distractor會作為第二次迭代的輸入，以提示模型不要生成出類似的DistractorFigure15:對比式學習於干擾選項生成之預想架構我們所規劃之整體框架，如Figure15所示。我們使用多次迭代的方式來生成Distractor，對於第一次迭代來說，我們會從其他in-batch的data中取得Sentence當作NegativeSample。而在第二次以後的迭代來說，除了其他in-batch的Sentence以外，我們還會將此次迭代前所生成的Distractor加入我們的NegativeSample中。透過NegativeSample和模型輸出的Distractor的dotproduct可以計算出ContrastiveLoss，此外我們也會加入Distractor和Label計算的CrossEntropyLoss，來計算出我們的TotalLoss15。ContrastiveLoss定義如下:x為生成的distractor，p為資料集中的Distractor(Label)，N為NegativeSampleSet，N={n1,n2,...nj}，其中包括先前迭代生成的Distractor以及其他in-batch的選項。7.基於NodeCentrality考量主題涵蓋性之選擇題題組生成如同前言所述，本研究議題之目標為考量主題涵蓋性之選擇題題組生成。因此，我們預計根據使用者所指定的主題，按Figure16架構規劃來進行題組生成。1.透過知識圖譜子圖萃取來涵蓋所指定主題之內容：藉由知識圖譜上NodeCentrality方式來萃取關鍵字，以為題幹之生成。我們的基本想法為在知識圖譜上我們定位出與使用者輸入之主題關鍵字相關之結點後，我們可以試者套入PageRank或者其他圖形上計算NodeCentrality重要節點挑選，並以重要節點為題幹生成之主軸。2.透過上一步取得之重要答案片段後，進行問題之生成。並生成較多的問題數量以為CandidateQuestionPool，而後將問題將題組生成問題設定一成為一選擇最佳化議題(Optimization)：根據主題生成大量的單一題目，而後定義UtilityScore來計算個別題目組合後之優劣，來挑選最佳化的題組以為輸出。於本研究項目中，我們預計探索以下兩種生成難度控制方式。•調整Masked-LM比例之反事實選項難度控制：現有的Masked-LM模型會將重要的專有名詞進行遮擋，模型會透過未遮擋的內容對該專有名詞進行預測。理想的情況下，當我們僅對該專有名詞進行遮擋時，模型可成功地預測出遮擋的專有名詞或語義相近的專有名詞，這不僅是因為MaskLM模型夠強大，也是因為未遮擋的內容提供模型足夠多的資訊讓模型預測，若未遮擋的內容提供的資訊不夠多，則模型預測出的內容有可能會與遮擋的內容語義相差很多。我們根據Masked-LM模型的此特性對題目的難易度進行控制，若我們將多個專有名詞進行遮擋，再讓模型預測時，模型會因為提供的資訊不足，而預測出語義較不相近的專有名詞，即較不可能的答案。以下表為例，我們可以觀察到當[Mask]越多時，我們要判斷該事實是否正確就越形容易。[Mask]借壓力差吸收[Mask]回微血管[Mask]借壓力差吸收[Mask]回[Mask]•調整ODQA檢索文章之難度控制：由於我們的基礎架構為基於文章檢索(DocumentRetrieving)來取得包含使用者所給定的關鍵字之文章，並使用該文章與關鍵字為問題生成模型之輸入，來生成選擇題問句。我們若挑選較為冷門之文章進行生成，則預期可以生成較為困難之題幹。而就干擾選項而言，我們可藉由調整輸入給閱讀理解模型之文章來產生難度較高的錯誤。具體而言，給定一有包含正確答案A之文章C，我們先將答案片段A於文章D中刪除，而後將其輸入至閱讀理解模型，藉此產生錯誤之閱讀理解結果，以這樣的錯誤結果當Distractor預期將有較高之誘導能力。相類似的想法也可以套用為挑選Retriever認為相關度比較不高的文章以為閱讀理解模型之輸入。此外，就生成題目之廣度與深度而言，我們可藉由控制取得之文章來調配題組之廣度與深度。例如，若我們以單篇或少數篇文章進行生成，我們所生成的主題題組針對測驗內容而言，應該可產生知識性較為深入之試題。而若針對較多篇文章，則應可產生範圍較廣本研究計畫將以三年為期，共八項研究議題進行，各研究產出如上文所示，並整合各技術成為一選擇題自動生成系統，預期的展示情境為：我們可於系統直接輸入一知識範圍，系統將利用我們所開發之技術先進行自動關鍵語萃取，而後進行各種類型的選擇題出題並考量題組關聯性。我們也將彙整各研究議題中所發展之技術為一個成為公開原始碼專案，以為後續相關研究之用。並將計畫執行過程所累積的資料，公開希望能吸引更多研究者之投入。對於學術研究、國家發展及其他應用方面預期之貢獻自然語言生成已是未來深度學習領域之熱門研究重點。透過本研究計畫之投入我們將可進行先驅之探索，建立學生團隊與自然語言處理與生成之能力，並持續過去之閱讀測驗自動出題研究成果，以知識類型選擇題自動出題為主要系統開發目標為例，希冀能為國內外探索不同的自然語言生成應用服務。此外若可順利完成也可以成為一成功範例利用人工智慧輔助教師進行測驗的客製化與個人化學習。對比於題庫類型的方式，透過我們技術我們將可動態產生各式各樣不同之考題為每個人量身定做難度適中之考題。就此一觀點出發，我們預期本計畫除學術研究價值外，同時也兼具商業價值，也應可成為EdTech(教育科技)領域之一環。再者，疫情改變了學習與教學的模式。同步與非同步之線上教學已成為教學模式常態之一。線上教學帶來了好處，但也帶來了顧慮。其中，學習的專注度應是諸多顧慮中主要令人擔憂的一環。隔著螢幕學生學習是否專注？目前的教學內容學生是否已跟上進度？等等問題是線上教學學習專注度所在意的問題。對此，我們認為適時與即時地檢測學生學習成效，來提醒學生與教師目前之學習成效，應能適度緩解線上學習專注度之顧慮。因此，我們也常見於MOOCs課程於一個授課單元結束後適時地給予測驗(多數以選擇題測驗)，來確認學生學習成效。然而，人工設計試題耗費心力，一成不變全班同學皆相同試題也可能透過即時訊息(instantmessage)之分享而取得答案，造成檢測之無效。對此，我們認為隨著深度學習技術之快速演進，結合自然語言(NaturalLanguageGeneration)生成技術來進行自動出題技術已成為可能；透過自然語言生成技術自動化出題，產生多樣化的試題，來進行個人化的檢測，將有除了可以增進研讀論文、撰寫程式、架設系統以及撰寫論文的基本能力之外，參與本計畫之研究人員，將可以學習到深度學習與自然語言生成技術之基本觀念、實際應用以及相關的核心技術，在這些基礎之下，透過積極的研究與討論，將可以提升參與計畫人員思考問題與解決困難的重要能力。我們預期可獲得具體技能如下:(1)設計與建立一個深度學習模型之能力，(2)整合不同先進技術之能力，(3)自然語言生成技術之瞭解，(4)資訊檢索相關知識與軟體套件之暸解，(5)佈建深度學習模型於WebScale之能力。"}, "1676210724.327447": {"單位": "", "標題": "伍動螞蟻-", "作者": "", "發布日": 0, "摘要": "", "全文": "螞蟻會對我們造成很大的影響。入侵紅火蟻在澳洲、台灣肆虐，攻擊任何在牠們巢穴附近的生物；黃瘋蟻在耶誕島獵捕紅地蟹，造成耶誕島紅地蟹的浩劫；熱帶大頭家蟻在澳洲是最強勢的入侵物種，牠們會保護介殼蟲及蚜蟲，造成農民的損失……螞蟻至少有260屬，18000種，所以我會藉由相關文獻及實地研究探討螞蟻的不同習性與生活型態，希望能讓大家了解，螞蟻對我們此次有關螞蟻的獨立研究，我主要是以文獻探討中內容分析的方式呈現簡報。並且加入問卷測試小學生對於螞蟻知識的了解程度。以下是我針對文獻探討、前測問卷、講座推廣和後側(KahootQ&A、google問卷編1.\t前測問卷發放-小學生的螞蟻知識知多少?\t(1)分析問卷中的答對率，並從中了解小學生對螞蟻知識了解的的程度，針對錯比較多的題目，進行內容的蒐集與整理，製作成專題講及臭巨山蟻來更了解螞蟻的日常習性，並藉此螞蟻知識推廣-(kahoot機智問答)我於民國一一○年三月三十日星期二舉辦了一場《伍動螞蟻－螞蟻知識知多少》的螞蟻知識推廣的更懂螞蟻，所以我在專題講座後請參語的同學在我煩惱要做什麼當我的獨立研究時，忽然看到了螞蟻相關報告，讓我彷彿撥雲見日，我決定要以螞蟻當作我的獨立研究主題。在文獻探討中，我將其分為八個章節，在網路上搜尋有關螞蟻的知識。並在閒暇的時間進行有關螞蟻的小實驗，目的就是要讓我的報告更完整，且更有價值。經過了一段艱辛的路程後，我完成了我的獨立研究。希望能讓大家更了解螞蟻，且更尊重牠們。螞蟻會對我們造成很大的影響。入侵紅火蟻在澳洲、台灣肆虐，攻擊任何在牠們巢穴附近的生物；黃瘋蟻在耶誕島獵捕紅地蟹，造成耶誕島紅地蟹的浩劫；熱帶大頭家蟻在澳洲是最強勢的入侵物種，牠們會保護介殼蟲及蚜蟲，造成農民的損失……螞蟻至少有260屬，18000種，所以我會藉由相關文獻及實地研究探討螞蟻的不同習性與生活型態，希望能讓大家了解，螞蟻對我們的(二)讓大家了解螞蟻之餘，能更尊重牠們。為蟻窩中具有生殖能力的雌性螞蟻，整個蟻群都為蟻后的後代。有時不需要雄蟻介入也能進行孤雌生殖，不過在這種情況下所產出來的卵為未受精的卵，只會孵化出雄蟻。大多數的蟻后和雄蟻都會進行婚飛，也就是在特定時間讓蟻后與型蟻同時飛出巢外，並且進行交配，而不同螞蟻進行婚飛的時間也不盡相同。依照種類的不同，有些種一個蟻窩中只會有一隻蟻后，也有些種類的螞蟻在一個蟻窩中同時會有上百隻具有生殖能力的蟻后。蟻后可生存長達30年，紀錄中由德國昆蟲學家HermannAppel所飼養的黑褐毛山蟻蟻后活了28¾年。雖然蟻后被稱為「蟻后」，但不能將人類社會當中上下階級和統治的觀念給帶入螞蟻社會中。事實上，蟻后在蟻巢中只是個產卵機器，在部分情況下甚至會被推翻。由此可證蟻后不一定在蟻巢中絕對的權勢。雄蟻的主要工作是與蟻后交配。由未受精的卵細胞發育而來，性染色體為X(人類男性的性染色體為XY)，大部分有翅，交配完不久後便會死去。工蟻的主要工作為負責搜尋食物、照顧蟻卵、幼蟲等大部分的工作，體型比兵蟻小。是由受精的卵細胞發育而來，染色體雙套（2n），大部分在一出生就就受到蟻后分泌的費洛蒙影響，所以不會產卵，在蟻后死亡後可能會產下未受精卵。未受精卵有營養卵或是雄蟻卵兩種，營養卵是食物，雄蟻卵則會孵化成雄蟻；而某些物種的工蟻卵巢能夠發育，交配後能夠產下能發育的受精卵。部分物種的工蟻產卵管特化為螫針，可以用來制服獵物或是防衛蟻窩。在螞蟻巢中，較年輕的工蟻會負責照顧蟻后；而幾乎所有的螞蟻都具有年齡多形性（Agepolytheism）的分工機制，除了兩個物種以外，分別是Amblyoponepallipes和大眼擬鬥牛犬蟻（Nothomyrmeciamacrops）。1.和工蟻一起保衛蟻巢。如熱帶火蟻、收穫蟻屬。2.在工蟻出外覓食的時候，幫忙擔任清除雜物的「推土機」並非每一種螞蟻都有兵蟻，沒有兵蟻的物種包括針蟻亞科、琉璃蟻亞科、擬家蟻亞科等。兵蟻是由受精的卵細胞發育而來，染色體雙套，為沒有生殖能力的雌蟲。某些物種的兵蟻頭部及大顎高度骨化且發達。一般來說頭部異速生長的大型個體稱作兵蟻，而頭部沒有異速生長的大型個體則稱作大工蟻，但這兩個詞彙之間還是有難以區分的灰色地帶。具有兵蟻的物種如大頭家蟻屬，具在蟻巢中，除生殖階級外，所有個體無翅。觸角為膝狀，某些物種觸角末端膨大，稱作「垂節」，牠們的觸角，牠們也用觸角判斷彼此身分。大部分物種胸部和腹部之間隘縮，形成腰節，實則屬於胸部的延伸，或稱腹柄節，具有三個腹節。胸節與前伸腹節合稱「中軀」），腹垂節稱作「後軀」。螞蟻的體型範圍平均大約為0.75~52mm，目前已知體型最大的螞蟻為已滅絕的古巨蟻亞科，蟻后體長可達6cm，翅膀長度可達15cm。螞蟻可以透過費洛蒙溝通，一隻螞蟻如果發現了食物，它就會在回家的路上留下一路的氣味，其他的螞蟻就會沿著這條氣味路線去找食物，並不斷地留下氣味加強氣味。如果這裡的食物被採集完了，沒有螞蟻再來，氣味就會逐漸消散。如果一隻螞蟻被碾碎，就會散發出強烈的警戒費洛蒙(註一)引起其他螞蟻進入緊戒狀態；如果一隻螞蟻散發出死亡費洛蒙，不管那隻螞蟻是活的還是螞蟻和其他昆蟲一樣透過觸角辨識氣味，觸角的末幾節膨大，呈膝狀彎曲，非常靈活。由於觸角是一對，因此既能辨别氣味的強度，也能辨識氣味來源的方向距離。但是也造就了蟻客(專門寄生於螞蟻巢內的生物，部分蟻客對螞蟻沒有害處，不過有一部分的蟻客對螞蟻有明顯危害)成蟲互相交哺並通過其氣味了解對方的健康狀況，對方發現的食物等資訊。同時也能區別對方屬於從事哪個分工的階級。如負責餵養蟻后及幼蟲的螞蟻，或是負責搜集食物的螞蟻后會不斷地分泌費洛蒙，這種費洛蒙能抑制工蟻的卵巢發育並讓工蟻知道蟻后還在巢內，一旦以這種氣味消失，有些物種會出現新的蟻后，有些物種的工蟻則會開始產卵，填補蟻后的功能。某些螞蟻用大顎啃咬以攻擊或自衛，山蟻亞科的物種能從腹部末端分泌蟻酸（甲酸），刺激被叮咬的傷口紅腫疼痛；部分物種腹部末端具有螫針。螞蟻亦是全世界力氣最大的昆蟲之一，牠的負重能力相當驚人，能拖動比牠體重還重1400倍的物品，也能背負自身體重註一:警戒費洛蒙分成己醛、己醇、正十一酮和丁基辛酮。己醛會最先出現，提醒工蟻提高警覺；己醇是與酒精(乙醇)類似的物質，會促使工蟻尋找問題來源；正十一酮會吸引工蟻接近問題來源，並在碰到外來物時張口就咬；丁基辛酮最後才會出現，會讓工蟻展開攻擊與咬噬的行動。蠻蟻亞科Agroecomyrmecinae鈍針蟻亞科Amblyoponinae（包含新蟻亞科Apomyrminae）†布朗長蟻亞科†Brownimeciinae異針蟻亞科Heteroponerinae鬥牛犬蟻亞科Myrmeciinae（包含：擬鬥牛犬蟻亞擬家蟻亞科Pseudomyrmecinae†蜂蟻亞科†Sphecomyrminae†幽冥蟻亞科†Haidomyrmecini†阿爾曼蟻亞科†Armaniinae，有些文獻將牠列入蟻科，有些則獨立一科，但無論如何，該亞科都在蟻總科（Formicoidea）之下目前已確定有21亞科283屬，11700種。茲舉幾種以說明:(一)行軍蟻:牠們會一直移居，並且吞食路上遇到的獵物。在移居過程中，1.又名弓背蟻屬或木匠蟻屬，是蟻科山蟻亞科的一個屬。2.牠們不喜歡築巢在乾燥的地方，而巢穴可能建於房屋的橫樑、地板或巢穴中。牠們是雜食性的螞蟻，以食物碎屑或其他昆蟲為食，也會食3.巨山蟻屬的成員眾多，台灣目前本屬有十七種，巨山蟻屬以牠們獨特的習性聞名。牠們會在木材裡挖洞，並以這些通道為生活重心，因為牠們這種特殊的生活習性，所以規模較大的巨山蟻群落會造成房屋結構性毀損，但通常不像白蟻破壞過的那麼嚴重。4.一個成熟的巨山蟻群落一般會有一隻蟻后，許多待孵化的卵，超過兩千隻以上的工蟻，和一個只有工蟻的附屬群落。5.目前發現的蟻后體型最大者為1.91公分。而屋裡的巨山蟻可能是來自主群落或附屬群落，例如：巨山蟻可能來自戶外已枯萎的大樹，陸6.本屬的膨咕巨山蟻跟蜜瓶蟻一樣有貯蜜蟻階級(蜜瓶蟻中的貯蜜蟻又稱蜜缽階級工蟻)它們會收集糖蜜並儲存它們。而在鬧飢荒時，這些貯蜜蟻就派上用場了:牠們會把這些花蜜回哺給同伴食用。7.被巨山蟻在裡面築巢的木材會有木屑遺落在附近，而被白蟻破壞過的木材則會有含泥類物質，當發現附近有上述的木屑(通常會包含已死亡的螞蟻和部分被巨山蟻吃掉的昆蟲殘骸)，就代表附近可能有巨山蟻的巢穴。巨山蟻喜歡築巢在較潮濕的木頭中。(三)針蟻:這種螞蟻包含世界最大的螞蟻:巨人恐針蟻。牠們是少數沒有蟻后的螞蟻，交配過的工蟻會取代蟻后，可以透過解剖牠(四)蜜瓶蟻:牠們的部分大型工蟻會吸飽蜜，在鬧飢荒的時候回吐花蜜，因此被稱為蜜缽階級工蟻。牠們也是澳洲居民的美食，澳洲居民會從牠們的屁股吸取花蜜，但是被人類吸食的蜜瓶蟻也會因為人類破壞式的取(五)火蟻:火蟻屬中有惡名昭彰的入侵紅火蟻，但是火蟻屬中，在台灣已發現1.獵食紅火蟻：生活在森林邊緣，都市綠地、海邊、草地、農地等。在石頭下或土中常常可以發現牠們的蟻巢。牠們是多后制體系的螞蟻，成熟蟻巢由數隻具生殖能力的蟻后和數百隻工蟻組成。牠們分布在台灣中低海拔地區，牠們的食性是雜食偏素食性，主要食物是花蜜、蜜腺分泌和小型昆蟲屍體。牠們除了。會讓工蟻組成小組出外覓食，還會用蟻賊的方式，偷取附近蟻巢的資源。目前未發現有2.知本火蟻：牠們是一種分布在廣西、湖南、湖北天堂寨和台灣。牠們跟原生於南美洲、中美洲、墨西哥和部分美國南部地區的切葉蟻有親戚關係。牠們原生於美洲的熱帶與亞熱帶地區，後來隨著交通工具入侵印度、非洲、太平洋島嶼等地區。牠們攻擊性較強，會出現主動攻擊人畜與莊稼，甚至會咬破電線，破壞供電系統。牠們的毒腺中含有大量生物鹼，叮咬人時會造成劇烈疼痛。會危害當地的農林業生產、人體健康、公共安全及生態環境。3.入侵紅火蟻：牠們的英文簡稱為RIFA，是Redimportedfireant的縮寫，原分布在巴拉那河流域，由於生活在河邊，所以發展出「蟻筏」，也就是讓工蟻互相咬住對方，變成一個蟻筏，並且把蟻后、卵、幼蟲及蛹放在上面，載牠們渡河。由於牠們體表防水，所以不會淹死。紅火蟻在二十世紀初入侵美國南部，造成美國十二個州超過一億公畝(一百億平方公尺)被入侵紅火蟻佔領，每年損失約數十億美元以上。且目前未發現能夠有效防治入侵紅火蟻的生物。火蟻屬成員的毒液大多含有大量生物鹼，叮咬人時會產生如火燒般的劇痛，這也是牠們被取名為「火蟻」的原因。紅火蟻的成熟群落約有二十萬隻至五十萬隻，所以入侵者往往會被大量的火蟻叮咬。被火蟻叮咬後，大部分的人除了會感到劇痛，還會產生水泡，若把它抓破，則會造成二次性的感染。少部分的人甚至會產生嚴重的過螞蟻是靠氣味溝通的。所以蟻客就是掌握了螞蟻的費洛蒙，才能滲透進蟻巢。有些蟻客寄生在蟻巢內純粹是為了尋求庇護，如寄生家蟻；有些蟻客則是為了取得食物，如:黑隱翅蟲的幼蟲會寄生在蟻巢內，並吞食正牌的螞蟻幼蟲。牠們沒有消滅整個蟻巢，是因為牠們也會自相殘殺。甚至有一種蜘蛛會偽有些植物也跟螞蟻有共生關係。如:有一種植物會分泌蜜露，供螞蟻食用，和讓螞蟻築巢在裡面。而螞蟻則會趕走要吃牠們的動物，並且清除雜草。而這有助於植物生長。形成互利共生的關係。螞蟻可以簡單分為頭胸腹三部分，但是更細微的部分呢?以下是我從網路抓(KahootQ&A、google問卷編1.前測問卷發放-小學生的螞蟻知識知多少?(1)分析問卷中的答對率，並從中了解小學生對螞蟻知識了解的的程度，針對錯比較多的題目，進行內容的蒐集與整理，製作成專題2.螞蟻習性觀察:我透過飼養高雄巨山蟻、希氏巨山蟻，以及臭巨山蟻來更了解螞蟻的日常習性，並藉此驗證我所查到的文獻內容是否正3.校內演講:螞蟻知識推廣-(kahoot機智問答)我於民國一一○年三月三十日星期二舉辦了一場《伍動螞蟻－螞蟻4.後測問卷發放-我了解舉辦一場專題講座是否能夠讓同學更懂螞蟻，所以我在專題講座後請參語的同學幫我填寫回饋問卷，也給我一些一、前測問卷分析-小學生的螞蟻知識知多少?我為了想要了解小學生對螞蟻的認識，因此特地發了這份問卷，並請高年級的電腦老師鍾佑聆老師於電腦課的時候花幾分鐘的時間請大家填寫。下表顯示我於二零二零年十二月二十一日所發出，並於二零二零年十二月三十日回收的問卷共527筆的結果如下，請詳細閱讀。由此表可知，在527人中，有480個人回答了正確答案(正確答案:六隻腳)，21人回答螞蟻有四隻腳，26人回答螞蟻有八隻腳。難度:低由此表可知，在527人中，有491個人回答了正確答案(正確答案:蟻后)，6個人回答兵蟻，9個人回答雄蟻，17個人回答工蟻。難度:低由此表可知，在527人中，有十三人回答蟻后、四百八十一人回答工蟻、四十人回答雄蟻及一百八十二人回答兵蟻(正確答案:工蟻和兵蟻)。難度:低由此表可知，在527人中，有475個人回答了正確答案(正確答案:兵蟻)，7人回答蟻后，36人回答工蟻，8人回答雄蟻。難度:低由此表可知，在527人中，有489個人回答了正確答案(正確答案:雄蟻)，5人回答兵蟻，14人回答蟻后，19人回答工蟻。難度:低由此表可知，在525人中，有192個人回答了正確答案(正確答案:亞全山蟻)，125人回答狂蟻(小黑蟻)，53人回答亞絲山蟻，154人回答都不是。難度:高由此表可知，在523人中，有168個人回答了正確答案(正確答案:亞絲山蟻)，170人回答狂蟻(小黑蟻)，99人回答都不是，85人回答亞全山蟻。難度:高由此表可知，在523人中，有317個人回答了正確答案(正確答案:蜜瓶蟻)，78人回答狂蟻(小黑蟻)，43人回答亞絲山蟻，84人回答都不是，難度:中由此表可知，在523人中，有211個人回答了正確答案(正確答案:火家蟻)，130人回答黃瘋蟻，35人回答蜜瓶蟻，83人回答都不是，63人回答白霜前琉璃由此表可知，在523人中，有110人回答了正確答案(正確答案:蜈蚣)，67人回答青蛙，154人回答都不是，120人回答蜘蛛，72人回答蠍子。難度:高由此表可知，在523人中，有181個人回答了正確答案(正確答案:不是)，342由此表可知，在518人中，有179人回答了正確答案(正確答案:黃瘋蟻)，196人回答白霜前琉璃蟻，62人回答蜜瓶蟻，83人回答狂蟻。難度:高由此表可知，在518人中，有324人回答了正確答案(正確答案:鋸針蟻)，61人回答黃瘋蟻，100人回答狂蟻，33人回答蜜瓶蟻。難度:中1.螞蟻遇到石膏粉與滑石粉時，會因為摩擦力降低而無法爬上斜坡。2.不同種或同種不同窩的螞蟻在相遇時有可能會打起來，也可能會互相忽略，但是在其中一方確定自己有百分之百的勝算時，便會征服另一個群落。(三)專題講座後的搶答活動:後測kahoot問答:螞蟻冷知識知多少?在我進行完螞蟻專題講座後，我用kahoot進行機智問答。以下是我的題1.螞蟻有幾隻腳(選項:四隻腳、六隻腳、八隻腳、十隻腳)2.一般來說，什麼螞蟻會產卵(選項:雄蟻、蟻后、工蟻、兵蟻)3.請問何種螞蟻是由非受精卵孵化而成?(選項:工蟻、蟻后、雄蟻、兵4.以下何者是「蟻客」?(選項:獨角仙、鍬形蟲、黑隱翅蟲(幼蟲)、蟑5.下列何者會捕食螞蟻(選項:草齡幼蟲、蝴蝶、蜜蜂、以上皆非)6.下列敘述何者正確?(選項:螞蟻為群落服務並不為任何好處、所有的螞蟻都具有兵蟻階級、螞蟻群都是雜食性的昆蟲、螞蟻是一種真社會7.螞蟻的一生為何?(選項:卵-幼蟲-蛹-成蟲、卵-若蟲-成蟲、卵-若蟲8.古代有一種「蚳醢」，為帝王食補。請問蚳醢為下列何者?(選項:螞9.螞蟻和下列何者有親戚關係?(選項:蟑螂、白蟻、螳螂、胡蜂)10.白蟻和下列何者有親戚關係?選項:蟑螂、獨角仙、天蛾、胡蜂)11.請問螞蟻透過甚麼溝通?(選項:費洛蒙、聲音、震動、以上皆是)12.請問下列何種螞蟻會使用蟻酸攻擊?(選項:蠻蟻亞科、家蟻亞科、此為我於民國一一○年三月三十號螞蟻講座後發出的滿意度調查問卷:由此表可知，有四個人對於我在台上的表現給三分、二個人給我四分、五個人由此表可知，有四個人對於我在台上的表現給三分、三個人給我四分、四個人由此表可知，有十一個人在這次專題演講後，對於螞蟻知識有更深的了解、零由此表可知，有七個人在這次專題演講後，不會想要進一步了解螞蟻的相關知識或主動閱讀相關書籍、四個人會想要進一步了解螞蟻的相關知識或主動閱讀此為我對同學們發出的專題講座滿意度調查問卷:簡報得稍微有點多，我還沒看完就換下一頁。可以講得再慢一點搶答部分應先設定好平板、電腦，以免讓大家等太久由此表可知，有六個人認為不用改進、兩個人認為可以先把平板和電腦設定好、一個人認為我講太快了、一個人認為簡報上面的字有點多、一個人認為簡根據我發出的表格，有五題難度為低，三題難度為中，五題難度為高，平均難度為中。卻有很多題目連一半的答對率都不到，我覺得是因為我的問卷題目太刁鑽而且太難。下次可以考慮把題目的難度降低且不要出那麼刁鑽，會更我將研究結果統一整理成一個表格，來表達我的結果與討論，還有結論與此次有關螞蟻的獨立研究，我主要是以文獻探討中內容分析的方式呈現簡報。並且加入問卷測試小學生對於螞蟻知識的了解程度。以下是我針對文獻探討、前測問卷、講座推廣和後側問答的結果、結論和建議製作的表格:一、維基百科https://reurl.cc/6yV2xO二、超罕見的詭異螞蟻！澳洲原住民最愛的甜食「蜜罐蟻」，號稱行走食物櫃https://reurl.cc/R1Dao6三、臺灣生命大百科:獵食火家蟻https://reurl.cc/7oLnKN四、知本火蟻https://reurl.cc/r84Ga4五、認識火蟻https://reurl.cc/e87lGb六、巨山蟻屬https://reurl.cc/Q3xlpZ七、下課花路米第749集-小小螞蟻大世界(上)https://reurl.cc/VXVoW6八、下課花路米第750集-小小螞蟻大世界(下)https://reurl.cc/OqWom9九、下課花路米第1181集-台灣還有紅火蟻?https://reurl.cc/odG3K3十、流言追追追第51集-螞蟻剋星https://reurl.cc/9X8A5j流言追追追第4集https://reurl.cc/KjZg3e螞蟻的構造https://reurl.cc/pmndVZ螞蟻亞科列表https://reurl.cc/5ox9qv"}, "1676210732.0940633": {"單位": "", "標題": "(ㄧ)、研究計畫之背景與目的", "作者": "", "發布日": 0, "摘要": "", "全文": "自然語言生成(NaturalLanguageGeneration)將是未來深度學習領域之熱門研究議題。2022年年末ChatGPT語言模型所帶來的示範性與展示效果預期將吸引更多研究目光。而個人於中興大學之研究團隊近年於自然語言生成之耕耘已建立初步研究成果。在過去朝著自然語言生成方向前進，我們探索自動生成試題之研究方向前進。於前期研究計畫「基於深度學習語言模型之閱讀測驗自動化生成研究109-2221-E-005-058-MY3」中，我們著眼於基於深度學習模型進行閱讀測驗(ReadingComprehensionTesting)題型之自動出題。在計畫執行過程中，我們•Ying-HongChan,Yao-ChungFan:KeywordProvisionQuestionGenerationforFacilitatingEducationalReadingComprehensionPreparation.INLG(InternationalNaturalLanguageGen-erationConference)2022.•Ho-LamChung,Ying-HongChan,Yao-ChungFan:ABERT-basedDistractorGenerationSchemewithMulti-taskingandNegativeAnswerTrainingStrategies.EMNLP(EmpiricalMethodsinNaturalLanguageProcessing)Findings2020.•Shang-HsuanChiang,Ssu-ChengWang,Yao-ChungFan:CDGP:AutomaticClozeDistractorGenerationbasedonPre-trainedLanguageModel.EMNLP(EmpiricalMethodsinNaturalLanguageProcessing)Findings2022.•Hsien-YungPeng,Ho-LamChung,Ying-HongChan,Yao-ChungFan:MisleadingInferenceGenerationviaProximalPolicyOptimization.PAKDD(AdvancesinKnowledgeDiscoveryandDataMining-26thPacific-AsiaConference)2022此外我們也將前期計畫之自動出題研究成果進行實作，可於以下兩個網址進行實測。•問句生成QuestionGeneration：給定文章、指定片段，透過自然語言生成模型進行問句生成。目前系統可於https://classic.queratorai.com/進行實測。•克漏字干擾選項生成DistractorGeneration：此一部分針對給定之文章進行克漏字誘導選項生成。目前系統可於：https://cdgp-demo.nlpnchu.org/進行實測。基於過去之研究成果累積，本計劃延續自然語言生成技術於自動出題之研究成果，擬定知識類型選擇題(MultipleChoiceQuestion,MCQ)自動生成為目標。以四技二專統一測驗考試為例(如Table2所示)，知識類型選擇題將包含題幹與四個選項(其中包含正確答案與干擾選項)，主要針對某一知識範圍進行測驗。我們認為知識範圍選擇題之自動生成可分為兩部分：(1)問題題幹(Stem)生成與(2)干擾選項(Distractor)生成。題幹生成主要目標為選擇題問題本身之生成，而干擾選項則著眼於誘導選項之生成。知識類型選擇題為學習測驗最常見的測驗模式之一。對比於閱讀測驗生成，知識類型選擇題的自動生成，將可減輕教師準備個人化教學測驗之負擔，也為學生提供更有效率之學習檢測方式。以現有我們成果為基礎，一個基本之自動選擇題出題系統作法為(如圖1)所示：我們可利用資訊檢索技術(如BM25Retriever)，從給定之文集資料庫(目前使用2021中文維基百科所有中文文章)中檢索主題字詞相關文章後，結合前期計畫所開發之問句生成模型(QuestionGeneration)[4]生成題幹，而後再利用詞向量模型(例如word2vec模型)找出相近字詞以為干擾選項。我們將這樣的構想先行實作一系統(稱之為Retrieve-and-GeneratePipeline)，目前此系統可於https://mcq-demo.nlpnchu.org/使用。使用者可藉由輸入主題相關字詞，選擇題生成系統將自動生成輸入字詞相關之選擇題結果。舉例而言，我們以「骨髓、白血病、淋巴」為主題字詞輸入至系統。我們挑選較佳之成果顯示於Table1。自動生成選擇題AutomaticGeneratedMCQQ1:哪一種癌症種類的統稱中可以看出它會造成不正常白血球的大量增生?(a)白血病、(b)乳腺癌、(c)腎病、(d)帕金森氏症Q1:白血病的發病原因通常是發生在哪一個器官上?(a)肝臟、(b)淋巴、(c)甲狀腺、(d)骨髓Q:白血病是最常見的兒童癌症，其中80%的病例是慢性的哪一種性病變?(a)淋巴結、(b)微血管、(c)淋巴、(d)黏膜Table1:目前選擇題自動生成結果範例。目前系統可於https://mcq-demo.nlpnchu.org/。目前我們挑選較佳之生成結果顯示於此Table中。建議審查委員自行輸入關鍵字詞組，來觀察我們Figure1:Retrieve-and-Generate基本架構。本例中我們以”骨髓、白血病、淋巴”為關鍵詞進行文章檢索，而再使用”骨髓”為答案，分別生成問句與干擾選項。從展示之結果中，我們發現自動選擇題出題之可能性。但仔細觀察會發現現有系統離實際應用仍有空間須待努力。也因此觸發本期研究計畫之提出。具體而言，我們認為選擇題自•句子等級之干擾選項生成(Sentence-LevelDistractorGeneration)首先，關於干擾選項的部分，若仰賴詞向量模型來產生干擾選項，我們所能取得的干擾選項便僅限於單詞(Word-Level)等級。從統測試題中，我們發現許多題目之干擾選項屬於句子等級(Sentence-Level)。以Table2中Q3與Q4選擇題為例，我們可以看到選項皆為句子等級之描述，此為目前基於詞向量來產生干擾選項的做法所不能。•反事實干擾選項生成(Counter-FactualSentenceGeneration)我們觀察到選擇題題型中，有著下列何者為錯誤或正確類型之考題。其干擾選項是一個較長的描述句。以Table2為例，Q5與Q6為分別詢問下列選項何者正確/錯誤。此題型，為變像之是非題。除了句子等級生成外，核心技術牽涉的是反事實(Counter-Factual)句子之生成。如何生成此類型之句子以為干擾選項也將是研究重點所在。•選擇題生成品質提升(MCQQuestionQualityImprovement)此外，我們也看到retreive-and-generatepipeline的做法，所產生的題幹本身有時是一個過於空泛之問題(例如，下列何種疾病為兒童常見之癌症？)或者並非語意正確之語句(例如，中樞性的什麼症狀是缺乏抗利尿激素的?)。如何有效判斷生成不良之語句，減低人工進一步過濾/挑選題目之負擔也是需進行研究之議題。此外，基於詞向量之干擾選項作法也有待商榷;隨機從相近詞向量挑選distractor，是否能確保distractor之誘導品類型I:單詞等級的干擾選項Word-LevelDistractorsQ1：淋巴器官是人體防禦系統中重要的一環，可提供淋巴細胞發育或活化所需要的條件與狀態。下列何者是淋巴球進行防禦與活化的器官？(a)骨髓(b)胸腺(c)肝臟(d)脾臟Q2：健康人體感覺神經傳入大腦的訊息大部分經過轉接，(a)視丘(b)下視丘(c)中腦(d)橋腦類型II:句子等級的干擾選項Sentence-LevelDistractors(a)無就巢性(b)體型大(c)著肉多(d)耐粗食Q4：下列何者為「聯合國國際糧農組織」(FAO)成立最重要的目標及首要任務？(a)消除及戰勝飢餓(b)聯絡及交流農業資訊(c)自然環境的永續及保護(d)實現國際間的公平貿易類型III:句子等級且與事實相違背的干擾選項Counter-FactualDistractorsQ5：有關人類上皮細胞的細胞週期之敘述，下列何者正確？(c)後期，分離的染色分體往細胞中央移動(d)末期，細胞膜內凹且細胞質開始一分為二Q6：有關一健康成人泌尿系統功能的敘述，下列何者錯誤？(a)腎臟每日約產生濾液180公升及尿液1.5公升(b)腎小管藉壓力差方式再吸收葡萄糖回微血管(c)腎臟藉排尿量多寡來維持體內水分的恆定(d)若出現尿毒症狀時，可使用血液透析方式進行治療Table2:四技二專統一測驗選擇題類型概觀。Q1與Q2擷取自110學年度護理基礎生物考科，Q3與Q4110學年度農業群農業概論考科，Q5與Q6取自110學年度農業群基礎生物考科質？也許所挑選的distractor並不具誘導性，使得受測者很輕易地可挑選出正確答案，或者挑選到答案的同義詞，造成一題有多個正確答案。根據先前計畫執行訪談教師所知，於測驗時為考量受測全體鑑別度，教師出題時會考量干擾選項彼此之間之難易度；通常三個干擾選項中，會有一個離正確答案較遠之干擾選項(誘導性較低)，一個次之，一個最為困難最具有誘導性。藉此來有適當的鑑別度。目前我們直接使用詞向量之作法並未考量此設計。•主題題組生成(MCQQuestionGroupGeneration)目前我們的系統僅為單一關鍵字詞之出題。我們認為一個良好的自動出題應該是以題組為單位進行出題；使用者指定一主題知識，系統根據此一主題知識範圍進行5-10題之選擇題自動生成，題目之間也應該彼此考量。目前Retreive-and-Generatepipeline的做法，我們使用使用者給定的關鍵字為答案，進行題目之生成。然而比較好的做法應為將使用者給定的關鍵字集合為知識範圍之指定，系統根據該範圍自動挑選重要概念關鍵字進行答案挑選，而後再進行選擇題題組之生成。針對上述之研究挑戰，於本研究規劃中我們擬定下列八項研究議題進行探討。•研究項目ㄧ：基於語言模型與知識圖譜之Text2Text干擾選項生成模型(DistractorGenerationbasedonText2TextLanguageModelandKnowledgeGraph)Figure2:Text2Text干擾選項生成架構Figure3:基於Masked-LM之反事實干擾選項現有之Distractor生成研究多是基於CandicateGenerating-and-Ranking架構，先透過知識庫(如Wordnet)或者透過MaskedLM的方式取得候選字後，再適當進行候選詞特徵排序以為最終干擾選項。對此我們有不同之想法，我們預計嘗試把干擾選項生成設定為一個Text2Text任務；如圖2所例，預期的輸入為題幹Q與答案A之串接字串，透過ModelEncoder與Decoder轉化輸出為一連串之干擾選項字串“重組DNA技術[SEP]電腦斷層掃描[SEP]組織培養”。我們透過語言生成模型直接將輸入對比轉換為輸出。此外，透過此一Text2Text方式，生成句子等級的干擾選項也將為可能。再者，近年來語言模型研究趨勢除了將模型參數量與訓練資料持續增加外，引入知識圖譜來強化生成模型於常識理解與推理能力也成為研究社群的關注所在[5][13]。也因此，探索整合知識圖譜於選擇題自動生成也因此成為我們規劃之研究項目。希冀藉由整合知識圖譜來提升干擾選項之誘導性與題幹生成之正確性。•研究項目二：基於Masked-LM之反事實干擾選項生成研究(Counter-FactualSentenceGenerationbasedonMasked-LM)如同前言所述，“下列何者正確/錯誤”題目為常見之類型選擇題題型。對此，我們預計嘗試以下之作法。首先，我們假定可取得正確敘述之語句S。我們先針對語句S中的字詞進行命名實體辨識(NamedEntityRecognition,NER)，其後再利用語言模型針對所取得之命名實體進行WholeWordMasking(WWM)，填入機率較低之填空結果，藉此生成反事實描述之語句。如Figure4所例，我們將所給定之事實描述，藉由NER方式挑選”葡萄糖”一次進行Masked，再藉由語言模型推測該Masked字詞，並挑選機率相對較低之預測填入。藉此創造出反事實之陳述句。然而，如何確保不挑選到同義詞？要進行多少個字詞之Masked？如何自動從主題文章中挑選事實陳述句以進行改寫？等等•研究項目三：基於知識圖譜強化之反事實干擾選項生成(Counter-FactualSentenceGen-erationbasedonKnowledge-GraphAugmentedLanguageModel)針對反事實干擾選項生成之議題，除了研究項目二之Masked-LM作法外，我們另一個構想為利用知識圖譜來強化Counter-Factual語句之生成。如Figure4所示，我們基本想法為先萃取與題幹相關聯之知識圖譜子圖(LocalSubgraph)並取得知識三元組，以此輸入至語言模型中，訓練語言模型反向改寫三元組語句，以此為反事實干擾選項。然而，此模式之訓練資料該如何取得？如何從知識圖譜上萃取子圖，並生成合適三元組？如何引入GraphNeuralNetwork來萃取結構？為此研究項目需進行探討之工作內容。•研究項目四：基於對比式學習之干擾選項生成模型強化(DistractorGenerationModelbasedonContrastiveLearning)我們過去的研究經驗[8]與相關研究[35]皆指出當使用Generative語言模型來生成句子Figure4:基於知識圖譜強化之反事實干擾選項生成構想層級之輸出時，模型經常會產生語意之相同，但字面的結果。如Table3所示，選項A跟C雖然字面敘述上不同，但實際上都是描述同樣的意思。生成重複選項將導致選擇副交感神經興奮會導致使心跳加速，血壓上升Table3:模型生成的誤導選項重複出現的範例對此，我們預計探索對比式學習(ContrastiveLearning)[7][35]來提升生成品質。對比式學習於自然語言之應用中主要想法為透過選取PositiveSample以及NegativSample使得模型去學習到生成句子跟同義句子的相同以及跟異類句子的不同之處，這樣我們可以使用重複選項當作我們的NegativeSample，然後資料集中的干擾選項當作PostiveSample，期望模型可以學習避免生成出重複的誤導選項。因此，我們預計探索對比式•研究項目五：使用Open-DomainQuestionAnswering錯誤結果之干擾選項挑選研究(Us-ingIncorrectAnswersfromODQAasDistractors)而除了利用生成模型產生Distractors外，我們發現利用Open-DomainQuestionAnswer-ing(ODQA)模型所產生的錯誤結果是一個值得探討的干擾選項的取得方式。具體而言，現有的ODQA模型主要包含一個Retriever與一個Reader，Retriever負責檢索相關的文章，而Reader利用檢索出來的文章利用閱讀理解模型(ReadingComprehensionModel)來找尋答案片段。由於ODQA並非完美，因此也會有錯誤結果之回報。但我們發現錯誤結果與正確答案並非天差地遠，閱讀理解模型所回報之結果通常也與答案接以圖5為例，當我們下達Q：「下列何者為目前世界上應用最廣的殺草劑，其具備有生長素之特性，常用於闊葉雜草防治，亦為選擇性殺草劑的一種？」，ODQA系統將取得圖5中右半部之答案：「萌前除草劑、2,4-D、非選擇性除草劑、10殺丹粒劑」。其中，2,4-D為正確答案，餘者為錯誤答案。然而我們覺得這些錯誤答案，可以當作Q之干擾選項來使用。有別於基於生成模型研究項目，利用ODQA模型之錯誤答案來當作干擾選項應可提供不同思路進行干擾選項生成。這樣的想法乍看可行，但仍有若干技術點需進行探討。諸如，如何控制ODQA模型之錯誤？我們需要在Reader部分弱化Reader的能力，或者弱化Retriever能力，來製造ODQA的錯誤？如何從錯誤回報結果挑選干擾選項？這些皆是尚待深入研究之議題。也因此觸發本研究項目之規劃。Figure5:利用ODQA模型之錯誤答案來當作Distractors•研究項目六：基於NodeCentrality考量主題涵蓋性之選擇題題組生成(MCQGroupGenerationconsideringTopicCoveragebasedGraphNodeCentrality)上述所規劃的選擇題生成做法，主要針對單題進行生成。若要產生多題/題組考題，就是重複相同流程，獨立產生個別考題後，整合為一題組。然而，題組生成做法應當考慮題與題之間之關聯性與互補性。因此，我們認為比較好的做法應當為根據使用者所指定的主題，透過(1)知識圖譜子圖萃取來涵蓋所指定主題之內容與(2)透過主題重要概念萃取：藉由知識圖譜上NodeCentrality方式來萃取關鍵字，以為題幹之生成。此外，如何根據主題互補性，來挑選個別生成之題組，用以組成選擇題題組也是需進行•研究項目七：考量題幹可用性與干擾選項間關聯之Retrieve-and-Generate架構優化(Op-timizingRetrieve-and-GenerateFrameworkconsideringStemUtilityandDistractorCorrela-tion)本研究項目著眼於優化現有Retrieve-and-Generate架構設定。Retrieve-and-Generate架構雖然簡單，但我們可以發現其於單字等級的干擾選項生成已有一定的可用性。我們預計可優化之處為：(1)我們發現引用現有系統於題幹生成時，會有題幹生成結果不佳的狀況。對此，我們發現其中的一個原因為Retriever所檢索之文章品質不佳(文章過短或者與主題不相關)，導致不良之題幹生成。因此如何改善檢索機制來挑選較佳的文章用以進行生成，便是值得探討的課題。再者，題幹生成不佳也有部分原因是來自於問句生成模型本身之生成錯誤，如何有效過濾生成不佳的題幹也因此是一個可需進行研究之項目。(2)如同前言所述，干擾選項之挑選應該考量彼此間之關聯，而非獨立挑選。如何納入此觀點來進行干擾選項之生成也因此成為預計關注之議題。又再者，我們基於word2vec方式之字詞等級干擾選項挑選是否可以引入語言模型之詞向量來求取•研究項目八：生成難度可控之選擇題生成研究Difficulty-ControlledMCQGeneration發展自動化生成考題之可能願景為：針對個人化學習自對應生成難度適中之測驗，以增加學習與檢測之效率。因此，如何針對現有技術探討生成結果難度控制也因此成為一可探索之研究議題。由於我們的基礎架構為基於文章檢索來取得包含使用者所給定的關鍵字之文章，並使用該文章與關鍵字為問題生成模型之輸入，來生成選擇題問句。我們若挑選較為冷門之文章進行生成，則預期可以生成較為困難之題幹。而就干擾選項而言，若我們基於研究項目五「使用Open-DomainQuestionAnswering錯誤結果之干擾選項挑選研究」，利用ODQA來取得干擾選項之作法。我們可藉由調整輸入給閱讀理解模型之文章來產生難度較高的錯誤。具體而言，給定一有包含正確答案A之文章C，我們先將答案片段A於文章D中刪除，而後將其輸入至閱讀理解模型，藉此產生錯誤之閱讀理解結果，以這樣的錯誤結果當干擾選項預期將有較高之誘導能力。相類似的想法也可以套用為挑選Retriever認為相關度比較不高的文章以為閱讀理解模型之輸入。此外，就生成題目之廣度與深度而言，我們可藉由控制取得之文章來調配題組之廣度與深度。例如，若我們以單篇或少數篇文章進行生成，我們所生成的主題題組針對測驗內容而言，應該可產生知識性較為深入之試題。而若針對較多篇文章，則應可產生範圍較廣之測驗試題。上述之研究議題，乍看可行但仍有許多細節需進行探索，如上述研究背景目的所述，本研究計畫擬探索基於現有Retrieve-and-GeneratePipeline之選擇題自動生成架構進行強化。我們以三年為期，規劃在此目標下擬定八項關鍵技術進行研究探討。如Figure6所示，八個工作項目又可區分為三項主軸：「選擇題生成品質提升」、「反事實選項生成」與「群組生成與難度控制」。其中，於「選擇題生成品質提升」研究主軸中，我們探索利用語言模型與知識圖譜架構來生成，將選擇題選項生成問題設定成為一Text2Text任務，輸入為問題題幹與正確答案所串接之TextSequence，而輸出為所生成之distractor串接之TextSequence。此外，我們也嘗試引入對比式學習來改善Text2Text生成模式之品質。我們也預期規劃探索利用ODQA方式來取得干擾選項。而於「反事實選項生成」研究主軸中，我們探索反事實陳述句之生成。我們預計嘗試基於MaskedLM取代重要名詞之反事實語句生成作法與基於知識圖譜知識三元組萃取改寫之作法。於第三個研究主軸中，我們則嘗試針對題組生成與難度可控制之干擾選項生成進行研究。MultipleChoice,Multi-此資料集特色在於為Multi-hop形式，答案需從複數文檔中推理而成；資料來自維基百科以及PubMed，PubMed為生物醫學論文的搜尋引擎，在兩個來源各自提供51,318(Wiki)和2,508(PubMed)筆問題。資料來自國高中英文考試，提供超過10萬筆問題以及2.8萬篇文本，題型上為英文閱讀測驗，會需要閱讀故事或對話紀錄，而針對該文本會有複數題目，每個題目此資料集的挑戰在於，內容來自於7個不同領域(新聞、小學自然科學、旅遊、小說故事等等)，為文本與題目提供多樣性，總計約1萬筆問題與800篇文本。MultipleChoice,Open-為Open-domain形式，並不提供文本，只有問題與答案；包含12,247筆問題、一個正確答案和四個干擾選項；題目類型為知識與常識題，來自於ConceptNet與同時包含兩種形式資料，填空題與Open-domain；填空題形式命名為QUASAR-S，其中資料來源自StackOverflow，提供約37K筆問題。Open-domain形式命名為QUASAR-T，提供約43K筆問題。來自StanfordUniversity的閱讀理解資料集，包含了超過100K筆問題和536篇對應的維基百科文本；延續了SQuAD1.1的題目外，[31]另外加入約50K筆無答案的題目，進一步測試機器能否在無對應答案時回答否，而MedQA是基於美國醫師執照考試(USMLE)所產生的資料集，其中問題皆為較為困難的專業醫學相關的知識，且包含了英文、簡體中文、繁體中文分別有約12K、34K、14K個問題集，這種專業的知識能使機器針對單一領域產生更具參考價值的distractor，目前此任務排名上最佳為ValentinLiévin團隊做出的Codex5-shot訓練機器學習模型進行閱讀理解(MachineReadingComprehensionTask)為最常見的深度學習自然語言任務。諸如，SQuAD[32]、RACE[21]、DROP[10]與HotPotQA[41]等資料集，皆是測驗/訓練機器學習模型於答案找尋、邏輯推理與摘要之能力。其中，許多機器閱讀理解任務採用多選題方式進行測驗。選擇題的自動答題與選擇題的自動生成為一體兩面之應用。因此具有選擇題樣態的資料集為我們選擇題自動生成研究之根本資料集，我們將目前所探知之相選擇題與干擾選項生成之相關研究可按應用類型可區隔為英文學習測驗與知識學習測驗。干擾選項生成研究於英文學習測驗應用，主要以自動生成閱讀理解與克漏字類型的考題為目標。閱讀理解之干擾選項生成與本研究計畫之知識類型選擇題之自動出題目標不同。因此本研究回顧中，我們不做此分類每一研究細部說明。我們將此類型之相關研究按不同象限做分類：(1)干擾選項層級之差異(生成字詞等級與句子等級之差異)象限，(2)題目類型(閱讀理解與克漏字測驗)，(3)生成方式(CandidateRanking與Generative)。我們按此三象限，將目前之相關研究摘要整理於Table5中。而知識學習測驗類型的選擇題生成，則於這一兩年逐漸吸引相關研究投入。我們認為與本研究計畫較為相關之研究有[22][40][3]，以下我們針對個別文獻討論如下。首先，與我們有相同目標者為[22]，也是針對特定知識主題生成選擇題。其主要作法Table5:現有閱讀測驗與克漏字干擾選項生成研究整理表為：先由人工給定一個主題，程式使用該主題名稱的維基百科頁面作為起始點(例如給定“體育”後搜尋體育相關維基百科)，保留熱門文章並取出文章的第一段擷取摘要，由此產生大量句子。並將這些句子依單純英文文法規則生成簡單5W問句與答案，例如who問句是將第一個動詞前的名詞作為正確答案並從句子中移除，what問句是將第一個夾在動詞間的名詞移除作為正確答案並從句子中移除，移除答案後的句子再帶入問題模板生成制式問句。生出五種問句後再依英文文法的詞與詞的相關性去決定哪個問句最適合。在干擾選項生成的部分，先隨機選取其他句子所產生的答案，再藉由語言模型去排序與問題的相關性，取前三高之預測為干擾選項。但本論文並未使用問句生成模型，採取之作法也與本計畫所規劃有所不同。於論文中[40]，則探討如何讓干擾選項更加多樣且與正確答案相差不遠(以具誘導性)。對此，創造了一款監督式分類任務來找出文章中屬於關鍵句子(及對產生干擾選項生成有用的句子)，作者在訓練之前會將訓練用的干擾選項和文章中的每個句子做比對，若該句子和干擾選項相似度達到一定程度，則該句子將成為文章中的關鍵句，透過這個方式在文章中能被修改成為新的干擾選項的句子將會達到多樣性的效果，且透過相似度對比，干擾選項也不會跟原始文章相差太遠從而提升干擾選項的品質。本研究之作法可供我們借鏡參考對關鍵文章/句選擇設計，用以提升干擾選項之多樣性。而與本計畫研究項目五「使用Open-DomainQuestionAnswer錯誤結果之干擾選項」走非生成想法相似之研究為[3]。其主要想法為作者發現於干擾選項資料集中，有許多干擾選項一再被使用。促使論文作者想為新的問題檢索以重複使用干擾選項。作者提出一個Transformer-baseddistractorretrievalmodels，該模型通過考慮問題的上下文來選擇干擾選項模型包含三個部分，分別為D-SIM模型、Q-SIM模型以及DQ-SIM模型。在D-SIM模型中使用mBERT計算同一個MCQ中問題和答案以及對應的干擾選項的向量，並計算他們之間的相似度。而在Q-SIM模型中作者假設不同問題共享一個或多個干擾選項或答案，代表問題可能在語意上相關，因此他們相關的干擾選項可以用作彼此的良好候選干擾選項，因此作者將訓練資料重新排序，使這些共享至少一個干擾選項或答案的問題聚集在一起，接著將資料送進Q-SIM模型計算不同的MCQ中問題之間的相似度。在DQ-SIM模型線性結合前兩個模型，從各自的優勢中獲益，透過基於分數和基於排名的合併策略計算來自D-SIM模型和Q-SIM模型的分數，並設計演算法使得高排名之干擾選項有較大的權重。重複使用現有資料集中之干擾選項為一創新想法，但仍與我們之ODQA做法有所不同。[OpenDomainQuestionAnswering相關研究]本計畫的選擇題生成架構主要基於兩元件:(1)文章萃取與(2)問句生成。目前我們所使用的文章萃取機制為傳統之BM25檢索。而近年來由於ODQA研究領域之發展，基於深度學習之DenseRetrieval技巧也紛紛於ODQA研究領域中所提出。我們預期DenseRetrieval技術也將於我們知識類型選擇題自動出題中擔任關鍵角色。茲簡要回顧現有ODQA之研究如下。ODQA架構中Retriver之重要性不言可喻。不同層級的語意檢索也影響著ODQA的效能。於[28]研究中，作者提出一個簡單而有效的pipeline系統，研究段落和句子級別語義檢索及其對下游任務的潛在影響，作法上是先檢索段落級別的段落，然後從上一步檢索後取得的段落中再去檢索句子級別的段落。該系統在事實驗證和Multi-hopQA方面進行評估，實驗結果顯示QA任務與事實驗證皆仰來上游檢索之結果。於[23]論文中，作者在ODQA架構的retreiver端提出了一個有效率的弱監督式學習的方法(InverseClozeTask)來對retreiver進行預訓練，給予模型完全無人工標記的維基百科原始資料，藉由論文中的方法將資料切分成固定長度的段落，並隨機選取其中的一個句子當作標籤，讓模型去學習如何從給定的句子去資料庫中找尋與之最相關的文章。在實驗數據中此方法訓練出的ODQA模型分數高於傳統的BM25檢索方法。而於DPR[16]論文中，作者為解決傳統BM25無法檢索語意相似段落的問題，提出了基於深度學習模型之DensePassageRetrieval，並提出使用in-batchnegative的技術，在密集向量的檢索取得極佳的效果改進。我們也預計引入中文DRP模型來提升目前題幹生成所需之而於GAR[27]研究中，作者們探討傳統基於BM25的檢索無法檢索沒有單詞重疊或語意相似的段落的問題，而基於密集向量的檢索方式在計算上又過於複雜，且會有資訊損失的問題。該論文透過預訓練語言模型自動生成與查詢語句相關的內容，可增加查詢語句的資訊量，隨後透過BM25進行檢索，該方法比起傳統BM25能檢索到更精確的段落，且執行效率也快於基於密集向量的檢索方式。透過GAR中的PLM可對查詢語句的資訊進行補充，原論文中是補充答案、相關段落及段落標題，可基於該手法讓PLM生成問題與原始答案的關聯，對這些關聯進行組合或刪除即可檢索與原答案有相似關係的查詢語句，可檢索到適合製作干於[19]，作者觀察到基於語言模型之檢索技術雖然效果很好，但比以往的方法需要極高的計算成本，特別是在計算每個query與document間的relevancescore時。本篇論文作者提出了ColBERT，引入lateinteraction架構，對query與document進行獨立的encoding，於模型輸出最終階段才進行相似度運算。而[18]中，則使用近一步基於ColBERT當基礎模型來進行retreiver端的訓練，使用了Relevance-guidedSupervision(RGS)的方法來訓練retreiver的模型。相較於傳統的retreiver訓練資料，傳統方法需人工標註的訓練資料，但作者認為人工標住資料往往存在偏見，進而影響模型的效能，所以在論文中提出了用訓練好的模型來找尋最適合的訓練資料，並把這些資料用於訓練下一個模型，使用這個方式來進行後續的訓練，最後得出最佳的模型。此方法訓練出的模型分數明顯高於目前opendomain的其他方式。在找尋訓練資料時，模型會先搜尋前k筆資料，並把前20筆資料中含有shortanswer的資料當成possitiveexample，而前20筆以外剩餘的資料中不含有shortanswer的資料當成negativeexample，此方法尋找的負面訓練資料交由reader進行預測能得到與正確答案相關的錯誤答案，如能改良ranking的方式，應用於干擾選項生成，預期將能提升干擾選項之品質。[ContrastiveLearning於自然語言生成相關研究]對比式學習始見於電腦視覺領域，其於影像應用上有著顯著的效果提升。因此，許多自然語言處理之研究者也將其引用至自然語言生成領域，諸如[11][7][35]。於[11]研究中，作者們提出了使用ContrastiveLearning的作法來學習SentenceEmbedding，他們使用BERT作為基礎模型，透過不同的dropoutmask使模型能用同樣的輸入產生不同的SentenceEmbedding，但它們之間的距離會是近的，也就可作為PositiveSample。然後再使用其他in-batch的句子作為NegativeSample，以達到非監督式訓練。而在有訓練資料的情況下，如NLI資料集，則可以將Eentailment視為PositiveSample，Contradiction以及其他in-batch的句子視為Negative而再基於[11]研究後，於[7]研究中，作者們提出加上了Discriminator來加強SentenceEmbedding的學習效果。具體而言，對每筆輸入來說，會透過SentenceEncoder產生一個SentenceEmbedding，同時也會隨機的Masking輸入句子其中的幾個Token，並透過一個fixed的Generator來還原MaskToken，而Discriminator要試著透過沒被修改過的SentenceEmbedding的資訊去辨認修改後的句子中的哪一個Token有被修改過，整個框架會透過SentenceEmbedding的ContrastiveLearningLoss以及Discriminator的ReplacedTokenDetection而與我們預計引入對比式學習來提升句子等級的干擾選項生成效果想法接近的研究也可見於[35]。該研究指出發現了以往autoregressive模型使用BeamSearch去決定要生成的文本時，會產生語意不自然以及沒意義的重複文字，儘管後續有人使用取樣以及修改ObjectiveFunction來嘗試解決這個問題，但這個方法時常會產生語意不連貫的問題。因此作者提出了使用ContrastiveLearningLoss加上原先的MaximumLikelihoodEstimationLoss來訓練模型，並且使用ContrastiveSearch來作為decodingmethod。[Counter-FactualSentenceGeneration相關研究]於Counter-FactualGeneration研究中，[44,33]為相關研究之先驅。主要想法是使用神經網路來生成反事實敘述，這個概念背後的想法是同時使用語言模型和各種採樣的策略來達成生成反事實敘述的效果。接續之前的成果，[39]提出了Polyjuice架構，可以控制輸出的反事實敘述的類型。另外一種生成干擾選項的研究是使用干擾選項生成(DistractorGeneration)的手法，干擾選項生成(DistractorGeneration)的目標是給予了一篇文章、問題和正確答案後，模型能夠生成似是而非的干擾選項。於研究[24]中，作者最先提出使用了神經網路來生成干擾選項，而研究[8]則延續了前者的成果，把重點放在生成複數的干擾選項並提出了一個負樣本學習策略(negativetrainingstrategy)來加強干擾選項生成。但這上述兩個研究主要都著眼於閱讀理解題型中干擾選項之生成，與本研究知識類型選擇題生成目標所有不同。而在關於AbductiveInferenceGeneration的相關研究中，於[30]中作者提出了故事改寫的任務，此任務目標是要改寫一段故事的敘述並且導致產生與原先不同的結局，與顯永的論文的方式的差異為故事改寫任務是根據給定的誤導推論來生成不同的結局，在MIG任務中是給定敘述句與其前後文來生成誤導推論。[2]研究中是讓模型根據上下文來推論中間適合加入的句子，此研究主要是產生正確的推論敘述，而在MIG任務[29]主要是產生相反的誤導推論敘述。於我們先前的研究[29]中，我們提出一個自然語言的誤導推論生成任務，任務步驟是當給定了一個陳述事實的句子並給定其上下文，將其定義為(x,y,z)與誤導推論ˆy，模型會生成與該事實相反的推論句子，由此可以讓模型產生出多選題似是而非的干擾選項。然而這樣任務與我們所設定之反事實出題仍有所不同。而與我們生成Counter-Factual陳述句想法接近的研究為：[46]。於該研究中，作者探討有別於過往的是非題，將文章中原始句子作為True的問句並通過反義詞替換關鍵字生成False問句，本篇作者通過使用一種新穎的masking-and-infilling策略來生成更靈活與複雜的問句。要被mask的關鍵字找法有很多種，像是SRL結果中的predicate，句子中連接詞(”that”、”when”、”since”)之後的部分或是數字等等，來找出值得提問的句子。Infilling的作法則是以BART為基底作為生成模型，將原始句子的關鍵字mask後，提供前後句子作為BART的上下文後產生句子填入。作者為了找值得提問的句子所提出的方法亦可用來作為多選題產生題目的答案選擇方法，而infilling的想法如果使用得宜，在提供出題範圍的前後文後，也能將欲出題句子挖空讓BART模型填補後產生選項。雖然與我們目標相似，但仍與我整合知識圖譜與語言模型為近年語言模型訓練熱門關注議題。於[42]研究中，作者們提出一CounterfactualGenerationControllableCounterfactualGenerationDistractorGenerationMisleadingInferenceGeneration種能將文本和知識圖譜的資訊進行深度雙向、自我監督的預訓練的方法，透過作者的方法可以將一些特定領域的KnowledgeBase資料集結合相關文獻去訓練語言知識模型，讓語言模型和知識圖譜之間的資訊互相交換，並且在回答特定領域問答的任務上，表現優於現有的語言模型和知識圖譜增強模型。我們想透過作者提供的預訓練模型，來做多選題選項生成，透過知識圖譜所額外提供的資訊，讓在考慮生成選項時，除了題目和答案的文字外，用這些文字從知識圖譜萃取相關的知識圖譜子圖幫助生成選項有更多參考資訊。知識圖譜於問句生成之研究也見於[5][13]。於[5]研究中，作者之想法為使用知識圖譜進行問題生成任務時，通常只會取出資料與關係的三元組合，並不能完整利用知識圖譜中豐富的資訊，因此作者基於GraphNeuralNetworks提出了BidirectionalGatedGraphNeuralNetwork(BiGGNN)，將知識圖譜中的node及edge編碼成向量，使得每個node都包含了周圍其他node的資訊，並對答案的node進行標註，隨後透過RNN進行問句生成，生成過程中RNN會決定生成新文字或直接複製某個node的資訊放進問題。我們應可借鏡此研究論文的想法，可將知識圖譜及答案的資訊完整編碼，可基於此特性，在生成問題時同時考慮答案附近的node，讓模型在建立問題的同時也能根據附近的node產生干擾選項。而於[13]研究中，則提出的ISEEQ架構，主要目的是希望從簡短的敘述句中去尋找更多訊息來反問使用者是否符合關於敘述句背後未提及的情況，在使用時，使用者只需輸入簡短的敘述句，模型會先從知識圖譜擴展並增加query的關鍵字，透過這些關鍵字模型就能retrieve更多相關的文章進行ranking，在從這些文章中使用QG模型產生一些原始query中未提及但可能相關背景知識，例如當使用者說道:“我感到疲勞且渾身無力”，ISEEQ能回覆:“請問病徵持續了多久?”，或是“你最近14天有出國過嗎?”這類可能與其他知識相關的問句。這種方式最大的貢獻及是讓機器學習到”反問”的能力，而非一味的透過query生成出結果。這樣的想法，或許可引用來提升題幹生成的精確度，舉例來說:有位教師想出關於日治時期的歷史題目，我們就能先藉由ISEEQ來詢問是要出與日治時期戰爭相關的題目或是與日治時期建設相關的題目，以此來取得更精確的題幹生成需求。[1]J.Araki,D.Rajagopal,S.Sankaranarayanan,S.Holm,Y.Yamakawa,andT.Mitamura.Gen-eratingquestionsandmultiple-choiceanswersusingsemanticanalysisoftexts.InProceedingsofCOLING2016,the26thInternationalConferenceonComputationalLinguistics:TechnicalPapers,pages1125–1136,2016.[2]C.Bhagavatula,R.L.Bras,C.Malaviya,K.Sakaguchi,A.Holtzman,H.Rashkin,D.Downey,S.W.-t.Yih,andY.Choi.Abductivecommonsensereasoning.arXivpreprintarXiv:1908.05739,[3]S.K.Bitew,A.Hadifar,L.Sterckx,J.Deleu,C.Develder,andT.Demeester.Learningtoreusedistractorstosupportmultiplechoicequestiongenerationineducation.IEEETransactionsonLearningTechnologies,2022.[4]Y.-H.ChanandY.-C.Fan.Arecurrentbert-basedmodelforquestiongeneration.InProceedingsofthe2ndWorkshoponMachineReadingforQuestionAnswering,pages154–162,2019.[5]Y.Chen,L.Wu,andM.J.Zaki.Towardsubgraphguidedknowledgegraphquestiongenerationwithgraphneuralnetworks.arXivpreprintarXiv:2004.06015,2020.[6]S.-H.Chiang,S.-C.Wang,andY.-C.Fan.Cdgp:Automaticclozedistractorgenerationbasedonpre-trainedlanugagemodel.InFindingsoftheAssociationforComputationalLinguistics:[7]Y.-S.Chuang,R.Dangovski,H.Luo,Y.Zhang,S.Chang,M.Soljac￿ic￿,S.-W.Li,W.-t.Yih,Y.Kim,andJ.Glass.Diffcse:Difference-basedcontrastivelearningforsentenceembeddings.arXivpreprintarXiv:2204.10298,2022.[8]H.-L.Chung,Y.-H.Chan,andY.-C.Fan.Abert-baseddistractorgenerationschemewithmulti-taskingandnegativeanswertrainingstrategies.InFindingsoftheAssociationforComputationalLinguistics:EMNLP2020,pages4390–4400,2020.[9]B.Dhingra,K.Mazaitis,andW.W.Cohen.Quasar:Datasetsforquestionansweringbysearchandreading.arXivpreprintarXiv:1707.03904,2017.[10]D.Dua,Y.Wang,P.Dasigi,G.Stanovsky,S.Singh,andM.Gardner.ingcomprehensionbenchmarkrequiringdiscretereasoningoverparagraphs.arXiv:1903.00161,2019.[11]T.Gao,X.Yao,andD.Chen.Simcse:Simplecontrastivelearningofsentenceembeddings.arXivpreprintarXiv:2104.08821,2021.[12]Y.Gao,L.Bing,P.Li,I.King,andM.R.Lyu.Generatingdistractorsforreadingcomprehen-sionquestionsfromrealexaminations.InProceedingsoftheAAAIConferenceonArtificialIntelligence,volume33,pages6423–6430,2019.[13]M.Gaur,K.Gunaratna,V.Srinivasan,andH.Jin.Iseeq:Informationseekingquestiongenerationusingdynamicmeta-informationretrievalandknowledgegraphs.InProceedingsoftheAAAIConferenceonArtificialIntelligence,volume36,pages10672–10680,2022.[14]Q.Guo,C.Kulkarni,A.Kittur,J.P.Bigham,andE.Brunskill.Questimator:Generatingknowl-edgeassessmentsforarbitrarytopics.InIJCAI-16:ProceedingsoftheAAAITwenty-FifthInternationalJointConferenceonArtificialIntelligence,2016.[15]D.Jin,E.Pan,N.Oufattole,W.-H.Weng,H.Fang,andP.Szolovits.Whatdiseasedoesthispa-tienthave?alarge-scaleopendomainquestionansweringdatasetfrommedicalexams.AppliedSciences,11(14):6421,2021.[16]V.Karpukhin,B.Og￿uz,S.Min,P.Lewis,L.Wu,S.Edunov,D.Chen,andW.-t.Yih.Densepassageretrievalforopen-domainquestionanswering.arXivpreprintarXiv:2004.04906,2020.[17]D.Khashabi,S.Chaturvedi,M.Roth,S.Upadhyay,andD.Roth.Lookingbeyondthesurface:Achallengesetforreadingcomprehensionovermultiplesentences.InProceedingsofthe2018ConferenceoftheNorthAmericanChapteroftheAssociationforComputationalLinguistics:HumanLanguageTechnologies,Volume1(LongPapers),pages252–262,2018.[18]O.Khattab,C.Potts,andM.Zaharia.Relevance-guidedsupervisionforopenqawithcolbert.TransactionsoftheAssociationforComputationalLinguistics,9:929–944,2021.[19]O.KhattabandM.Zaharia.Colbert:Efficientandeffectivepassagesearchviacontextualizedlateinteractionoverbert.InProceedingsofthe43rdInternationalACMSIGIRconferenceonresearchanddevelopmentinInformationRetrieval,pages39–48,2020.[20]G.Kumar,R.E.Banchs,andL.F.D’Haro.Revup:Automaticgap-fillquestiongenerationfromeducationaltexts.InProceedingsoftheTenthWorkshoponInnovativeUseofNLPforBuildingEducationalApplications,pages154–161,2015.[21]G.Lai,Q.Xie,H.Liu,Y.Yang,andE.Hovy.Race:Large-scalereadingcomprehensiondatasetfromexaminations.arXivpreprintarXiv:1704.04683,2017.[22]G.LeBerre,C.Cerisara,P.Langlais,andG.Lapalme.Unsupervisedmultiple-choicequestiongenerationforout-of-domainq&afine-tuning.InProceedingsofthe60thAnnualMeetingoftheAssociationforComputationalLinguistics(Volume2:ShortPapers),pages732–738,2022.[23]K.Lee,M.-W.Chang,andK.Toutanova.Latentretrievalforweaklysupervisedopendomainquestionanswering.arXivpreprintarXiv:1906.00300,2019.[24]C.Liang,X.Yang,N.Dave,D.Wham,B.Pursel,andC.L.Giles.Distractorgenerationformultiplechoicequestionsusinglearningtorank.InProceedingsofthethirteenthworkshoponinnovativeuseofNLPforbuildingeducationalapplications,pages284–290,2018.[25]C.Liang,X.Yang,D.Wham,B.Pursel,R.Passonneaur,andC.L.Giles.Distractorgenera-tionwithgenerativeadversarialnetsforautomaticallycreatingfill-in-the-blankquestions.InProceedingsoftheKnowledgeCaptureConference,pages1–4,2017.[26]V.Liévin,C.E.Hother,andO.Winther.Canlargelanguagemodelsreasonaboutmedicalquestions?arXivpreprintarXiv:2207.08143,2022.[27]Y.Mao,P.He,X.Liu,Y.Shen,J.Gao,J.Han,andW.Chen.Generation-augmentedretrievalforopen-domainquestionanswering.arXivpreprintarXiv:2009.08553,2020.[28]Y.Nie,S.Wang,andM.Bansal.Revealingtheimportanceofsemanticretrievalformachinereadingatscale.arXivpreprintarXiv:1909.08041,2019.[29]H.-Y.Peng,H.-L.Chung,Y.-H.Chan,andY.-C.Fan.Misleadinginferencegenerationviaproxi-malpolicyoptimization.InPacific-AsiaConferenceonKnowledgeDiscoveryandDataMining,pages497–509.Springer,2022.[30]L.Qin,A.Bosselut,A.Holtzman,C.Bhagavatula,E.Clark,andY.Choi.Counterfactualstoryreasoningandgeneration.arXivpreprintarXiv:1909.04076,2019.[31]P.Rajpurkar,R.Jia,andP.Liang.Knowwhatyoudon’tknow:Unanswerablequestionsforsquad.arXivpreprintarXiv:1806.03822,2018.[32]P.Rajpurkar,J.Zhang,K.Lopyrev,andP.Liang.Squad:100,000+questionsformachinecomprehensionoftext.arXivpreprintarXiv:1606.05250,2016.[33]S.Ren,Y.Deng,K.He,andW.Che.Generatingnaturallanguageadversarialexamplesthroughprobabilityweightedwordsaliency.InProceedingsofthe57thannualmeetingoftheassociationforcomputationallinguistics,pages1085–1097,2019.[34]S.RenandK.Q.Zhu.Knowledge-drivendistractorgenerationforcloze-stylemultiplechoicequestions.InProceedingsoftheAAAIConferenceonArtificialIntelligence,volume35,pages[35]Y.Su,T.Lan,Y.Wang,D.Yogatama,L.Kong,andN.Collier.Acontrastiveframeworkforneuraltextgeneration.arXivpreprintarXiv:2202.06417,2022.[36]A.Talmor,J.Herzig,N.Lourie,andJ.Berant.Commonsenseqa:Aquestionansweringchallengetargetingcommonsenseknowledge.arXivpreprintarXiv:1811.00937,2018.[37]J.Welbl,N.F.Liu,andM.Gardner.Crowdsourcingmultiplechoicesciencequestions.arXivpreprintarXiv:1707.06209,2017.[38]J.Welbl,P.Stenetorp,andS.Riedel.Constructingdatasetsformulti-hopreadingcomprehensionacrossdocuments.TransactionsoftheAssociationforComputationalLinguistics,6:287–302,[39]T.Wu,M.T.Ribeiro,J.Heer,andD.S.Weld.Polyjuice:Automated,general-purposecounter-factualgeneration.arXivpreprintarXiv:2101.00288,1(2),2021.[40]J.Xie,N.Peng,Y.Cai,T.Wang,andQ.Huang.Diversedistractorgenerationforconstruct-inghigh-qualitymultiplechoicequestions.IEEE/ACMTransactionsonAudio,Speech,andLanguageProcessing,30:280–291,2021.[41]Z.Yang,P.Qi,S.Zhang,Y.Bengio,W.W.Cohen,R.Salakhutdinov,andC.D.Manning.Hotpotqa:Adatasetfordiverse,explainablemulti-hopquestionanswering.arXiv:1809.09600,2018.[42]M.Yasunaga,A.Bosselut,H.Ren,X.Zhang,C.D.Manning,P.Liang,andJ.Leskovec.Deepbidirectionallanguage-knowledgegraphpretraining.arXivpreprintarXiv:2210.09338,2022.[43]C.Y.Yeung,J.S.Lee,andB.K.Tsou.Difficulty-awaredistractorgenerationforgap-fillitems.InProceedingsoftheThe17thAnnualWorkshopoftheAustralasianLanguageTechnologyAssociation,pages159–164,2019.[44]C.Zhang,Y.Sun,H.Chen,andJ.Wang.Generatingadequatedistractorsformultiple-choicequestions.arXivpreprintarXiv:2010.12658,2020.[45]X.Zhou,S.Luo,andY.Wu.Co-attentionhierarchicalnetwork:Generatingcoherentlongdistractorsforreadingcomprehension.InProceedingsoftheAAAIConferenceonArtificialIntelligence,volume34,pages9725–9732,2020.[46]B.Zou,P.Li,L.Pan,andA.Aw.Automatictrue/falsequestiongenerationforeducationalpur-pose.InProceedingsofthe17thWorkshoponInnovativeUseofNLPforBuildingEducationalApplications(BEA2022),pages61–70,2022.於本研究計畫中，我們目標基於自然語言生成技術進行選擇題自動出題研究。我們預期的展示情境為：給定一知識範圍(如農業文集)，我們可於系統直接輸入一組主題關鍵詞，而後自動生成關於該主題相關之選擇題題組(含字詞等級、句子等級、挑選是非類型之選擇題)。然而，如同前言背景中所言，現有技術離目標仍有許多待強化之處，對此我們擬定三大出題目標：字詞等級干擾選項、句子等級干擾選項與反事實干擾選項進行研究。針對此三大目標，我們擬定(1)基於知識圖譜之強化與(2)基於生成語言模型與對比學習之強化三個核心技術進行探討。研究技術項目與項目間之關係與進行規劃如Figure7所示。於第一年度中，我們規劃「考量題幹可用性與干擾選項間關聯之Retrieve-and-Generate架構優化」、「基於語言模型與知識圖譜之Text2Text干擾選項生成」與「基於Masked-LM之反事實干擾選項生成研究」進行研究。「考量題幹可用性與干擾選項間關聯之Retrieve-and-Generate架構優化」著眼於優化目前https://mcq-demo.nlpnchu.org之WordLevel之選擇題生成系統。希冀自動過濾相似或語意不正確題幹，並且考量干擾選項彼此間之關聯進行最終干擾選項集合之綜合挑選。「基於生成語言模型之Text2Text干擾選項生成模型」則考量Text2Text模式利用生成模型直接生成干擾選項序列，藉此嘗試生成句子等級之干擾選項。並進一步地探索結合知識圖譜與語言模型之干擾選項生成，希冀透過知識圖譜之整合來生成更有誘導性且合理之字詞等級干擾選項。此外，於「基於Masked-LM之反事實干擾選項生成研究」項目中，我們預計著手挑戰反事實干擾選項生成，我們嘗試利用透過NER擷取給定事實陳述句中之關鍵字並Masked-LM該關鍵字來填入機率較低之字詞，以為反事實語句生成之作法。我們預期第一年度工作完成後，將可優化現有單詞等級干擾選項之選擇題自動生成系統，並於第二年度中，我們則規劃「基於知識圖譜強化之反事實干擾選項生成」、「基於對比式學習之干擾選項生成模型強化」與「利用Open-DomainQuestionAnswering錯誤結果之干擾選項挑選」。於此年度中，進一步嘗試利用知識圖譜萃取三元組知識後進行反事實語句之生成，基本想法為透過萃取三元組後再進一步輸入語言模型，並希望語言模型適當地進行反事實語句的改寫。此一部分預計使用第一年度之「基於Masked-LM之反事實干擾選項生成研究」之基礎以為訓練資料。對於字詞等級的干擾選項，我們則探索利用Open-DomainQuestionAnswering模型所錯誤回報之答案當作干擾選項。我們希望透過此不同之想法來取得更多樣的干擾選項。再者，如前言所述，生成模型生成序列時常出現生成結果重複的狀況，對此我們嘗試利用對比式來改善生成模型這方面的缺點，以優化干擾選項之生成。第二年度工作項目完成後，我們可初步提供反事實干擾選項之選擇題生成、優化後之句子等級干擾選項生成與基於ODQA方式取得之字詞等級干擾選項生成。於第三年度中，我們則規劃「基於NodeCentrality考量主題涵蓋性之選擇題題組生成」與「生成難度可控之干擾選項生成研究」。如同前言所述，一個理想的選擇題出題系統應該考慮題組生成，整體題目之主題涵蓋性，對此我們也將嘗試基於知識圖譜涵蓋的方式考慮節點centrality重要度方式來進行題組之生成。同時我們也將嘗試題目難易度之控制生成研究。我們的想法是當自動生成成為可能後未來的出題應該是個人化模式；針對個人的學習狀況進行測驗，因此我們規劃「個人化與生成難度可控之干擾選項生成研究」進行探討。第三年度，我們也預計投入人力進行系統的實作開發與整合。以下茲針對擬訂之研究議題其問題挑戰與1.基於語言模型與知識圖譜之Text2Text干擾選項生成模型近年來，語言模型除了使用CandicateGeneratingandRanking架構的MaskedLM之外，使用Encoder-Decoder架構的Text2Text模型於生成任務上也有很好的成果，因此我們的想法是把干擾選項生成Model視為一個Text2Text任務，嘗試讓模型直接生成干擾選項。且使用Text2Text模型生成Distractor有別於用MaskedLM生成Distractor，我們希望模型能生成的Distractor有以下特性(1)非單詞之Distractor:目前在Distractor生成上多為單詞生成，要生成一段敘述/句子的干擾選項還沒有很好的做法。(2)特定domain之MCQ:目前的MCQ多為英文測驗，若想針對特定領域出選擇題，使用MaskedLM生成的Distractor會因語料庫限制而為達到上述的兩種目標需求，我們設Q為輸入之問句，A為輸入問句之答案，並將Q與A字串接字串當作輸入，訓練模型輸出一連串問句Q的干擾選項字串D=(d1,d2,d3)。模型架構如圖2所示，我們將「哪一種技術最適用於嚴重特殊傳染性肺炎之病原快速篩檢?」與「聚合￿連鎖反應」接在一起輸入至Text2Text模型，而模型生成了「重組DNA技術」、「電腦斷層掃描」與「組織培養」三個干擾選項，三個選項皆為醫療檢測方式且都不是單詞，符合如前言所述，我們也預計探索使用知識圖譜來強化生成干擾選項的能力。如Figure8的圖例，我們可以使用語言模型結合醫療知識圖譜，利用給定的文本從知識圖譜中萃取有包含這些topicentities以及相鄰的nodeentities的子圖，再分別將文本與子圖輸入至語言模型和GraphNeuralNetwork(GNN)，讓模型學習找出關鍵的資訊與強化network中重要entitynodes跟edges的權重，藉由GNN的額外資訊與語言模型能力生成更好的選項。Figure8為我們規劃做法的概觀。主要分兩階段進行：(1)關鍵字子圖與GNNEmbedding萃取與(2)GNNEmbedding與LanugageEmbedding間之FusionEncoder生成。具體而言，第一個階段是將從知識圖譜裡萃取相關的知識圖譜子圖。我們一開始先使用NERModel抓取題幹中的entities，再透過這些entities透過知識圖譜去尋找entitynodes以及所有跟entitynodes相鄰3-hop內的bridgenodes，將檢索出來的結果當成知識圖譜子圖，並在後續第二階段提供以Figure9的圖例，我們先將問題透過NERModel斷詞得到”白血病”、”發病”、”器官”與”發病原因”當作entities，再透過這些entities當作關鍵字透過Retriever從知識圖譜萃取Figure8:結合知識圖譜與語言模型生成之構想架構圖與這些詞相關的node和edge，如利用”白血病”與”器官”能找到”肝臟”、”骨髓”、”肺部”等等entities，我們能利用這些entities來當作後續的生成選項。第二個階段則將文本與第一階段得到的知識圖譜子圖分別輸入至語言模型與GNN進行訓練，訓練目標為將Input的[MASK]Token還原來產生干擾選項，先將語言模型的Input額外新增[MASK]Token，經過N層LMLayer做self-attention後，透過FusionEncoder再結合知識圖譜的資訊與原本的文本去生成選項，透過以上的手法，來強化干擾選項之生成。2.考量題幹可用性與干擾選項間關聯之Retrieve-and-Generate架構優化如同研究目的所述，本研究項目著眼於優化現有Retrieve-and-Generate生成架構。我們•文章檢索器過濾:在Retrieving-and-Generating方法中Retriever會檢索回傳含有主題字詞的文章，但有部分文章並不適合當作問句生成模型的輸入，那些文章含有太多的雜訊，我們想嘗試在這步驟先過濾一些文章。我們的想法是利用詞向量中的相似詞再次檢索Retriever回傳的文章，把過於不相關的文章濾掉，減少問句生成模型輸出過於複•生成題幹品質自動過濾：問句生成模型輸出的問句，會再交由使用者挑選，但為了更好的效果，我們想嘗試利用語言模型做初步的篩選，減少人工挑選時間。我們想嘗試利用ODQA模型來過濾問句，將問句生成模型輸出的問句輸入至問答模型後，根據問答模型的回答結果與信心分數來決定該問句是否合適當作MCQ。如Figure10所例，我們將生成之問句再次投入ODQA模型，透過答案選項之信心度來為問句生成過濾之基Figure10:Unanswerablequestionsolution•考量干擾選項間關聯之干擾選項挑選:如前言所述，一個可以提升選擇題生成品質￿考量干擾選項彼此之間之難易度。而在Retrieving-and-Generating方法中，生成干擾選項的方式為取出詞向量模型中與主題字詞相似之詞，使用相似度前三名當作干擾選項，因此有選項過於相近而造成問題太難的可能。為做改善，我們取出所有相似詞的向量與主題字詞向量計算相對距離，設定各個選項與答案的相似度範圍(距離範圍)，再從中隨機挑選相似詞，讓干擾選項彼此之間難易度有所區分以提升試題之鑑別度。•基於語言模型之干擾選項候選生成:雖然詞向量模型生成distractor過程迅速，但如果有主題字詞沒有被詞向量模型看過或有主題字詞一字多意，就無法生成適當的干擾選項。這個問題我們想嘗試使用MaskedLM來解決，我們將Retriever回傳的文章中含有主題字詞的段落記錄下來，移除主題字詞後輸入MaskedLM，再從MaskedLM輸出的候選字中選出distractor，使用這個方式生成的干擾選項不會被上述的缺點受限，也更Figure11:OptimizedRetrievingandGeneratingFramework3.基於Masked-LM之反事實干擾選項生成研究本研究項目為基於Masked-LM技巧來生成反事實干擾選項。我們需要先查找與事實相符之敘述，再修改成反事實干擾選項。我們預計之作法為：首先讓使用者先輸入考題關鍵字，再使用retriever查找相關文章，其目的為從相關文章中選取與事實相符之敘述。接著，將retriever檢索到的文章回傳給使用者，由使用者選取與事實相符之敘述。並按照以下之步驟進行Masked-LM反事實干擾選項生成。為了抽換專有名詞，我們先使用命名實體與事實相符之敘述中的所有專有名詞，以下圖為例”腎小管藉壓力差方式再吸收葡萄糖回微血管”，透過命名實體模型會萃取出專透過命名實體模型，我們會萃取出多個與事實相符之敘述中的專有名詞，我們需要選擇一個專有名詞行抽換。我們認為該專有名詞在相關文章中的重要程度越高，越高機率是考題中的重點。為了確保抽換專有名詞的品質，我們使用BM25來查看各個專有Figure12:AutoEncoder模型與AutoRegressive模型做MaskLM3.使用Maskedlanguagemodel模型進行改寫Maskedlanguagemodel，簡稱為MaskLM，該任務的會將輸入中的部分內容遮擋起來，模型會根據其他未被遮擋的內容，預測出被遮擋的內容或與其擁有相似語義的內容，現有的MaskLM模型分為兩種，分別為AutoEncoder框架的MaskLM模型與AutoRe-gressive框架的MaskLM模型，經典模型如BERT與BART。針對MaskLM任務，兩種模型進行預測的方式也有所不同，AutoEncoder框架的模型會預測出字，而AutoRe-gressive框架的模型會預測出字、詞或是句子，詳細範例如Figure12。根據抽換專有名詞的需求，我們將使用Autoregressive模型做MaskLM任務來抽換重要的專有名詞。然而模型預測出的內容眾多，有可能是原先遮擋的內容、遮擋內容的同義詞或是與遮擋內容擁有相似語義的內容。理想的情況下，模型預測的內容最高機率為原先遮擋的內容，若我們選擇原先遮擋的內容或是遮擋內容的同義詞進行抽換，會造成與事實相符之敘述未改變，便無法生成反事實干擾選項。為了避免此狀況發生，我們使用NLI任務中的換句話說(MRPC)任務來檢測其是否為干擾選項。換句話說任務主要在判斷兩句話的語意是否相同，為了避免MaskLM模型選用同義詞進行抽換，即模型生成的干擾選項仍是正確的，我們將與事實相符之敘述與生成之干擾選項作為模型的輸入，若模型認為兩句話具有相同語義，我們會讓MaskLM模型預4.基於知識圖譜強化之反事實干擾選項生成於本研究項目中，我們嘗試利用知識圖譜提供語言模型額外資訊達成反事實干擾選項生成。我們的構想如Figure4所示，包含以下步驟。首先我們會讓使用者先輸入考題內文，再將內文使用斷詞或NER模型萃取entity，利用這些entity於知識圖譜中查找相關子圖，子圖必須包含內文中所有的entitynodes以及所有跟entitynodes相鄰節點，以此為知識生成之基礎。接著我們將檢索到的子圖內的nodes與edges萃取成三元組，每筆三元組的結構是以node,edge,node組成。如同圖中所示，每一筆三元組描述一基本事實陳述，如導體可以3.將與事實相符之敘述修改為反事實干擾選項為了將與事實相符之敘述更改為反事實干擾選項，我們將前一步驟所萃取的三元組與考題內文當作作語言模型的輸入來生成反事實干擾選項。我們預計透過生成語言模型與輸入給定的問句和三元組來生成與問句情境相符合的反事實干擾選項，以FigurejQuery36306984126397040076_1676210426716為例，生成語言模型在給定關於與導體相關的題目敘述以及從知識圖譜萃取與導體相關的三元組後，模型藉由這些三元組推斷其中的關係，比如”導體可以導電->石墨是導體->筆芯是由石墨製成”，從上述的三元組推斷出”筆芯可以導電”的事實，也可以改寫成”筆芯不能導電”的反事實干擾選項。5.使用Open-DomainQuestionAnswering錯誤結果之干擾選項挑選研究如前言所述，我們預計使用ODQA模型所回答之錯誤結果來當作干擾選項(如Figure5)。以我們先前曾開發https://agri.nlpnchu.org/之農業問答系統為例，我們發現以上想法所面臨的挑於ODQA的架構下，可能會遇到ODQA正確回答所下達之問題，其回傳結果皆為可行之答案，也因此無法被使用來當干擾選項，對此，我們探索兩種作法來進行調整。–1.使用BM25，且刪除尋找出來文章的正確答案在使用BM25的情境下，因為是使用關鍵字比對，所以會找出許多跟正確答案相關的文章，我們將找到文章的正確答案刪除，這樣這個文章會變成絕對回答不出正確答案的文章，可是因為Reader對文字有一定的理解能力，並且具有關鍵字的文章中也很可能會有，他會從剩下可能的答案中，尋找一個最相近的選項，這樣就可以較容易生出干擾選項。如Figure13所示，我們將答案”2,4-D”從文章中直接刪除後，再交給Reader進行閱讀，藉此確保正確答案不會被回傳。Figure13:基於ODQA架構之Retriever弱化方法–2.使用DensePassageRetrieverDensePassageRetriever[16]，DensePassageRetriever可以將資料庫的文章轉為向量的方式表示，在搜尋的時候也將Query轉為向量，並且比對兩個向量的相似度，這樣就可以找出相似的文章，可是我們想說，可以利用DPR找尋文章相似度的這一點，理論上相似的文章跟Query算出來的相似度會是最高的，我們可以將相似度高的文章拿出來，並且查看有沒有答案出現在上面，如果沒有答案，我們就將他丟給Reader，因為我們覺得如我相似度高，就代表Retriever認為裡面有答案，可是裡面並沒有含有我們的正解，所以這篇文章就含Retriever認為是答案，但是實際上不是答案的文字，所以這時候我們將這篇文章傳給Reader，就會有比較高在我們使用baseline的情境中，有時候也會出現不相關的答案，例如在??中，問模型關於殺草劑的問題，會出現的人力除草，人力除草跟題目所問的殺草劑名稱是不相關的，所以我們需要用一個方法把他過濾掉。在這種情況下，我們如果加入正確答案當作Query一起輸入Retriever，就會增加模型找到正確passage的機率，對我們想找跟正確答案不相關但是相似度較高的Passage會產生反效果，所以我們採用的是從Reader輸出之後把他過濾掉的方法。當Reader從Retriever拿到Passage之後，我們會將正確答案去除，之後再將剩餘的答案丟入一個LanguageModel作判斷，如果他跟答案的距離過於遙遠，就放入NegativeList，如果是相關的就放入PositiveList，之後要挑相關答案的時候可以從PositiveList裡面挑選。6.基於對比式學習之干擾選項生成模型強化ContrastiveLearning的設計上需要考量如何去設定PositiveSample以及NegativeSample。在我們的目標中，因為不希望模型會重複的生成出相同意思的句子，所以我們預計使用同義的句子當作NegativeSample，而資料集中的干擾選項可以被當作PositiveSample。然而，由於我們的資料集中並沒有NagativeSample的選項，也就是同義於模型生成的選項。對此，我們的想法為透過迭代生成來產生我們需要的NegativeSample。具體而言，如Figure14所舉例，在DistractorGenerator的第一次生成中，我們會輸入Question以及Answer使模型生成出干擾選項。而在第二次迭代中，我們除了會輸入QuestionAnswerPair外，還會再加上在上一次迭代中生成的干擾選項，以提示模型先前已生成的Distractor的資訊。對於第二次迭代的輸出來說，第一次迭代輸出的Distractor，即為它的NegativeSample，因為DistractorGenerator必須要生成出與上一次迭代生成的Distractor意義相異的句子，所以它們之間的SentenceEmbedding不能太過接近。Figure14:IterationGenerater示意，在第一次的迭代中生成的Distractor會作為第二次迭代的輸入，以提示模型不要生成出類似的DistractorFigure15:對比式學習於干擾選項生成之預想架構我們所規劃之整體框架，如Figure15所示。我們使用多次迭代的方式來生成Distractor，對於第一次迭代來說，我們會從其他in-batch的data中取得Sentence當作NegativeSample。而在第二次以後的迭代來說，除了其他in-batch的Sentence以外，我們還會將此次迭代前所生成的Distractor加入我們的NegativeSample中。透過NegativeSample和模型輸出的Distractor的dotproduct可以計算出ContrastiveLoss，此外我們也會加入Distractor和Label計算的CrossEntropyLoss，來計算出我們的TotalLoss15。ContrastiveLoss定義如下:x為生成的distractor，p為資料集中的Distractor(Label)，N為NegativeSampleSet，N={n1,n2,...nj}，其中包括先前迭代生成的Distractor以及其他in-batch的選項。7.基於NodeCentrality考量主題涵蓋性之選擇題題組生成如同前言所述，本研究議題之目標為考量主題涵蓋性之選擇題題組生成。因此，我們預計根據使用者所指定的主題，按Figure16架構規劃來進行題組生成。1.透過知識圖譜子圖萃取來涵蓋所指定主題之內容：藉由知識圖譜上NodeCentrality方式來萃取關鍵字，以為題幹之生成。我們的基本想法為在知識圖譜上我們定位出與使用者輸入之主題關鍵字相關之結點後，我們可以試者套入PageRank或者其他圖形上計算NodeCentrality重要節點挑選，並以重要節點為題幹生成之主軸。2.透過上一步取得之重要答案片段後，進行問題之生成。並生成較多的問題數量以為CandidateQuestionPool，而後將問題將題組生成問題設定一成為一選擇最佳化議題(Optimization)：根據主題生成大量的單一題目，而後定義UtilityScore來計算個別題目組合後之優劣，來挑選最佳化的題組以為輸出。於本研究項目中，我們預計探索以下兩種生成難度控制方式。•調整Masked-LM比例之反事實選項難度控制：現有的Masked-LM模型會將重要的專有名詞進行遮擋，模型會透過未遮擋的內容對該專有名詞進行預測。理想的情況下，當我們僅對該專有名詞進行遮擋時，模型可成功地預測出遮擋的專有名詞或語義相近的專有名詞，這不僅是因為MaskLM模型夠強大，也是因為未遮擋的內容提供模型足夠多的資訊讓模型預測，若未遮擋的內容提供的資訊不夠多，則模型預測出的內容有可能會與遮擋的內容語義相差很多。我們根據Masked-LM模型的此特性對題目的難易度進行控制，若我們將多個專有名詞進行遮擋，再讓模型預測時，模型會因為提供的資訊不足，而預測出語義較不相近的專有名詞，即較不可能的答案。以下表為例，我們可以觀察到當[Mask]越多時，我們要判斷該事實是否正確就越形容易。[Mask]借壓力差吸收[Mask]回微血管[Mask]借壓力差吸收[Mask]回[Mask]•調整ODQA檢索文章之難度控制：由於我們的基礎架構為基於文章檢索(DocumentRetrieving)來取得包含使用者所給定的關鍵字之文章，並使用該文章與關鍵字為問題生成模型之輸入，來生成選擇題問句。我們若挑選較為冷門之文章進行生成，則預期可以生成較為困難之題幹。而就干擾選項而言，我們可藉由調整輸入給閱讀理解模型之文章來產生難度較高的錯誤。具體而言，給定一有包含正確答案A之文章C，我們先將答案片段A於文章D中刪除，而後將其輸入至閱讀理解模型，藉此產生錯誤之閱讀理解結果，以這樣的錯誤結果當Distractor預期將有較高之誘導能力。相類似的想法也可以套用為挑選Retriever認為相關度比較不高的文章以為閱讀理解模型之輸入。此外，就生成題目之廣度與深度而言，我們可藉由控制取得之文章來調配題組之廣度與深度。例如，若我們以單篇或少數篇文章進行生成，我們所生成的主題題組針對測驗內容而言，應該可產生知識性較為深入之試題。而若針對較多篇文章，則應可產生範圍較廣本研究計畫將以三年為期，共八項研究議題進行，各研究產出如上文所示，並整合各技術成為一選擇題自動生成系統，預期的展示情境為：我們可於系統直接輸入一知識範圍，系統將利用我們所開發之技術先進行自動關鍵語萃取，而後進行各種類型的選擇題出題並考量題組關聯性。我們也將彙整各研究議題中所發展之技術為一個成為公開原始碼專案，以為後續相關研究之用。並將計畫執行過程所累積的資料，公開希望能吸引更多研究者之投入。對於學術研究、國家發展及其他應用方面預期之貢獻自然語言生成已是未來深度學習領域之熱門研究重點。透過本研究計畫之投入我們將可進行先驅之探索，建立學生團隊與自然語言處理與生成之能力，並持續過去之閱讀測驗自動出題研究成果，以知識類型選擇題自動出題為主要系統開發目標為例，希冀能為國內外探索不同的自然語言生成應用服務。此外若可順利完成也可以成為一成功範例利用人工智慧輔助教師進行測驗的客製化與個人化學習。對比於題庫類型的方式，透過我們技術我們將可動態產生各式各樣不同之考題為每個人量身定做難度適中之考題。就此一觀點出發，我們預期本計畫除學術研究價值外，同時也兼具商業價值，也應可成為EdTech(教育科技)領域之一環。再者，疫情改變了學習與教學的模式。同步與非同步之線上教學已成為教學模式常態之一。線上教學帶來了好處，但也帶來了顧慮。其中，學習的專注度應是諸多顧慮中主要令人擔憂的一環。隔著螢幕學生學習是否專注？目前的教學內容學生是否已跟上進度？等等問題是線上教學學習專注度所在意的問題。對此，我們認為適時與即時地檢測學生學習成效，來提醒學生與教師目前之學習成效，應能適度緩解線上學習專注度之顧慮。因此，我們也常見於MOOCs課程於一個授課單元結束後適時地給予測驗(多數以選擇題測驗)，來確認學生學習成效。然而，人工設計試題耗費心力，一成不變全班同學皆相同試題也可能透過即時訊息(instantmessage)之分享而取得答案，造成檢測之無效。對此，我們認為隨著深度學習技術之快速演進，結合自然語言(NaturalLanguageGeneration)生成技術來進行自動出題技術已成為可能；透過自然語言生成技術自動化出題，產生多樣化的試題，來進行個人化的檢測，將有除了可以增進研讀論文、撰寫程式、架設系統以及撰寫論文的基本能力之外，參與本計畫之研究人員，將可以學習到深度學習與自然語言生成技術之基本觀念、實際應用以及相關的核心技術，在這些基礎之下，透過積極的研究與討論，將可以提升參與計畫人員思考問題與解決困難的重要能力。我們預期可獲得具體技能如下:(1)設計與建立一個深度學習模型之能力，(2)整合不同先進技術之能力，(3)自然語言生成技術之瞭解，(4)資訊檢索相關知識與軟體套件之暸解，(5)佈建深度學習模型於WebScale之能力。"}, "1676210774.983037": {"單位": "", "標題": "(ㄧ)、研究計畫之背景與目的", "作者": "", "發布日": 0, "摘要": "", "全文": "自然語言生成(NaturalLanguageGeneration)將是未來深度學習領域之熱門研究議題。2022年年末ChatGPT語言模型所帶來的示範性與展示效果預期將吸引更多研究目光。而個人於中興大學之研究團隊近年於自然語言生成之耕耘已建立初步研究成果。在過去朝著自然語言生成方向前進，我們探索自動生成試題之研究方向前進。於前期研究計畫「基於深度學習語言模型之閱讀測驗自動化生成研究109-2221-E-005-058-MY3」中，我們著眼於基於深度學習模型進行閱讀測驗(ReadingComprehensionTesting)題型之自動出題。在計畫執行過程中，我們•Ying-HongChan,Yao-ChungFan:KeywordProvisionQuestionGenerationforFacilitatingEducationalReadingComprehensionPreparation.INLG(InternationalNaturalLanguageGen-erationConference)2022.•Ho-LamChung,Ying-HongChan,Yao-ChungFan:ABERT-basedDistractorGenerationSchemewithMulti-taskingandNegativeAnswerTrainingStrategies.EMNLP(EmpiricalMethodsinNaturalLanguageProcessing)Findings2020.•Shang-HsuanChiang,Ssu-ChengWang,Yao-ChungFan:CDGP:AutomaticClozeDistractorGenerationbasedonPre-trainedLanguageModel.EMNLP(EmpiricalMethodsinNaturalLanguageProcessing)Findings2022.•Hsien-YungPeng,Ho-LamChung,Ying-HongChan,Yao-ChungFan:MisleadingInferenceGenerationviaProximalPolicyOptimization.PAKDD(AdvancesinKnowledgeDiscoveryandDataMining-26thPacific-AsiaConference)2022此外我們也將前期計畫之自動出題研究成果進行實作，可於以下兩個網址進行實測。•問句生成QuestionGeneration：給定文章、指定片段，透過自然語言生成模型進行問句生成。目前系統可於https://classic.queratorai.com/進行實測。•克漏字干擾選項生成DistractorGeneration：此一部分針對給定之文章進行克漏字誘導選項生成。目前系統可於：https://cdgp-demo.nlpnchu.org/進行實測。基於過去之研究成果累積，本計劃延續自然語言生成技術於自動出題之研究成果，擬定知識類型選擇題(MultipleChoiceQuestion,MCQ)自動生成為目標。以四技二專統一測驗考試為例(如Table2所示)，知識類型選擇題將包含題幹與四個選項(其中包含正確答案與干擾選項)，主要針對某一知識範圍進行測驗。我們認為知識範圍選擇題之自動生成可分為兩部分：(1)問題題幹(Stem)生成與(2)干擾選項(Distractor)生成。題幹生成主要目標為選擇題問題本身之生成，而干擾選項則著眼於誘導選項之生成。知識類型選擇題為學習測驗最常見的測驗模式之一。對比於閱讀測驗生成，知識類型選擇題的自動生成，將可減輕教師準備個人化教學測驗之負擔，也為學生提供更有效率之學習檢測方式。以現有我們成果為基礎，一個基本之自動選擇題出題系統作法為(如圖1)所示：我們可利用資訊檢索技術(如BM25Retriever)，從給定之文集資料庫(目前使用2021中文維基百科所有中文文章)中檢索主題字詞相關文章後，結合前期計畫所開發之問句生成模型(QuestionGeneration)[4]生成題幹，而後再利用詞向量模型(例如word2vec模型)找出相近字詞以為干擾選項。我們將這樣的構想先行實作一系統(稱之為Retrieve-and-GeneratePipeline)，目前此系統可於https://mcq-demo.nlpnchu.org/使用。使用者可藉由輸入主題相關字詞，選擇題生成系統將自動生成輸入字詞相關之選擇題結果。舉例而言，我們以「骨髓、白血病、淋巴」為主題字詞輸入至系統。我們挑選較佳之成果顯示於Table1。自動生成選擇題AutomaticGeneratedMCQQ1:哪一種癌症種類的統稱中可以看出它會造成不正常白血球的大量增生?(a)白血病、(b)乳腺癌、(c)腎病、(d)帕金森氏症Q1:白血病的發病原因通常是發生在哪一個器官上?(a)肝臟、(b)淋巴、(c)甲狀腺、(d)骨髓Q:白血病是最常見的兒童癌症，其中80%的病例是慢性的哪一種性病變?(a)淋巴結、(b)微血管、(c)淋巴、(d)黏膜Table1:目前選擇題自動生成結果範例。目前系統可於https://mcq-demo.nlpnchu.org/。目前我們挑選較佳之生成結果顯示於此Table中。建議審查委員自行輸入關鍵字詞組，來觀察我們Figure1:Retrieve-and-Generate基本架構。本例中我們以”骨髓、白血病、淋巴”為關鍵詞進行文章檢索，而再使用”骨髓”為答案，分別生成問句與干擾選項。從展示之結果中，我們發現自動選擇題出題之可能性。但仔細觀察會發現現有系統離實際應用仍有空間須待努力。也因此觸發本期研究計畫之提出。具體而言，我們認為選擇題自•句子等級之干擾選項生成(Sentence-LevelDistractorGeneration)首先，關於干擾選項的部分，若仰賴詞向量模型來產生干擾選項，我們所能取得的干擾選項便僅限於單詞(Word-Level)等級。從統測試題中，我們發現許多題目之干擾選項屬於句子等級(Sentence-Level)。以Table2中Q3與Q4選擇題為例，我們可以看到選項皆為句子等級之描述，此為目前基於詞向量來產生干擾選項的做法所不能。•反事實干擾選項生成(Counter-FactualSentenceGeneration)我們觀察到選擇題題型中，有著下列何者為錯誤或正確類型之考題。其干擾選項是一個較長的描述句。以Table2為例，Q5與Q6為分別詢問下列選項何者正確/錯誤。此題型，為變像之是非題。除了句子等級生成外，核心技術牽涉的是反事實(Counter-Factual)句子之生成。如何生成此類型之句子以為干擾選項也將是研究重點所在。•選擇題生成品質提升(MCQQuestionQualityImprovement)此外，我們也看到retreive-and-generatepipeline的做法，所產生的題幹本身有時是一個過於空泛之問題(例如，下列何種疾病為兒童常見之癌症？)或者並非語意正確之語句(例如，中樞性的什麼症狀是缺乏抗利尿激素的?)。如何有效判斷生成不良之語句，減低人工進一步過濾/挑選題目之負擔也是需進行研究之議題。此外，基於詞向量之干擾選項作法也有待商榷;隨機從相近詞向量挑選distractor，是否能確保distractor之誘導品類型I:單詞等級的干擾選項Word-LevelDistractorsQ1：淋巴器官是人體防禦系統中重要的一環，可提供淋巴細胞發育或活化所需要的條件與狀態。下列何者是淋巴球進行防禦與活化的器官？(a)骨髓(b)胸腺(c)肝臟(d)脾臟Q2：健康人體感覺神經傳入大腦的訊息大部分經過轉接，(a)視丘(b)下視丘(c)中腦(d)橋腦類型II:句子等級的干擾選項Sentence-LevelDistractors(a)無就巢性(b)體型大(c)著肉多(d)耐粗食Q4：下列何者為「聯合國國際糧農組織」(FAO)成立最重要的目標及首要任務？(a)消除及戰勝飢餓(b)聯絡及交流農業資訊(c)自然環境的永續及保護(d)實現國際間的公平貿易類型III:句子等級且與事實相違背的干擾選項Counter-FactualDistractorsQ5：有關人類上皮細胞的細胞週期之敘述，下列何者正確？(c)後期，分離的染色分體往細胞中央移動(d)末期，細胞膜內凹且細胞質開始一分為二Q6：有關一健康成人泌尿系統功能的敘述，下列何者錯誤？(a)腎臟每日約產生濾液180公升及尿液1.5公升(b)腎小管藉壓力差方式再吸收葡萄糖回微血管(c)腎臟藉排尿量多寡來維持體內水分的恆定(d)若出現尿毒症狀時，可使用血液透析方式進行治療Table2:四技二專統一測驗選擇題類型概觀。Q1與Q2擷取自110學年度護理基礎生物考科，Q3與Q4110學年度農業群農業概論考科，Q5與Q6取自110學年度農業群基礎生物考科質？也許所挑選的distractor並不具誘導性，使得受測者很輕易地可挑選出正確答案，或者挑選到答案的同義詞，造成一題有多個正確答案。根據先前計畫執行訪談教師所知，於測驗時為考量受測全體鑑別度，教師出題時會考量干擾選項彼此之間之難易度；通常三個干擾選項中，會有一個離正確答案較遠之干擾選項(誘導性較低)，一個次之，一個最為困難最具有誘導性。藉此來有適當的鑑別度。目前我們直接使用詞向量之作法並未考量此設計。•主題題組生成(MCQQuestionGroupGeneration)目前我們的系統僅為單一關鍵字詞之出題。我們認為一個良好的自動出題應該是以題組為單位進行出題；使用者指定一主題知識，系統根據此一主題知識範圍進行5-10題之選擇題自動生成，題目之間也應該彼此考量。目前Retreive-and-Generatepipeline的做法，我們使用使用者給定的關鍵字為答案，進行題目之生成。然而比較好的做法應為將使用者給定的關鍵字集合為知識範圍之指定，系統根據該範圍自動挑選重要概念關鍵字進行答案挑選，而後再進行選擇題題組之生成。針對上述之研究挑戰，於本研究規劃中我們擬定下列八項研究議題進行探討。•研究項目ㄧ：基於語言模型與知識圖譜之Text2Text干擾選項生成模型(DistractorGenerationbasedonText2TextLanguageModelandKnowledgeGraph)Figure2:Text2Text干擾選項生成架構Figure3:基於Masked-LM之反事實干擾選項現有之Distractor生成研究多是基於CandicateGenerating-and-Ranking架構，先透過知識庫(如Wordnet)或者透過MaskedLM的方式取得候選字後，再適當進行候選詞特徵排序以為最終干擾選項。對此我們有不同之想法，我們預計嘗試把干擾選項生成設定為一個Text2Text任務；如圖2所例，預期的輸入為題幹Q與答案A之串接字串，透過ModelEncoder與Decoder轉化輸出為一連串之干擾選項字串“重組DNA技術[SEP]電腦斷層掃描[SEP]組織培養”。我們透過語言生成模型直接將輸入對比轉換為輸出。此外，透過此一Text2Text方式，生成句子等級的干擾選項也將為可能。再者，近年來語言模型研究趨勢除了將模型參數量與訓練資料持續增加外，引入知識圖譜來強化生成模型於常識理解與推理能力也成為研究社群的關注所在[5][13]。也因此，探索整合知識圖譜於選擇題自動生成也因此成為我們規劃之研究項目。希冀藉由整合知識圖譜來提升干擾選項之誘導性與題幹生成之正確性。•研究項目二：基於Masked-LM之反事實干擾選項生成研究(Counter-FactualSentenceGenerationbasedonMasked-LM)如同前言所述，“下列何者正確/錯誤”題目為常見之類型選擇題題型。對此，我們預計嘗試以下之作法。首先，我們假定可取得正確敘述之語句S。我們先針對語句S中的字詞進行命名實體辨識(NamedEntityRecognition,NER)，其後再利用語言模型針對所取得之命名實體進行WholeWordMasking(WWM)，填入機率較低之填空結果，藉此生成反事實描述之語句。如Figure4所例，我們將所給定之事實描述，藉由NER方式挑選”葡萄糖”一次進行Masked，再藉由語言模型推測該Masked字詞，並挑選機率相對較低之預測填入。藉此創造出反事實之陳述句。然而，如何確保不挑選到同義詞？要進行多少個字詞之Masked？如何自動從主題文章中挑選事實陳述句以進行改寫？等等•研究項目三：基於知識圖譜強化之反事實干擾選項生成(Counter-FactualSentenceGen-erationbasedonKnowledge-GraphAugmentedLanguageModel)針對反事實干擾選項生成之議題，除了研究項目二之Masked-LM作法外，我們另一個構想為利用知識圖譜來強化Counter-Factual語句之生成。如Figure4所示，我們基本想法為先萃取與題幹相關聯之知識圖譜子圖(LocalSubgraph)並取得知識三元組，以此輸入至語言模型中，訓練語言模型反向改寫三元組語句，以此為反事實干擾選項。然而，此模式之訓練資料該如何取得？如何從知識圖譜上萃取子圖，並生成合適三元組？如何引入GraphNeuralNetwork來萃取結構？為此研究項目需進行探討之工作內容。•研究項目四：基於對比式學習之干擾選項生成模型強化(DistractorGenerationModelbasedonContrastiveLearning)我們過去的研究經驗[8]與相關研究[35]皆指出當使用Generative語言模型來生成句子Figure4:基於知識圖譜強化之反事實干擾選項生成構想層級之輸出時，模型經常會產生語意之相同，但字面的結果。如Table3所示，選項A跟C雖然字面敘述上不同，但實際上都是描述同樣的意思。生成重複選項將導致選擇副交感神經興奮會導致使心跳加速，血壓上升Table3:模型生成的誤導選項重複出現的範例對此，我們預計探索對比式學習(ContrastiveLearning)[7][35]來提升生成品質。對比式學習於自然語言之應用中主要想法為透過選取PositiveSample以及NegativSample使得模型去學習到生成句子跟同義句子的相同以及跟異類句子的不同之處，這樣我們可以使用重複選項當作我們的NegativeSample，然後資料集中的干擾選項當作PostiveSample，期望模型可以學習避免生成出重複的誤導選項。因此，我們預計探索對比式•研究項目五：使用Open-DomainQuestionAnswering錯誤結果之干擾選項挑選研究(Us-ingIncorrectAnswersfromODQAasDistractors)而除了利用生成模型產生Distractors外，我們發現利用Open-DomainQuestionAnswer-ing(ODQA)模型所產生的錯誤結果是一個值得探討的干擾選項的取得方式。具體而言，現有的ODQA模型主要包含一個Retriever與一個Reader，Retriever負責檢索相關的文章，而Reader利用檢索出來的文章利用閱讀理解模型(ReadingComprehensionModel)來找尋答案片段。由於ODQA並非完美，因此也會有錯誤結果之回報。但我們發現錯誤結果與正確答案並非天差地遠，閱讀理解模型所回報之結果通常也與答案接以圖5為例，當我們下達Q：「下列何者為目前世界上應用最廣的殺草劑，其具備有生長素之特性，常用於闊葉雜草防治，亦為選擇性殺草劑的一種？」，ODQA系統將取得圖5中右半部之答案：「萌前除草劑、2,4-D、非選擇性除草劑、10殺丹粒劑」。其中，2,4-D為正確答案，餘者為錯誤答案。然而我們覺得這些錯誤答案，可以當作Q之干擾選項來使用。有別於基於生成模型研究項目，利用ODQA模型之錯誤答案來當作干擾選項應可提供不同思路進行干擾選項生成。這樣的想法乍看可行，但仍有若干技術點需進行探討。諸如，如何控制ODQA模型之錯誤？我們需要在Reader部分弱化Reader的能力，或者弱化Retriever能力，來製造ODQA的錯誤？如何從錯誤回報結果挑選干擾選項？這些皆是尚待深入研究之議題。也因此觸發本研究項目之規劃。Figure5:利用ODQA模型之錯誤答案來當作Distractors•研究項目六：基於NodeCentrality考量主題涵蓋性之選擇題題組生成(MCQGroupGenerationconsideringTopicCoveragebasedGraphNodeCentrality)上述所規劃的選擇題生成做法，主要針對單題進行生成。若要產生多題/題組考題，就是重複相同流程，獨立產生個別考題後，整合為一題組。然而，題組生成做法應當考慮題與題之間之關聯性與互補性。因此，我們認為比較好的做法應當為根據使用者所指定的主題，透過(1)知識圖譜子圖萃取來涵蓋所指定主題之內容與(2)透過主題重要概念萃取：藉由知識圖譜上NodeCentrality方式來萃取關鍵字，以為題幹之生成。此外，如何根據主題互補性，來挑選個別生成之題組，用以組成選擇題題組也是需進行•研究項目七：考量題幹可用性與干擾選項間關聯之Retrieve-and-Generate架構優化(Op-timizingRetrieve-and-GenerateFrameworkconsideringStemUtilityandDistractorCorrela-tion)本研究項目著眼於優化現有Retrieve-and-Generate架構設定。Retrieve-and-Generate架構雖然簡單，但我們可以發現其於單字等級的干擾選項生成已有一定的可用性。我們預計可優化之處為：(1)我們發現引用現有系統於題幹生成時，會有題幹生成結果不佳的狀況。對此，我們發現其中的一個原因為Retriever所檢索之文章品質不佳(文章過短或者與主題不相關)，導致不良之題幹生成。因此如何改善檢索機制來挑選較佳的文章用以進行生成，便是值得探討的課題。再者，題幹生成不佳也有部分原因是來自於問句生成模型本身之生成錯誤，如何有效過濾生成不佳的題幹也因此是一個可需進行研究之項目。(2)如同前言所述，干擾選項之挑選應該考量彼此間之關聯，而非獨立挑選。如何納入此觀點來進行干擾選項之生成也因此成為預計關注之議題。又再者，我們基於word2vec方式之字詞等級干擾選項挑選是否可以引入語言模型之詞向量來求取•研究項目八：生成難度可控之選擇題生成研究Difficulty-ControlledMCQGeneration發展自動化生成考題之可能願景為：針對個人化學習自對應生成難度適中之測驗，以增加學習與檢測之效率。因此，如何針對現有技術探討生成結果難度控制也因此成為一可探索之研究議題。由於我們的基礎架構為基於文章檢索來取得包含使用者所給定的關鍵字之文章，並使用該文章與關鍵字為問題生成模型之輸入，來生成選擇題問句。我們若挑選較為冷門之文章進行生成，則預期可以生成較為困難之題幹。而就干擾選項而言，若我們基於研究項目五「使用Open-DomainQuestionAnswering錯誤結果之干擾選項挑選研究」，利用ODQA來取得干擾選項之作法。我們可藉由調整輸入給閱讀理解模型之文章來產生難度較高的錯誤。具體而言，給定一有包含正確答案A之文章C，我們先將答案片段A於文章D中刪除，而後將其輸入至閱讀理解模型，藉此產生錯誤之閱讀理解結果，以這樣的錯誤結果當干擾選項預期將有較高之誘導能力。相類似的想法也可以套用為挑選Retriever認為相關度比較不高的文章以為閱讀理解模型之輸入。此外，就生成題目之廣度與深度而言，我們可藉由控制取得之文章來調配題組之廣度與深度。例如，若我們以單篇或少數篇文章進行生成，我們所生成的主題題組針對測驗內容而言，應該可產生知識性較為深入之試題。而若針對較多篇文章，則應可產生範圍較廣之測驗試題。上述之研究議題，乍看可行但仍有許多細節需進行探索，如上述研究背景目的所述，本研究計畫擬探索基於現有Retrieve-and-GeneratePipeline之選擇題自動生成架構進行強化。我們以三年為期，規劃在此目標下擬定八項關鍵技術進行研究探討。如Figure6所示，八個工作項目又可區分為三項主軸：「選擇題生成品質提升」、「反事實選項生成」與「群組生成與難度控制」。其中，於「選擇題生成品質提升」研究主軸中，我們探索利用語言模型與知識圖譜架構來生成，將選擇題選項生成問題設定成為一Text2Text任務，輸入為問題題幹與正確答案所串接之TextSequence，而輸出為所生成之distractor串接之TextSequence。此外，我們也嘗試引入對比式學習來改善Text2Text生成模式之品質。我們也預期規劃探索利用ODQA方式來取得干擾選項。而於「反事實選項生成」研究主軸中，我們探索反事實陳述句之生成。我們預計嘗試基於MaskedLM取代重要名詞之反事實語句生成作法與基於知識圖譜知識三元組萃取改寫之作法。於第三個研究主軸中，我們則嘗試針對題組生成與難度可控制之干擾選項生成進行研究。MultipleChoice,Multi-此資料集特色在於為Multi-hop形式，答案需從複數文檔中推理而成；資料來自維基百科以及PubMed，PubMed為生物醫學論文的搜尋引擎，在兩個來源各自提供51,318(Wiki)和2,508(PubMed)筆問題。資料來自國高中英文考試，提供超過10萬筆問題以及2.8萬篇文本，題型上為英文閱讀測驗，會需要閱讀故事或對話紀錄，而針對該文本會有複數題目，每個題目此資料集的挑戰在於，內容來自於7個不同領域(新聞、小學自然科學、旅遊、小說故事等等)，為文本與題目提供多樣性，總計約1萬筆問題與800篇文本。MultipleChoice,Open-為Open-domain形式，並不提供文本，只有問題與答案；包含12,247筆問題、一個正確答案和四個干擾選項；題目類型為知識與常識題，來自於ConceptNet與同時包含兩種形式資料，填空題與Open-domain；填空題形式命名為QUASAR-S，其中資料來源自StackOverflow，提供約37K筆問題。Open-domain形式命名為QUASAR-T，提供約43K筆問題。來自StanfordUniversity的閱讀理解資料集，包含了超過100K筆問題和536篇對應的維基百科文本；延續了SQuAD1.1的題目外，[31]另外加入約50K筆無答案的題目，進一步測試機器能否在無對應答案時回答否，而MedQA是基於美國醫師執照考試(USMLE)所產生的資料集，其中問題皆為較為困難的專業醫學相關的知識，且包含了英文、簡體中文、繁體中文分別有約12K、34K、14K個問題集，這種專業的知識能使機器針對單一領域產生更具參考價值的distractor，目前此任務排名上最佳為ValentinLiévin團隊做出的Codex5-shot訓練機器學習模型進行閱讀理解(MachineReadingComprehensionTask)為最常見的深度學習自然語言任務。諸如，SQuAD[32]、RACE[21]、DROP[10]與HotPotQA[41]等資料集，皆是測驗/訓練機器學習模型於答案找尋、邏輯推理與摘要之能力。其中，許多機器閱讀理解任務採用多選題方式進行測驗。選擇題的自動答題與選擇題的自動生成為一體兩面之應用。因此具有選擇題樣態的資料集為我們選擇題自動生成研究之根本資料集，我們將目前所探知之相選擇題與干擾選項生成之相關研究可按應用類型可區隔為英文學習測驗與知識學習測驗。干擾選項生成研究於英文學習測驗應用，主要以自動生成閱讀理解與克漏字類型的考題為目標。閱讀理解之干擾選項生成與本研究計畫之知識類型選擇題之自動出題目標不同。因此本研究回顧中，我們不做此分類每一研究細部說明。我們將此類型之相關研究按不同象限做分類：(1)干擾選項層級之差異(生成字詞等級與句子等級之差異)象限，(2)題目類型(閱讀理解與克漏字測驗)，(3)生成方式(CandidateRanking與Generative)。我們按此三象限，將目前之相關研究摘要整理於Table5中。而知識學習測驗類型的選擇題生成，則於這一兩年逐漸吸引相關研究投入。我們認為與本研究計畫較為相關之研究有[22][40][3]，以下我們針對個別文獻討論如下。首先，與我們有相同目標者為[22]，也是針對特定知識主題生成選擇題。其主要作法Table5:現有閱讀測驗與克漏字干擾選項生成研究整理表為：先由人工給定一個主題，程式使用該主題名稱的維基百科頁面作為起始點(例如給定“體育”後搜尋體育相關維基百科)，保留熱門文章並取出文章的第一段擷取摘要，由此產生大量句子。並將這些句子依單純英文文法規則生成簡單5W問句與答案，例如who問句是將第一個動詞前的名詞作為正確答案並從句子中移除，what問句是將第一個夾在動詞間的名詞移除作為正確答案並從句子中移除，移除答案後的句子再帶入問題模板生成制式問句。生出五種問句後再依英文文法的詞與詞的相關性去決定哪個問句最適合。在干擾選項生成的部分，先隨機選取其他句子所產生的答案，再藉由語言模型去排序與問題的相關性，取前三高之預測為干擾選項。但本論文並未使用問句生成模型，採取之作法也與本計畫所規劃有所不同。於論文中[40]，則探討如何讓干擾選項更加多樣且與正確答案相差不遠(以具誘導性)。對此，創造了一款監督式分類任務來找出文章中屬於關鍵句子(及對產生干擾選項生成有用的句子)，作者在訓練之前會將訓練用的干擾選項和文章中的每個句子做比對，若該句子和干擾選項相似度達到一定程度，則該句子將成為文章中的關鍵句，透過這個方式在文章中能被修改成為新的干擾選項的句子將會達到多樣性的效果，且透過相似度對比，干擾選項也不會跟原始文章相差太遠從而提升干擾選項的品質。本研究之作法可供我們借鏡參考對關鍵文章/句選擇設計，用以提升干擾選項之多樣性。而與本計畫研究項目五「使用Open-DomainQuestionAnswer錯誤結果之干擾選項」走非生成想法相似之研究為[3]。其主要想法為作者發現於干擾選項資料集中，有許多干擾選項一再被使用。促使論文作者想為新的問題檢索以重複使用干擾選項。作者提出一個Transformer-baseddistractorretrievalmodels，該模型通過考慮問題的上下文來選擇干擾選項模型包含三個部分，分別為D-SIM模型、Q-SIM模型以及DQ-SIM模型。在D-SIM模型中使用mBERT計算同一個MCQ中問題和答案以及對應的干擾選項的向量，並計算他們之間的相似度。而在Q-SIM模型中作者假設不同問題共享一個或多個干擾選項或答案，代表問題可能在語意上相關，因此他們相關的干擾選項可以用作彼此的良好候選干擾選項，因此作者將訓練資料重新排序，使這些共享至少一個干擾選項或答案的問題聚集在一起，接著將資料送進Q-SIM模型計算不同的MCQ中問題之間的相似度。在DQ-SIM模型線性結合前兩個模型，從各自的優勢中獲益，透過基於分數和基於排名的合併策略計算來自D-SIM模型和Q-SIM模型的分數，並設計演算法使得高排名之干擾選項有較大的權重。重複使用現有資料集中之干擾選項為一創新想法，但仍與我們之ODQA做法有所不同。[OpenDomainQuestionAnswering相關研究]本計畫的選擇題生成架構主要基於兩元件:(1)文章萃取與(2)問句生成。目前我們所使用的文章萃取機制為傳統之BM25檢索。而近年來由於ODQA研究領域之發展，基於深度學習之DenseRetrieval技巧也紛紛於ODQA研究領域中所提出。我們預期DenseRetrieval技術也將於我們知識類型選擇題自動出題中擔任關鍵角色。茲簡要回顧現有ODQA之研究如下。ODQA架構中Retriver之重要性不言可喻。不同層級的語意檢索也影響著ODQA的效能。於[28]研究中，作者提出一個簡單而有效的pipeline系統，研究段落和句子級別語義檢索及其對下游任務的潛在影響，作法上是先檢索段落級別的段落，然後從上一步檢索後取得的段落中再去檢索句子級別的段落。該系統在事實驗證和Multi-hopQA方面進行評估，實驗結果顯示QA任務與事實驗證皆仰來上游檢索之結果。於[23]論文中，作者在ODQA架構的retreiver端提出了一個有效率的弱監督式學習的方法(InverseClozeTask)來對retreiver進行預訓練，給予模型完全無人工標記的維基百科原始資料，藉由論文中的方法將資料切分成固定長度的段落，並隨機選取其中的一個句子當作標籤，讓模型去學習如何從給定的句子去資料庫中找尋與之最相關的文章。在實驗數據中此方法訓練出的ODQA模型分數高於傳統的BM25檢索方法。而於DPR[16]論文中，作者為解決傳統BM25無法檢索語意相似段落的問題，提出了基於深度學習模型之DensePassageRetrieval，並提出使用in-batchnegative的技術，在密集向量的檢索取得極佳的效果改進。我們也預計引入中文DRP模型來提升目前題幹生成所需之而於GAR[27]研究中，作者們探討傳統基於BM25的檢索無法檢索沒有單詞重疊或語意相似的段落的問題，而基於密集向量的檢索方式在計算上又過於複雜，且會有資訊損失的問題。該論文透過預訓練語言模型自動生成與查詢語句相關的內容，可增加查詢語句的資訊量，隨後透過BM25進行檢索，該方法比起傳統BM25能檢索到更精確的段落，且執行效率也快於基於密集向量的檢索方式。透過GAR中的PLM可對查詢語句的資訊進行補充，原論文中是補充答案、相關段落及段落標題，可基於該手法讓PLM生成問題與原始答案的關聯，對這些關聯進行組合或刪除即可檢索與原答案有相似關係的查詢語句，可檢索到適合製作干於[19]，作者觀察到基於語言模型之檢索技術雖然效果很好，但比以往的方法需要極高的計算成本，特別是在計算每個query與document間的relevancescore時。本篇論文作者提出了ColBERT，引入lateinteraction架構，對query與document進行獨立的encoding，於模型輸出最終階段才進行相似度運算。而[18]中，則使用近一步基於ColBERT當基礎模型來進行retreiver端的訓練，使用了Relevance-guidedSupervision(RGS)的方法來訓練retreiver的模型。相較於傳統的retreiver訓練資料，傳統方法需人工標註的訓練資料，但作者認為人工標住資料往往存在偏見，進而影響模型的效能，所以在論文中提出了用訓練好的模型來找尋最適合的訓練資料，並把這些資料用於訓練下一個模型，使用這個方式來進行後續的訓練，最後得出最佳的模型。此方法訓練出的模型分數明顯高於目前opendomain的其他方式。在找尋訓練資料時，模型會先搜尋前k筆資料，並把前20筆資料中含有shortanswer的資料當成possitiveexample，而前20筆以外剩餘的資料中不含有shortanswer的資料當成negativeexample，此方法尋找的負面訓練資料交由reader進行預測能得到與正確答案相關的錯誤答案，如能改良ranking的方式，應用於干擾選項生成，預期將能提升干擾選項之品質。[ContrastiveLearning於自然語言生成相關研究]對比式學習始見於電腦視覺領域，其於影像應用上有著顯著的效果提升。因此，許多自然語言處理之研究者也將其引用至自然語言生成領域，諸如[11][7][35]。於[11]研究中，作者們提出了使用ContrastiveLearning的作法來學習SentenceEmbedding，他們使用BERT作為基礎模型，透過不同的dropoutmask使模型能用同樣的輸入產生不同的SentenceEmbedding，但它們之間的距離會是近的，也就可作為PositiveSample。然後再使用其他in-batch的句子作為NegativeSample，以達到非監督式訓練。而在有訓練資料的情況下，如NLI資料集，則可以將Eentailment視為PositiveSample，Contradiction以及其他in-batch的句子視為Negative而再基於[11]研究後，於[7]研究中，作者們提出加上了Discriminator來加強SentenceEmbedding的學習效果。具體而言，對每筆輸入來說，會透過SentenceEncoder產生一個SentenceEmbedding，同時也會隨機的Masking輸入句子其中的幾個Token，並透過一個fixed的Generator來還原MaskToken，而Discriminator要試著透過沒被修改過的SentenceEmbedding的資訊去辨認修改後的句子中的哪一個Token有被修改過，整個框架會透過SentenceEmbedding的ContrastiveLearningLoss以及Discriminator的ReplacedTokenDetection而與我們預計引入對比式學習來提升句子等級的干擾選項生成效果想法接近的研究也可見於[35]。該研究指出發現了以往autoregressive模型使用BeamSearch去決定要生成的文本時，會產生語意不自然以及沒意義的重複文字，儘管後續有人使用取樣以及修改ObjectiveFunction來嘗試解決這個問題，但這個方法時常會產生語意不連貫的問題。因此作者提出了使用ContrastiveLearningLoss加上原先的MaximumLikelihoodEstimationLoss來訓練模型，並且使用ContrastiveSearch來作為decodingmethod。[Counter-FactualSentenceGeneration相關研究]於Counter-FactualGeneration研究中，[44,33]為相關研究之先驅。主要想法是使用神經網路來生成反事實敘述，這個概念背後的想法是同時使用語言模型和各種採樣的策略來達成生成反事實敘述的效果。接續之前的成果，[39]提出了Polyjuice架構，可以控制輸出的反事實敘述的類型。另外一種生成干擾選項的研究是使用干擾選項生成(DistractorGeneration)的手法，干擾選項生成(DistractorGeneration)的目標是給予了一篇文章、問題和正確答案後，模型能夠生成似是而非的干擾選項。於研究[24]中，作者最先提出使用了神經網路來生成干擾選項，而研究[8]則延續了前者的成果，把重點放在生成複數的干擾選項並提出了一個負樣本學習策略(negativetrainingstrategy)來加強干擾選項生成。但這上述兩個研究主要都著眼於閱讀理解題型中干擾選項之生成，與本研究知識類型選擇題生成目標所有不同。而在關於AbductiveInferenceGeneration的相關研究中，於[30]中作者提出了故事改寫的任務，此任務目標是要改寫一段故事的敘述並且導致產生與原先不同的結局，與顯永的論文的方式的差異為故事改寫任務是根據給定的誤導推論來生成不同的結局，在MIG任務中是給定敘述句與其前後文來生成誤導推論。[2]研究中是讓模型根據上下文來推論中間適合加入的句子，此研究主要是產生正確的推論敘述，而在MIG任務[29]主要是產生相反的誤導推論敘述。於我們先前的研究[29]中，我們提出一個自然語言的誤導推論生成任務，任務步驟是當給定了一個陳述事實的句子並給定其上下文，將其定義為(x,y,z)與誤導推論ˆy，模型會生成與該事實相反的推論句子，由此可以讓模型產生出多選題似是而非的干擾選項。然而這樣任務與我們所設定之反事實出題仍有所不同。而與我們生成Counter-Factual陳述句想法接近的研究為：[46]。於該研究中，作者探討有別於過往的是非題，將文章中原始句子作為True的問句並通過反義詞替換關鍵字生成False問句，本篇作者通過使用一種新穎的masking-and-infilling策略來生成更靈活與複雜的問句。要被mask的關鍵字找法有很多種，像是SRL結果中的predicate，句子中連接詞(”that”、”when”、”since”)之後的部分或是數字等等，來找出值得提問的句子。Infilling的作法則是以BART為基底作為生成模型，將原始句子的關鍵字mask後，提供前後句子作為BART的上下文後產生句子填入。作者為了找值得提問的句子所提出的方法亦可用來作為多選題產生題目的答案選擇方法，而infilling的想法如果使用得宜，在提供出題範圍的前後文後，也能將欲出題句子挖空讓BART模型填補後產生選項。雖然與我們目標相似，但仍與我整合知識圖譜與語言模型為近年語言模型訓練熱門關注議題。於[42]研究中，作者們提出一CounterfactualGenerationControllableCounterfactualGenerationDistractorGenerationMisleadingInferenceGeneration種能將文本和知識圖譜的資訊進行深度雙向、自我監督的預訓練的方法，透過作者的方法可以將一些特定領域的KnowledgeBase資料集結合相關文獻去訓練語言知識模型，讓語言模型和知識圖譜之間的資訊互相交換，並且在回答特定領域問答的任務上，表現優於現有的語言模型和知識圖譜增強模型。我們想透過作者提供的預訓練模型，來做多選題選項生成，透過知識圖譜所額外提供的資訊，讓在考慮生成選項時，除了題目和答案的文字外，用這些文字從知識圖譜萃取相關的知識圖譜子圖幫助生成選項有更多參考資訊。知識圖譜於問句生成之研究也見於[5][13]。於[5]研究中，作者之想法為使用知識圖譜進行問題生成任務時，通常只會取出資料與關係的三元組合，並不能完整利用知識圖譜中豐富的資訊，因此作者基於GraphNeuralNetworks提出了BidirectionalGatedGraphNeuralNetwork(BiGGNN)，將知識圖譜中的node及edge編碼成向量，使得每個node都包含了周圍其他node的資訊，並對答案的node進行標註，隨後透過RNN進行問句生成，生成過程中RNN會決定生成新文字或直接複製某個node的資訊放進問題。我們應可借鏡此研究論文的想法，可將知識圖譜及答案的資訊完整編碼，可基於此特性，在生成問題時同時考慮答案附近的node，讓模型在建立問題的同時也能根據附近的node產生干擾選項。而於[13]研究中，則提出的ISEEQ架構，主要目的是希望從簡短的敘述句中去尋找更多訊息來反問使用者是否符合關於敘述句背後未提及的情況，在使用時，使用者只需輸入簡短的敘述句，模型會先從知識圖譜擴展並增加query的關鍵字，透過這些關鍵字模型就能retrieve更多相關的文章進行ranking，在從這些文章中使用QG模型產生一些原始query中未提及但可能相關背景知識，例如當使用者說道:“我感到疲勞且渾身無力”，ISEEQ能回覆:“請問病徵持續了多久?”，或是“你最近14天有出國過嗎?”這類可能與其他知識相關的問句。這種方式最大的貢獻及是讓機器學習到”反問”的能力，而非一味的透過query生成出結果。這樣的想法，或許可引用來提升題幹生成的精確度，舉例來說:有位教師想出關於日治時期的歷史題目，我們就能先藉由ISEEQ來詢問是要出與日治時期戰爭相關的題目或是與日治時期建設相關的題目，以此來取得更精確的題幹生成需求。[1]J.Araki,D.Rajagopal,S.Sankaranarayanan,S.Holm,Y.Yamakawa,andT.Mitamura.Gen-eratingquestionsandmultiple-choiceanswersusingsemanticanalysisoftexts.InProceedingsofCOLING2016,the26thInternationalConferenceonComputationalLinguistics:TechnicalPapers,pages1125–1136,2016.[2]C.Bhagavatula,R.L.Bras,C.Malaviya,K.Sakaguchi,A.Holtzman,H.Rashkin,D.Downey,S.W.-t.Yih,andY.Choi.Abductivecommonsensereasoning.arXivpreprintarXiv:1908.05739,[3]S.K.Bitew,A.Hadifar,L.Sterckx,J.Deleu,C.Develder,andT.Demeester.Learningtoreusedistractorstosupportmultiplechoicequestiongenerationineducation.IEEETransactionsonLearningTechnologies,2022.[4]Y.-H.ChanandY.-C.Fan.Arecurrentbert-basedmodelforquestiongeneration.InProceedingsofthe2ndWorkshoponMachineReadingforQuestionAnswering,pages154–162,2019.[5]Y.Chen,L.Wu,andM.J.Zaki.Towardsubgraphguidedknowledgegraphquestiongenerationwithgraphneuralnetworks.arXivpreprintarXiv:2004.06015,2020.[6]S.-H.Chiang,S.-C.Wang,andY.-C.Fan.Cdgp:Automaticclozedistractorgenerationbasedonpre-trainedlanugagemodel.InFindingsoftheAssociationforComputationalLinguistics:[7]Y.-S.Chuang,R.Dangovski,H.Luo,Y.Zhang,S.Chang,M.Soljac￿ic￿,S.-W.Li,W.-t.Yih,Y.Kim,andJ.Glass.Diffcse:Difference-basedcontrastivelearningforsentenceembeddings.arXivpreprintarXiv:2204.10298,2022.[8]H.-L.Chung,Y.-H.Chan,andY.-C.Fan.Abert-baseddistractorgenerationschemewithmulti-taskingandnegativeanswertrainingstrategies.InFindingsoftheAssociationforComputationalLinguistics:EMNLP2020,pages4390–4400,2020.[9]B.Dhingra,K.Mazaitis,andW.W.Cohen.Quasar:Datasetsforquestionansweringbysearchandreading.arXivpreprintarXiv:1707.03904,2017.[10]D.Dua,Y.Wang,P.Dasigi,G.Stanovsky,S.Singh,andM.Gardner.ingcomprehensionbenchmarkrequiringdiscretereasoningoverparagraphs.arXiv:1903.00161,2019.[11]T.Gao,X.Yao,andD.Chen.Simcse:Simplecontrastivelearningofsentenceembeddings.arXivpreprintarXiv:2104.08821,2021.[12]Y.Gao,L.Bing,P.Li,I.King,andM.R.Lyu.Generatingdistractorsforreadingcomprehen-sionquestionsfromrealexaminations.InProceedingsoftheAAAIConferenceonArtificialIntelligence,volume33,pages6423–6430,2019.[13]M.Gaur,K.Gunaratna,V.Srinivasan,andH.Jin.Iseeq:Informationseekingquestiongenerationusingdynamicmeta-informationretrievalandknowledgegraphs.InProceedingsoftheAAAIConferenceonArtificialIntelligence,volume36,pages10672–10680,2022.[14]Q.Guo,C.Kulkarni,A.Kittur,J.P.Bigham,andE.Brunskill.Questimator:Generatingknowl-edgeassessmentsforarbitrarytopics.InIJCAI-16:ProceedingsoftheAAAITwenty-FifthInternationalJointConferenceonArtificialIntelligence,2016.[15]D.Jin,E.Pan,N.Oufattole,W.-H.Weng,H.Fang,andP.Szolovits.Whatdiseasedoesthispa-tienthave?alarge-scaleopendomainquestionansweringdatasetfrommedicalexams.AppliedSciences,11(14):6421,2021.[16]V.Karpukhin,B.Og￿uz,S.Min,P.Lewis,L.Wu,S.Edunov,D.Chen,andW.-t.Yih.Densepassageretrievalforopen-domainquestionanswering.arXivpreprintarXiv:2004.04906,2020.[17]D.Khashabi,S.Chaturvedi,M.Roth,S.Upadhyay,andD.Roth.Lookingbeyondthesurface:Achallengesetforreadingcomprehensionovermultiplesentences.InProceedingsofthe2018ConferenceoftheNorthAmericanChapteroftheAssociationforComputationalLinguistics:HumanLanguageTechnologies,Volume1(LongPapers),pages252–262,2018.[18]O.Khattab,C.Potts,andM.Zaharia.Relevance-guidedsupervisionforopenqawithcolbert.TransactionsoftheAssociationforComputationalLinguistics,9:929–944,2021.[19]O.KhattabandM.Zaharia.Colbert:Efficientandeffectivepassagesearchviacontextualizedlateinteractionoverbert.InProceedingsofthe43rdInternationalACMSIGIRconferenceonresearchanddevelopmentinInformationRetrieval,pages39–48,2020.[20]G.Kumar,R.E.Banchs,andL.F.D’Haro.Revup:Automaticgap-fillquestiongenerationfromeducationaltexts.InProceedingsoftheTenthWorkshoponInnovativeUseofNLPforBuildingEducationalApplications,pages154–161,2015.[21]G.Lai,Q.Xie,H.Liu,Y.Yang,andE.Hovy.Race:Large-scalereadingcomprehensiondatasetfromexaminations.arXivpreprintarXiv:1704.04683,2017.[22]G.LeBerre,C.Cerisara,P.Langlais,andG.Lapalme.Unsupervisedmultiple-choicequestiongenerationforout-of-domainq&afine-tuning.InProceedingsofthe60thAnnualMeetingoftheAssociationforComputationalLinguistics(Volume2:ShortPapers),pages732–738,2022.[23]K.Lee,M.-W.Chang,andK.Toutanova.Latentretrievalforweaklysupervisedopendomainquestionanswering.arXivpreprintarXiv:1906.00300,2019.[24]C.Liang,X.Yang,N.Dave,D.Wham,B.Pursel,andC.L.Giles.Distractorgenerationformultiplechoicequestionsusinglearningtorank.InProceedingsofthethirteenthworkshoponinnovativeuseofNLPforbuildingeducationalapplications,pages284–290,2018.[25]C.Liang,X.Yang,D.Wham,B.Pursel,R.Passonneaur,andC.L.Giles.Distractorgenera-tionwithgenerativeadversarialnetsforautomaticallycreatingfill-in-the-blankquestions.InProceedingsoftheKnowledgeCaptureConference,pages1–4,2017.[26]V.Liévin,C.E.Hother,andO.Winther.Canlargelanguagemodelsreasonaboutmedicalquestions?arXivpreprintarXiv:2207.08143,2022.[27]Y.Mao,P.He,X.Liu,Y.Shen,J.Gao,J.Han,andW.Chen.Generation-augmentedretrievalforopen-domainquestionanswering.arXivpreprintarXiv:2009.08553,2020.[28]Y.Nie,S.Wang,andM.Bansal.Revealingtheimportanceofsemanticretrievalformachinereadingatscale.arXivpreprintarXiv:1909.08041,2019.[29]H.-Y.Peng,H.-L.Chung,Y.-H.Chan,andY.-C.Fan.Misleadinginferencegenerationviaproxi-malpolicyoptimization.InPacific-AsiaConferenceonKnowledgeDiscoveryandDataMining,pages497–509.Springer,2022.[30]L.Qin,A.Bosselut,A.Holtzman,C.Bhagavatula,E.Clark,andY.Choi.Counterfactualstoryreasoningandgeneration.arXivpreprintarXiv:1909.04076,2019.[31]P.Rajpurkar,R.Jia,andP.Liang.Knowwhatyoudon’tknow:Unanswerablequestionsforsquad.arXivpreprintarXiv:1806.03822,2018.[32]P.Rajpurkar,J.Zhang,K.Lopyrev,andP.Liang.Squad:100,000+questionsformachinecomprehensionoftext.arXivpreprintarXiv:1606.05250,2016.[33]S.Ren,Y.Deng,K.He,andW.Che.Generatingnaturallanguageadversarialexamplesthroughprobabilityweightedwordsaliency.InProceedingsofthe57thannualmeetingoftheassociationforcomputationallinguistics,pages1085–1097,2019.[34]S.RenandK.Q.Zhu.Knowledge-drivendistractorgenerationforcloze-stylemultiplechoicequestions.InProceedingsoftheAAAIConferenceonArtificialIntelligence,volume35,pages[35]Y.Su,T.Lan,Y.Wang,D.Yogatama,L.Kong,andN.Collier.Acontrastiveframeworkforneuraltextgeneration.arXivpreprintarXiv:2202.06417,2022.[36]A.Talmor,J.Herzig,N.Lourie,andJ.Berant.Commonsenseqa:Aquestionansweringchallengetargetingcommonsenseknowledge.arXivpreprintarXiv:1811.00937,2018.[37]J.Welbl,N.F.Liu,andM.Gardner.Crowdsourcingmultiplechoicesciencequestions.arXivpreprintarXiv:1707.06209,2017.[38]J.Welbl,P.Stenetorp,andS.Riedel.Constructingdatasetsformulti-hopreadingcomprehensionacrossdocuments.TransactionsoftheAssociationforComputationalLinguistics,6:287–302,[39]T.Wu,M.T.Ribeiro,J.Heer,andD.S.Weld.Polyjuice:Automated,general-purposecounter-factualgeneration.arXivpreprintarXiv:2101.00288,1(2),2021.[40]J.Xie,N.Peng,Y.Cai,T.Wang,andQ.Huang.Diversedistractorgenerationforconstruct-inghigh-qualitymultiplechoicequestions.IEEE/ACMTransactionsonAudio,Speech,andLanguageProcessing,30:280–291,2021.[41]Z.Yang,P.Qi,S.Zhang,Y.Bengio,W.W.Cohen,R.Salakhutdinov,andC.D.Manning.Hotpotqa:Adatasetfordiverse,explainablemulti-hopquestionanswering.arXiv:1809.09600,2018.[42]M.Yasunaga,A.Bosselut,H.Ren,X.Zhang,C.D.Manning,P.Liang,andJ.Leskovec.Deepbidirectionallanguage-knowledgegraphpretraining.arXivpreprintarXiv:2210.09338,2022.[43]C.Y.Yeung,J.S.Lee,andB.K.Tsou.Difficulty-awaredistractorgenerationforgap-fillitems.InProceedingsoftheThe17thAnnualWorkshopoftheAustralasianLanguageTechnologyAssociation,pages159–164,2019.[44]C.Zhang,Y.Sun,H.Chen,andJ.Wang.Generatingadequatedistractorsformultiple-choicequestions.arXivpreprintarXiv:2010.12658,2020.[45]X.Zhou,S.Luo,andY.Wu.Co-attentionhierarchicalnetwork:Generatingcoherentlongdistractorsforreadingcomprehension.InProceedingsoftheAAAIConferenceonArtificialIntelligence,volume34,pages9725–9732,2020.[46]B.Zou,P.Li,L.Pan,andA.Aw.Automatictrue/falsequestiongenerationforeducationalpur-pose.InProceedingsofthe17thWorkshoponInnovativeUseofNLPforBuildingEducationalApplications(BEA2022),pages61–70,2022.於本研究計畫中，我們目標基於自然語言生成技術進行選擇題自動出題研究。我們預期的展示情境為：給定一知識範圍(如農業文集)，我們可於系統直接輸入一組主題關鍵詞，而後自動生成關於該主題相關之選擇題題組(含字詞等級、句子等級、挑選是非類型之選擇題)。然而，如同前言背景中所言，現有技術離目標仍有許多待強化之處，對此我們擬定三大出題目標：字詞等級干擾選項、句子等級干擾選項與反事實干擾選項進行研究。針對此三大目標，我們擬定(1)基於知識圖譜之強化與(2)基於生成語言模型與對比學習之強化三個核心技術進行探討。研究技術項目與項目間之關係與進行規劃如Figure7所示。於第一年度中，我們規劃「考量題幹可用性與干擾選項間關聯之Retrieve-and-Generate架構優化」、「基於語言模型與知識圖譜之Text2Text干擾選項生成」與「基於Masked-LM之反事實干擾選項生成研究」進行研究。「考量題幹可用性與干擾選項間關聯之Retrieve-and-Generate架構優化」著眼於優化目前https://mcq-demo.nlpnchu.org之WordLevel之選擇題生成系統。希冀自動過濾相似或語意不正確題幹，並且考量干擾選項彼此間之關聯進行最終干擾選項集合之綜合挑選。「基於生成語言模型之Text2Text干擾選項生成模型」則考量Text2Text模式利用生成模型直接生成干擾選項序列，藉此嘗試生成句子等級之干擾選項。並進一步地探索結合知識圖譜與語言模型之干擾選項生成，希冀透過知識圖譜之整合來生成更有誘導性且合理之字詞等級干擾選項。此外，於「基於Masked-LM之反事實干擾選項生成研究」項目中，我們預計著手挑戰反事實干擾選項生成，我們嘗試利用透過NER擷取給定事實陳述句中之關鍵字並Masked-LM該關鍵字來填入機率較低之字詞，以為反事實語句生成之作法。我們預期第一年度工作完成後，將可優化現有單詞等級干擾選項之選擇題自動生成系統，並於第二年度中，我們則規劃「基於知識圖譜強化之反事實干擾選項生成」、「基於對比式學習之干擾選項生成模型強化」與「利用Open-DomainQuestionAnswering錯誤結果之干擾選項挑選」。於此年度中，進一步嘗試利用知識圖譜萃取三元組知識後進行反事實語句之生成，基本想法為透過萃取三元組後再進一步輸入語言模型，並希望語言模型適當地進行反事實語句的改寫。此一部分預計使用第一年度之「基於Masked-LM之反事實干擾選項生成研究」之基礎以為訓練資料。對於字詞等級的干擾選項，我們則探索利用Open-DomainQuestionAnswering模型所錯誤回報之答案當作干擾選項。我們希望透過此不同之想法來取得更多樣的干擾選項。再者，如前言所述，生成模型生成序列時常出現生成結果重複的狀況，對此我們嘗試利用對比式來改善生成模型這方面的缺點，以優化干擾選項之生成。第二年度工作項目完成後，我們可初步提供反事實干擾選項之選擇題生成、優化後之句子等級干擾選項生成與基於ODQA方式取得之字詞等級干擾選項生成。於第三年度中，我們則規劃「基於NodeCentrality考量主題涵蓋性之選擇題題組生成」與「生成難度可控之干擾選項生成研究」。如同前言所述，一個理想的選擇題出題系統應該考慮題組生成，整體題目之主題涵蓋性，對此我們也將嘗試基於知識圖譜涵蓋的方式考慮節點centrality重要度方式來進行題組之生成。同時我們也將嘗試題目難易度之控制生成研究。我們的想法是當自動生成成為可能後未來的出題應該是個人化模式；針對個人的學習狀況進行測驗，因此我們規劃「個人化與生成難度可控之干擾選項生成研究」進行探討。第三年度，我們也預計投入人力進行系統的實作開發與整合。以下茲針對擬訂之研究議題其問題挑戰與1.基於語言模型與知識圖譜之Text2Text干擾選項生成模型近年來，語言模型除了使用CandicateGeneratingandRanking架構的MaskedLM之外，使用Encoder-Decoder架構的Text2Text模型於生成任務上也有很好的成果，因此我們的想法是把干擾選項生成Model視為一個Text2Text任務，嘗試讓模型直接生成干擾選項。且使用Text2Text模型生成Distractor有別於用MaskedLM生成Distractor，我們希望模型能生成的Distractor有以下特性(1)非單詞之Distractor:目前在Distractor生成上多為單詞生成，要生成一段敘述/句子的干擾選項還沒有很好的做法。(2)特定domain之MCQ:目前的MCQ多為英文測驗，若想針對特定領域出選擇題，使用MaskedLM生成的Distractor會因語料庫限制而為達到上述的兩種目標需求，我們設Q為輸入之問句，A為輸入問句之答案，並將Q與A字串接字串當作輸入，訓練模型輸出一連串問句Q的干擾選項字串D=(d1,d2,d3)。模型架構如圖2所示，我們將「哪一種技術最適用於嚴重特殊傳染性肺炎之病原快速篩檢?」與「聚合￿連鎖反應」接在一起輸入至Text2Text模型，而模型生成了「重組DNA技術」、「電腦斷層掃描」與「組織培養」三個干擾選項，三個選項皆為醫療檢測方式且都不是單詞，符合如前言所述，我們也預計探索使用知識圖譜來強化生成干擾選項的能力。如Figure8的圖例，我們可以使用語言模型結合醫療知識圖譜，利用給定的文本從知識圖譜中萃取有包含這些topicentities以及相鄰的nodeentities的子圖，再分別將文本與子圖輸入至語言模型和GraphNeuralNetwork(GNN)，讓模型學習找出關鍵的資訊與強化network中重要entitynodes跟edges的權重，藉由GNN的額外資訊與語言模型能力生成更好的選項。Figure8為我們規劃做法的概觀。主要分兩階段進行：(1)關鍵字子圖與GNNEmbedding萃取與(2)GNNEmbedding與LanugageEmbedding間之FusionEncoder生成。具體而言，第一個階段是將從知識圖譜裡萃取相關的知識圖譜子圖。我們一開始先使用NERModel抓取題幹中的entities，再透過這些entities透過知識圖譜去尋找entitynodes以及所有跟entitynodes相鄰3-hop內的bridgenodes，將檢索出來的結果當成知識圖譜子圖，並在後續第二階段提供以Figure9的圖例，我們先將問題透過NERModel斷詞得到”白血病”、”發病”、”器官”與”發病原因”當作entities，再透過這些entities當作關鍵字透過Retriever從知識圖譜萃取Figure8:結合知識圖譜與語言模型生成之構想架構圖與這些詞相關的node和edge，如利用”白血病”與”器官”能找到”肝臟”、”骨髓”、”肺部”等等entities，我們能利用這些entities來當作後續的生成選項。第二個階段則將文本與第一階段得到的知識圖譜子圖分別輸入至語言模型與GNN進行訓練，訓練目標為將Input的[MASK]Token還原來產生干擾選項，先將語言模型的Input額外新增[MASK]Token，經過N層LMLayer做self-attention後，透過FusionEncoder再結合知識圖譜的資訊與原本的文本去生成選項，透過以上的手法，來強化干擾選項之生成。2.考量題幹可用性與干擾選項間關聯之Retrieve-and-Generate架構優化如同研究目的所述，本研究項目著眼於優化現有Retrieve-and-Generate生成架構。我們•文章檢索器過濾:在Retrieving-and-Generating方法中Retriever會檢索回傳含有主題字詞的文章，但有部分文章並不適合當作問句生成模型的輸入，那些文章含有太多的雜訊，我們想嘗試在這步驟先過濾一些文章。我們的想法是利用詞向量中的相似詞再次檢索Retriever回傳的文章，把過於不相關的文章濾掉，減少問句生成模型輸出過於複•生成題幹品質自動過濾：問句生成模型輸出的問句，會再交由使用者挑選，但為了更好的效果，我們想嘗試利用語言模型做初步的篩選，減少人工挑選時間。我們想嘗試利用ODQA模型來過濾問句，將問句生成模型輸出的問句輸入至問答模型後，根據問答模型的回答結果與信心分數來決定該問句是否合適當作MCQ。如Figure10所例，我們將生成之問句再次投入ODQA模型，透過答案選項之信心度來為問句生成過濾之基Figure10:Unanswerablequestionsolution•考量干擾選項間關聯之干擾選項挑選:如前言所述，一個可以提升選擇題生成品質￿考量干擾選項彼此之間之難易度。而在Retrieving-and-Generating方法中，生成干擾選項的方式為取出詞向量模型中與主題字詞相似之詞，使用相似度前三名當作干擾選項，因此有選項過於相近而造成問題太難的可能。為做改善，我們取出所有相似詞的向量與主題字詞向量計算相對距離，設定各個選項與答案的相似度範圍(距離範圍)，再從中隨機挑選相似詞，讓干擾選項彼此之間難易度有所區分以提升試題之鑑別度。•基於語言模型之干擾選項候選生成:雖然詞向量模型生成distractor過程迅速，但如果有主題字詞沒有被詞向量模型看過或有主題字詞一字多意，就無法生成適當的干擾選項。這個問題我們想嘗試使用MaskedLM來解決，我們將Retriever回傳的文章中含有主題字詞的段落記錄下來，移除主題字詞後輸入MaskedLM，再從MaskedLM輸出的候選字中選出distractor，使用這個方式生成的干擾選項不會被上述的缺點受限，也更Figure11:OptimizedRetrievingandGeneratingFramework3.基於Masked-LM之反事實干擾選項生成研究本研究項目為基於Masked-LM技巧來生成反事實干擾選項。我們需要先查找與事實相符之敘述，再修改成反事實干擾選項。我們預計之作法為：首先讓使用者先輸入考題關鍵字，再使用retriever查找相關文章，其目的為從相關文章中選取與事實相符之敘述。接著，將retriever檢索到的文章回傳給使用者，由使用者選取與事實相符之敘述。並按照以下之步驟進行Masked-LM反事實干擾選項生成。為了抽換專有名詞，我們先使用命名實體與事實相符之敘述中的所有專有名詞，以下圖為例”腎小管藉壓力差方式再吸收葡萄糖回微血管”，透過命名實體模型會萃取出專透過命名實體模型，我們會萃取出多個與事實相符之敘述中的專有名詞，我們需要選擇一個專有名詞行抽換。我們認為該專有名詞在相關文章中的重要程度越高，越高機率是考題中的重點。為了確保抽換專有名詞的品質，我們使用BM25來查看各個專有Figure12:AutoEncoder模型與AutoRegressive模型做MaskLM3.使用Maskedlanguagemodel模型進行改寫Maskedlanguagemodel，簡稱為MaskLM，該任務的會將輸入中的部分內容遮擋起來，模型會根據其他未被遮擋的內容，預測出被遮擋的內容或與其擁有相似語義的內容，現有的MaskLM模型分為兩種，分別為AutoEncoder框架的MaskLM模型與AutoRe-gressive框架的MaskLM模型，經典模型如BERT與BART。針對MaskLM任務，兩種模型進行預測的方式也有所不同，AutoEncoder框架的模型會預測出字，而AutoRe-gressive框架的模型會預測出字、詞或是句子，詳細範例如Figure12。根據抽換專有名詞的需求，我們將使用Autoregressive模型做MaskLM任務來抽換重要的專有名詞。然而模型預測出的內容眾多，有可能是原先遮擋的內容、遮擋內容的同義詞或是與遮擋內容擁有相似語義的內容。理想的情況下，模型預測的內容最高機率為原先遮擋的內容，若我們選擇原先遮擋的內容或是遮擋內容的同義詞進行抽換，會造成與事實相符之敘述未改變，便無法生成反事實干擾選項。為了避免此狀況發生，我們使用NLI任務中的換句話說(MRPC)任務來檢測其是否為干擾選項。換句話說任務主要在判斷兩句話的語意是否相同，為了避免MaskLM模型選用同義詞進行抽換，即模型生成的干擾選項仍是正確的，我們將與事實相符之敘述與生成之干擾選項作為模型的輸入，若模型認為兩句話具有相同語義，我們會讓MaskLM模型預4.基於知識圖譜強化之反事實干擾選項生成於本研究項目中，我們嘗試利用知識圖譜提供語言模型額外資訊達成反事實干擾選項生成。我們的構想如Figure4所示，包含以下步驟。首先我們會讓使用者先輸入考題內文，再將內文使用斷詞或NER模型萃取entity，利用這些entity於知識圖譜中查找相關子圖，子圖必須包含內文中所有的entitynodes以及所有跟entitynodes相鄰節點，以此為知識生成之基礎。接著我們將檢索到的子圖內的nodes與edges萃取成三元組，每筆三元組的結構是以node,edge,node組成。如同圖中所示，每一筆三元組描述一基本事實陳述，如導體可以3.將與事實相符之敘述修改為反事實干擾選項為了將與事實相符之敘述更改為反事實干擾選項，我們將前一步驟所萃取的三元組與考題內文當作作語言模型的輸入來生成反事實干擾選項。我們預計透過生成語言模型與輸入給定的問句和三元組來生成與問句情境相符合的反事實干擾選項，以FigurejQuery36306984126397040076_1676210426716為例，生成語言模型在給定關於與導體相關的題目敘述以及從知識圖譜萃取與導體相關的三元組後，模型藉由這些三元組推斷其中的關係，比如”導體可以導電->石墨是導體->筆芯是由石墨製成”，從上述的三元組推斷出”筆芯可以導電”的事實，也可以改寫成”筆芯不能導電”的反事實干擾選項。5.使用Open-DomainQuestionAnswering錯誤結果之干擾選項挑選研究如前言所述，我們預計使用ODQA模型所回答之錯誤結果來當作干擾選項(如Figure5)。以我們先前曾開發https://agri.nlpnchu.org/之農業問答系統為例，我們發現以上想法所面臨的挑於ODQA的架構下，可能會遇到ODQA正確回答所下達之問題，其回傳結果皆為可行之答案，也因此無法被使用來當干擾選項，對此，我們探索兩種作法來進行調整。–1.使用BM25，且刪除尋找出來文章的正確答案在使用BM25的情境下，因為是使用關鍵字比對，所以會找出許多跟正確答案相關的文章，我們將找到文章的正確答案刪除，這樣這個文章會變成絕對回答不出正確答案的文章，可是因為Reader對文字有一定的理解能力，並且具有關鍵字的文章中也很可能會有，他會從剩下可能的答案中，尋找一個最相近的選項，這樣就可以較容易生出干擾選項。如Figure13所示，我們將答案”2,4-D”從文章中直接刪除後，再交給Reader進行閱讀，藉此確保正確答案不會被回傳。Figure13:基於ODQA架構之Retriever弱化方法–2.使用DensePassageRetrieverDensePassageRetriever[16]，DensePassageRetriever可以將資料庫的文章轉為向量的方式表示，在搜尋的時候也將Query轉為向量，並且比對兩個向量的相似度，這樣就可以找出相似的文章，可是我們想說，可以利用DPR找尋文章相似度的這一點，理論上相似的文章跟Query算出來的相似度會是最高的，我們可以將相似度高的文章拿出來，並且查看有沒有答案出現在上面，如果沒有答案，我們就將他丟給Reader，因為我們覺得如我相似度高，就代表Retriever認為裡面有答案，可是裡面並沒有含有我們的正解，所以這篇文章就含Retriever認為是答案，但是實際上不是答案的文字，所以這時候我們將這篇文章傳給Reader，就會有比較高在我們使用baseline的情境中，有時候也會出現不相關的答案，例如在??中，問模型關於殺草劑的問題，會出現的人力除草，人力除草跟題目所問的殺草劑名稱是不相關的，所以我們需要用一個方法把他過濾掉。在這種情況下，我們如果加入正確答案當作Query一起輸入Retriever，就會增加模型找到正確passage的機率，對我們想找跟正確答案不相關但是相似度較高的Passage會產生反效果，所以我們採用的是從Reader輸出之後把他過濾掉的方法。當Reader從Retriever拿到Passage之後，我們會將正確答案去除，之後再將剩餘的答案丟入一個LanguageModel作判斷，如果他跟答案的距離過於遙遠，就放入NegativeList，如果是相關的就放入PositiveList，之後要挑相關答案的時候可以從PositiveList裡面挑選。6.基於對比式學習之干擾選項生成模型強化ContrastiveLearning的設計上需要考量如何去設定PositiveSample以及NegativeSample。在我們的目標中，因為不希望模型會重複的生成出相同意思的句子，所以我們預計使用同義的句子當作NegativeSample，而資料集中的干擾選項可以被當作PositiveSample。然而，由於我們的資料集中並沒有NagativeSample的選項，也就是同義於模型生成的選項。對此，我們的想法為透過迭代生成來產生我們需要的NegativeSample。具體而言，如Figure14所舉例，在DistractorGenerator的第一次生成中，我們會輸入Question以及Answer使模型生成出干擾選項。而在第二次迭代中，我們除了會輸入QuestionAnswerPair外，還會再加上在上一次迭代中生成的干擾選項，以提示模型先前已生成的Distractor的資訊。對於第二次迭代的輸出來說，第一次迭代輸出的Distractor，即為它的NegativeSample，因為DistractorGenerator必須要生成出與上一次迭代生成的Distractor意義相異的句子，所以它們之間的SentenceEmbedding不能太過接近。Figure14:IterationGenerater示意，在第一次的迭代中生成的Distractor會作為第二次迭代的輸入，以提示模型不要生成出類似的DistractorFigure15:對比式學習於干擾選項生成之預想架構我們所規劃之整體框架，如Figure15所示。我們使用多次迭代的方式來生成Distractor，對於第一次迭代來說，我們會從其他in-batch的data中取得Sentence當作NegativeSample。而在第二次以後的迭代來說，除了其他in-batch的Sentence以外，我們還會將此次迭代前所生成的Distractor加入我們的NegativeSample中。透過NegativeSample和模型輸出的Distractor的dotproduct可以計算出ContrastiveLoss，此外我們也會加入Distractor和Label計算的CrossEntropyLoss，來計算出我們的TotalLoss15。ContrastiveLoss定義如下:x為生成的distractor，p為資料集中的Distractor(Label)，N為NegativeSampleSet，N={n1,n2,...nj}，其中包括先前迭代生成的Distractor以及其他in-batch的選項。7.基於NodeCentrality考量主題涵蓋性之選擇題題組生成如同前言所述，本研究議題之目標為考量主題涵蓋性之選擇題題組生成。因此，我們預計根據使用者所指定的主題，按Figure16架構規劃來進行題組生成。1.透過知識圖譜子圖萃取來涵蓋所指定主題之內容：藉由知識圖譜上NodeCentrality方式來萃取關鍵字，以為題幹之生成。我們的基本想法為在知識圖譜上我們定位出與使用者輸入之主題關鍵字相關之結點後，我們可以試者套入PageRank或者其他圖形上計算NodeCentrality重要節點挑選，並以重要節點為題幹生成之主軸。2.透過上一步取得之重要答案片段後，進行問題之生成。並生成較多的問題數量以為CandidateQuestionPool，而後將問題將題組生成問題設定一成為一選擇最佳化議題(Optimization)：根據主題生成大量的單一題目，而後定義UtilityScore來計算個別題目組合後之優劣，來挑選最佳化的題組以為輸出。於本研究項目中，我們預計探索以下兩種生成難度控制方式。•調整Masked-LM比例之反事實選項難度控制：現有的Masked-LM模型會將重要的專有名詞進行遮擋，模型會透過未遮擋的內容對該專有名詞進行預測。理想的情況下，當我們僅對該專有名詞進行遮擋時，模型可成功地預測出遮擋的專有名詞或語義相近的專有名詞，這不僅是因為MaskLM模型夠強大，也是因為未遮擋的內容提供模型足夠多的資訊讓模型預測，若未遮擋的內容提供的資訊不夠多，則模型預測出的內容有可能會與遮擋的內容語義相差很多。我們根據Masked-LM模型的此特性對題目的難易度進行控制，若我們將多個專有名詞進行遮擋，再讓模型預測時，模型會因為提供的資訊不足，而預測出語義較不相近的專有名詞，即較不可能的答案。以下表為例，我們可以觀察到當[Mask]越多時，我們要判斷該事實是否正確就越形容易。[Mask]借壓力差吸收[Mask]回微血管[Mask]借壓力差吸收[Mask]回[Mask]•調整ODQA檢索文章之難度控制：由於我們的基礎架構為基於文章檢索(DocumentRetrieving)來取得包含使用者所給定的關鍵字之文章，並使用該文章與關鍵字為問題生成模型之輸入，來生成選擇題問句。我們若挑選較為冷門之文章進行生成，則預期可以生成較為困難之題幹。而就干擾選項而言，我們可藉由調整輸入給閱讀理解模型之文章來產生難度較高的錯誤。具體而言，給定一有包含正確答案A之文章C，我們先將答案片段A於文章D中刪除，而後將其輸入至閱讀理解模型，藉此產生錯誤之閱讀理解結果，以這樣的錯誤結果當Distractor預期將有較高之誘導能力。相類似的想法也可以套用為挑選Retriever認為相關度比較不高的文章以為閱讀理解模型之輸入。此外，就生成題目之廣度與深度而言，我們可藉由控制取得之文章來調配題組之廣度與深度。例如，若我們以單篇或少數篇文章進行生成，我們所生成的主題題組針對測驗內容而言，應該可產生知識性較為深入之試題。而若針對較多篇文章，則應可產生範圍較廣本研究計畫將以三年為期，共八項研究議題進行，各研究產出如上文所示，並整合各技術成為一選擇題自動生成系統，預期的展示情境為：我們可於系統直接輸入一知識範圍，系統將利用我們所開發之技術先進行自動關鍵語萃取，而後進行各種類型的選擇題出題並考量題組關聯性。我們也將彙整各研究議題中所發展之技術為一個成為公開原始碼專案，以為後續相關研究之用。並將計畫執行過程所累積的資料，公開希望能吸引更多研究者之投入。對於學術研究、國家發展及其他應用方面預期之貢獻自然語言生成已是未來深度學習領域之熱門研究重點。透過本研究計畫之投入我們將可進行先驅之探索，建立學生團隊與自然語言處理與生成之能力，並持續過去之閱讀測驗自動出題研究成果，以知識類型選擇題自動出題為主要系統開發目標為例，希冀能為國內外探索不同的自然語言生成應用服務。此外若可順利完成也可以成為一成功範例利用人工智慧輔助教師進行測驗的客製化與個人化學習。對比於題庫類型的方式，透過我們技術我們將可動態產生各式各樣不同之考題為每個人量身定做難度適中之考題。就此一觀點出發，我們預期本計畫除學術研究價值外，同時也兼具商業價值，也應可成為EdTech(教育科技)領域之一環。再者，疫情改變了學習與教學的模式。同步與非同步之線上教學已成為教學模式常態之一。線上教學帶來了好處，但也帶來了顧慮。其中，學習的專注度應是諸多顧慮中主要令人擔憂的一環。隔著螢幕學生學習是否專注？目前的教學內容學生是否已跟上進度？等等問題是線上教學學習專注度所在意的問題。對此，我們認為適時與即時地檢測學生學習成效，來提醒學生與教師目前之學習成效，應能適度緩解線上學習專注度之顧慮。因此，我們也常見於MOOCs課程於一個授課單元結束後適時地給予測驗(多數以選擇題測驗)，來確認學生學習成效。然而，人工設計試題耗費心力，一成不變全班同學皆相同試題也可能透過即時訊息(instantmessage)之分享而取得答案，造成檢測之無效。對此，我們認為隨著深度學習技術之快速演進，結合自然語言(NaturalLanguageGeneration)生成技術來進行自動出題技術已成為可能；透過自然語言生成技術自動化出題，產生多樣化的試題，來進行個人化的檢測，將有除了可以增進研讀論文、撰寫程式、架設系統以及撰寫論文的基本能力之外，參與本計畫之研究人員，將可以學習到深度學習與自然語言生成技術之基本觀念、實際應用以及相關的核心技術，在這些基礎之下，透過積極的研究與討論，將可以提升參與計畫人員思考問題與解決困難的重要能力。我們預期可獲得具體技能如下:(1)設計與建立一個深度學習模型之能力，(2)整合不同先進技術之能力，(3)自然語言生成技術之瞭解，(4)資訊檢索相關知識與軟體套件之暸解，(5)佈建深度學習模型於WebScale之能力。"}, "1676210785.403142": {"單位": "", "標題": "(ㄧ)、研究計畫之背景與目的", "作者": "", "發布日": 0, "摘要": "", "全文": "自然語言生成(NaturalLanguageGeneration)將是未來深度學習領域之熱門研究議題。2022年年末ChatGPT語言模型所帶來的示範性與展示效果預期將吸引更多研究目光。而個人於中興大學之研究團隊近年於自然語言生成之耕耘已建立初步研究成果。在過去朝著自然語言生成方向前進，我們探索自動生成試題之研究方向前進。於前期研究計畫「基於深度學習語言模型之閱讀測驗自動化生成研究109-2221-E-005-058-MY3」中，我們著眼於基於深度學習模型進行閱讀測驗(ReadingComprehensionTesting)題型之自動出題。在計畫執行過程中，我們•Ying-HongChan,Yao-ChungFan:KeywordProvisionQuestionGenerationforFacilitatingEducationalReadingComprehensionPreparation.INLG(InternationalNaturalLanguageGen-erationConference)2022.•Ho-LamChung,Ying-HongChan,Yao-ChungFan:ABERT-basedDistractorGenerationSchemewithMulti-taskingandNegativeAnswerTrainingStrategies.EMNLP(EmpiricalMethodsinNaturalLanguageProcessing)Findings2020.•Shang-HsuanChiang,Ssu-ChengWang,Yao-ChungFan:CDGP:AutomaticClozeDistractorGenerationbasedonPre-trainedLanguageModel.EMNLP(EmpiricalMethodsinNaturalLanguageProcessing)Findings2022.•Hsien-YungPeng,Ho-LamChung,Ying-HongChan,Yao-ChungFan:MisleadingInferenceGenerationviaProximalPolicyOptimization.PAKDD(AdvancesinKnowledgeDiscoveryandDataMining-26thPacific-AsiaConference)2022此外我們也將前期計畫之自動出題研究成果進行實作，可於以下兩個網址進行實測。•問句生成QuestionGeneration：給定文章、指定片段，透過自然語言生成模型進行問句生成。目前系統可於https://classic.queratorai.com/進行實測。•克漏字干擾選項生成DistractorGeneration：此一部分針對給定之文章進行克漏字誘導選項生成。目前系統可於：https://cdgp-demo.nlpnchu.org/進行實測。基於過去之研究成果累積，本計劃延續自然語言生成技術於自動出題之研究成果，擬定知識類型選擇題(MultipleChoiceQuestion,MCQ)自動生成為目標。以四技二專統一測驗考試為例(如Table2所示)，知識類型選擇題將包含題幹與四個選項(其中包含正確答案與干擾選項)，主要針對某一知識範圍進行測驗。我們認為知識範圍選擇題之自動生成可分為兩部分：(1)問題題幹(Stem)生成與(2)干擾選項(Distractor)生成。題幹生成主要目標為選擇題問題本身之生成，而干擾選項則著眼於誘導選項之生成。知識類型選擇題為學習測驗最常見的測驗模式之一。對比於閱讀測驗生成，知識類型選擇題的自動生成，將可減輕教師準備個人化教學測驗之負擔，也為學生提供更有效率之學習檢測方式。以現有我們成果為基礎，一個基本之自動選擇題出題系統作法為(如圖1)所示：我們可利用資訊檢索技術(如BM25Retriever)，從給定之文集資料庫(目前使用2021中文維基百科所有中文文章)中檢索主題字詞相關文章後，結合前期計畫所開發之問句生成模型(QuestionGeneration)[4]生成題幹，而後再利用詞向量模型(例如word2vec模型)找出相近字詞以為干擾選項。我們將這樣的構想先行實作一系統(稱之為Retrieve-and-GeneratePipeline)，目前此系統可於https://mcq-demo.nlpnchu.org/使用。使用者可藉由輸入主題相關字詞，選擇題生成系統將自動生成輸入字詞相關之選擇題結果。舉例而言，我們以「骨髓、白血病、淋巴」為主題字詞輸入至系統。我們挑選較佳之成果顯示於Table1。自動生成選擇題AutomaticGeneratedMCQQ1:哪一種癌症種類的統稱中可以看出它會造成不正常白血球的大量增生?(a)白血病、(b)乳腺癌、(c)腎病、(d)帕金森氏症Q1:白血病的發病原因通常是發生在哪一個器官上?(a)肝臟、(b)淋巴、(c)甲狀腺、(d)骨髓Q:白血病是最常見的兒童癌症，其中80%的病例是慢性的哪一種性病變?(a)淋巴結、(b)微血管、(c)淋巴、(d)黏膜Table1:目前選擇題自動生成結果範例。目前系統可於https://mcq-demo.nlpnchu.org/。目前我們挑選較佳之生成結果顯示於此Table中。建議審查委員自行輸入關鍵字詞組，來觀察我們Figure1:Retrieve-and-Generate基本架構。本例中我們以”骨髓、白血病、淋巴”為關鍵詞進行文章檢索，而再使用”骨髓”為答案，分別生成問句與干擾選項。從展示之結果中，我們發現自動選擇題出題之可能性。但仔細觀察會發現現有系統離實際應用仍有空間須待努力。也因此觸發本期研究計畫之提出。具體而言，我們認為選擇題自•句子等級之干擾選項生成(Sentence-LevelDistractorGeneration)首先，關於干擾選項的部分，若仰賴詞向量模型來產生干擾選項，我們所能取得的干擾選項便僅限於單詞(Word-Level)等級。從統測試題中，我們發現許多題目之干擾選項屬於句子等級(Sentence-Level)。以Table2中Q3與Q4選擇題為例，我們可以看到選項皆為句子等級之描述，此為目前基於詞向量來產生干擾選項的做法所不能。•反事實干擾選項生成(Counter-FactualSentenceGeneration)我們觀察到選擇題題型中，有著下列何者為錯誤或正確類型之考題。其干擾選項是一個較長的描述句。以Table2為例，Q5與Q6為分別詢問下列選項何者正確/錯誤。此題型，為變像之是非題。除了句子等級生成外，核心技術牽涉的是反事實(Counter-Factual)句子之生成。如何生成此類型之句子以為干擾選項也將是研究重點所在。•選擇題生成品質提升(MCQQuestionQualityImprovement)此外，我們也看到retreive-and-generatepipeline的做法，所產生的題幹本身有時是一個過於空泛之問題(例如，下列何種疾病為兒童常見之癌症？)或者並非語意正確之語句(例如，中樞性的什麼症狀是缺乏抗利尿激素的?)。如何有效判斷生成不良之語句，減低人工進一步過濾/挑選題目之負擔也是需進行研究之議題。此外，基於詞向量之干擾選項作法也有待商榷;隨機從相近詞向量挑選distractor，是否能確保distractor之誘導品類型I:單詞等級的干擾選項Word-LevelDistractorsQ1：淋巴器官是人體防禦系統中重要的一環，可提供淋巴細胞發育或活化所需要的條件與狀態。下列何者是淋巴球進行防禦與活化的器官？(a)骨髓(b)胸腺(c)肝臟(d)脾臟Q2：健康人體感覺神經傳入大腦的訊息大部分經過轉接，(a)視丘(b)下視丘(c)中腦(d)橋腦類型II:句子等級的干擾選項Sentence-LevelDistractors(a)無就巢性(b)體型大(c)著肉多(d)耐粗食Q4：下列何者為「聯合國國際糧農組織」(FAO)成立最重要的目標及首要任務？(a)消除及戰勝飢餓(b)聯絡及交流農業資訊(c)自然環境的永續及保護(d)實現國際間的公平貿易類型III:句子等級且與事實相違背的干擾選項Counter-FactualDistractorsQ5：有關人類上皮細胞的細胞週期之敘述，下列何者正確？(c)後期，分離的染色分體往細胞中央移動(d)末期，細胞膜內凹且細胞質開始一分為二Q6：有關一健康成人泌尿系統功能的敘述，下列何者錯誤？(a)腎臟每日約產生濾液180公升及尿液1.5公升(b)腎小管藉壓力差方式再吸收葡萄糖回微血管(c)腎臟藉排尿量多寡來維持體內水分的恆定(d)若出現尿毒症狀時，可使用血液透析方式進行治療Table2:四技二專統一測驗選擇題類型概觀。Q1與Q2擷取自110學年度護理基礎生物考科，Q3與Q4110學年度農業群農業概論考科，Q5與Q6取自110學年度農業群基礎生物考科質？也許所挑選的distractor並不具誘導性，使得受測者很輕易地可挑選出正確答案，或者挑選到答案的同義詞，造成一題有多個正確答案。根據先前計畫執行訪談教師所知，於測驗時為考量受測全體鑑別度，教師出題時會考量干擾選項彼此之間之難易度；通常三個干擾選項中，會有一個離正確答案較遠之干擾選項(誘導性較低)，一個次之，一個最為困難最具有誘導性。藉此來有適當的鑑別度。目前我們直接使用詞向量之作法並未考量此設計。•主題題組生成(MCQQuestionGroupGeneration)目前我們的系統僅為單一關鍵字詞之出題。我們認為一個良好的自動出題應該是以題組為單位進行出題；使用者指定一主題知識，系統根據此一主題知識範圍進行5-10題之選擇題自動生成，題目之間也應該彼此考量。目前Retreive-and-Generatepipeline的做法，我們使用使用者給定的關鍵字為答案，進行題目之生成。然而比較好的做法應為將使用者給定的關鍵字集合為知識範圍之指定，系統根據該範圍自動挑選重要概念關鍵字進行答案挑選，而後再進行選擇題題組之生成。針對上述之研究挑戰，於本研究規劃中我們擬定下列八項研究議題進行探討。•研究項目ㄧ：基於語言模型與知識圖譜之Text2Text干擾選項生成模型(DistractorGenerationbasedonText2TextLanguageModelandKnowledgeGraph)Figure2:Text2Text干擾選項生成架構Figure3:基於Masked-LM之反事實干擾選項現有之Distractor生成研究多是基於CandicateGenerating-and-Ranking架構，先透過知識庫(如Wordnet)或者透過MaskedLM的方式取得候選字後，再適當進行候選詞特徵排序以為最終干擾選項。對此我們有不同之想法，我們預計嘗試把干擾選項生成設定為一個Text2Text任務；如圖2所例，預期的輸入為題幹Q與答案A之串接字串，透過ModelEncoder與Decoder轉化輸出為一連串之干擾選項字串“重組DNA技術[SEP]電腦斷層掃描[SEP]組織培養”。我們透過語言生成模型直接將輸入對比轉換為輸出。此外，透過此一Text2Text方式，生成句子等級的干擾選項也將為可能。再者，近年來語言模型研究趨勢除了將模型參數量與訓練資料持續增加外，引入知識圖譜來強化生成模型於常識理解與推理能力也成為研究社群的關注所在[5][13]。也因此，探索整合知識圖譜於選擇題自動生成也因此成為我們規劃之研究項目。希冀藉由整合知識圖譜來提升干擾選項之誘導性與題幹生成之正確性。•研究項目二：基於Masked-LM之反事實干擾選項生成研究(Counter-FactualSentenceGenerationbasedonMasked-LM)如同前言所述，“下列何者正確/錯誤”題目為常見之類型選擇題題型。對此，我們預計嘗試以下之作法。首先，我們假定可取得正確敘述之語句S。我們先針對語句S中的字詞進行命名實體辨識(NamedEntityRecognition,NER)，其後再利用語言模型針對所取得之命名實體進行WholeWordMasking(WWM)，填入機率較低之填空結果，藉此生成反事實描述之語句。如Figure4所例，我們將所給定之事實描述，藉由NER方式挑選”葡萄糖”一次進行Masked，再藉由語言模型推測該Masked字詞，並挑選機率相對較低之預測填入。藉此創造出反事實之陳述句。然而，如何確保不挑選到同義詞？要進行多少個字詞之Masked？如何自動從主題文章中挑選事實陳述句以進行改寫？等等•研究項目三：基於知識圖譜強化之反事實干擾選項生成(Counter-FactualSentenceGen-erationbasedonKnowledge-GraphAugmentedLanguageModel)針對反事實干擾選項生成之議題，除了研究項目二之Masked-LM作法外，我們另一個構想為利用知識圖譜來強化Counter-Factual語句之生成。如Figure4所示，我們基本想法為先萃取與題幹相關聯之知識圖譜子圖(LocalSubgraph)並取得知識三元組，以此輸入至語言模型中，訓練語言模型反向改寫三元組語句，以此為反事實干擾選項。然而，此模式之訓練資料該如何取得？如何從知識圖譜上萃取子圖，並生成合適三元組？如何引入GraphNeuralNetwork來萃取結構？為此研究項目需進行探討之工作內容。•研究項目四：基於對比式學習之干擾選項生成模型強化(DistractorGenerationModelbasedonContrastiveLearning)我們過去的研究經驗[8]與相關研究[35]皆指出當使用Generative語言模型來生成句子Figure4:基於知識圖譜強化之反事實干擾選項生成構想層級之輸出時，模型經常會產生語意之相同，但字面的結果。如Table3所示，選項A跟C雖然字面敘述上不同，但實際上都是描述同樣的意思。生成重複選項將導致選擇副交感神經興奮會導致使心跳加速，血壓上升Table3:模型生成的誤導選項重複出現的範例對此，我們預計探索對比式學習(ContrastiveLearning)[7][35]來提升生成品質。對比式學習於自然語言之應用中主要想法為透過選取PositiveSample以及NegativSample使得模型去學習到生成句子跟同義句子的相同以及跟異類句子的不同之處，這樣我們可以使用重複選項當作我們的NegativeSample，然後資料集中的干擾選項當作PostiveSample，期望模型可以學習避免生成出重複的誤導選項。因此，我們預計探索對比式•研究項目五：使用Open-DomainQuestionAnswering錯誤結果之干擾選項挑選研究(Us-ingIncorrectAnswersfromODQAasDistractors)而除了利用生成模型產生Distractors外，我們發現利用Open-DomainQuestionAnswer-ing(ODQA)模型所產生的錯誤結果是一個值得探討的干擾選項的取得方式。具體而言，現有的ODQA模型主要包含一個Retriever與一個Reader，Retriever負責檢索相關的文章，而Reader利用檢索出來的文章利用閱讀理解模型(ReadingComprehensionModel)來找尋答案片段。由於ODQA並非完美，因此也會有錯誤結果之回報。但我們發現錯誤結果與正確答案並非天差地遠，閱讀理解模型所回報之結果通常也與答案接以圖5為例，當我們下達Q：「下列何者為目前世界上應用最廣的殺草劑，其具備有生長素之特性，常用於闊葉雜草防治，亦為選擇性殺草劑的一種？」，ODQA系統將取得圖5中右半部之答案：「萌前除草劑、2,4-D、非選擇性除草劑、10殺丹粒劑」。其中，2,4-D為正確答案，餘者為錯誤答案。然而我們覺得這些錯誤答案，可以當作Q之干擾選項來使用。有別於基於生成模型研究項目，利用ODQA模型之錯誤答案來當作干擾選項應可提供不同思路進行干擾選項生成。這樣的想法乍看可行，但仍有若干技術點需進行探討。諸如，如何控制ODQA模型之錯誤？我們需要在Reader部分弱化Reader的能力，或者弱化Retriever能力，來製造ODQA的錯誤？如何從錯誤回報結果挑選干擾選項？這些皆是尚待深入研究之議題。也因此觸發本研究項目之規劃。Figure5:利用ODQA模型之錯誤答案來當作Distractors•研究項目六：基於NodeCentrality考量主題涵蓋性之選擇題題組生成(MCQGroupGenerationconsideringTopicCoveragebasedGraphNodeCentrality)上述所規劃的選擇題生成做法，主要針對單題進行生成。若要產生多題/題組考題，就是重複相同流程，獨立產生個別考題後，整合為一題組。然而，題組生成做法應當考慮題與題之間之關聯性與互補性。因此，我們認為比較好的做法應當為根據使用者所指定的主題，透過(1)知識圖譜子圖萃取來涵蓋所指定主題之內容與(2)透過主題重要概念萃取：藉由知識圖譜上NodeCentrality方式來萃取關鍵字，以為題幹之生成。此外，如何根據主題互補性，來挑選個別生成之題組，用以組成選擇題題組也是需進行•研究項目七：考量題幹可用性與干擾選項間關聯之Retrieve-and-Generate架構優化(Op-timizingRetrieve-and-GenerateFrameworkconsideringStemUtilityandDistractorCorrela-tion)本研究項目著眼於優化現有Retrieve-and-Generate架構設定。Retrieve-and-Generate架構雖然簡單，但我們可以發現其於單字等級的干擾選項生成已有一定的可用性。我們預計可優化之處為：(1)我們發現引用現有系統於題幹生成時，會有題幹生成結果不佳的狀況。對此，我們發現其中的一個原因為Retriever所檢索之文章品質不佳(文章過短或者與主題不相關)，導致不良之題幹生成。因此如何改善檢索機制來挑選較佳的文章用以進行生成，便是值得探討的課題。再者，題幹生成不佳也有部分原因是來自於問句生成模型本身之生成錯誤，如何有效過濾生成不佳的題幹也因此是一個可需進行研究之項目。(2)如同前言所述，干擾選項之挑選應該考量彼此間之關聯，而非獨立挑選。如何納入此觀點來進行干擾選項之生成也因此成為預計關注之議題。又再者，我們基於word2vec方式之字詞等級干擾選項挑選是否可以引入語言模型之詞向量來求取•研究項目八：生成難度可控之選擇題生成研究Difficulty-ControlledMCQGeneration發展自動化生成考題之可能願景為：針對個人化學習自對應生成難度適中之測驗，以增加學習與檢測之效率。因此，如何針對現有技術探討生成結果難度控制也因此成為一可探索之研究議題。由於我們的基礎架構為基於文章檢索來取得包含使用者所給定的關鍵字之文章，並使用該文章與關鍵字為問題生成模型之輸入，來生成選擇題問句。我們若挑選較為冷門之文章進行生成，則預期可以生成較為困難之題幹。而就干擾選項而言，若我們基於研究項目五「使用Open-DomainQuestionAnswering錯誤結果之干擾選項挑選研究」，利用ODQA來取得干擾選項之作法。我們可藉由調整輸入給閱讀理解模型之文章來產生難度較高的錯誤。具體而言，給定一有包含正確答案A之文章C，我們先將答案片段A於文章D中刪除，而後將其輸入至閱讀理解模型，藉此產生錯誤之閱讀理解結果，以這樣的錯誤結果當干擾選項預期將有較高之誘導能力。相類似的想法也可以套用為挑選Retriever認為相關度比較不高的文章以為閱讀理解模型之輸入。此外，就生成題目之廣度與深度而言，我們可藉由控制取得之文章來調配題組之廣度與深度。例如，若我們以單篇或少數篇文章進行生成，我們所生成的主題題組針對測驗內容而言，應該可產生知識性較為深入之試題。而若針對較多篇文章，則應可產生範圍較廣之測驗試題。上述之研究議題，乍看可行但仍有許多細節需進行探索，如上述研究背景目的所述，本研究計畫擬探索基於現有Retrieve-and-GeneratePipeline之選擇題自動生成架構進行強化。我們以三年為期，規劃在此目標下擬定八項關鍵技術進行研究探討。如Figure6所示，八個工作項目又可區分為三項主軸：「選擇題生成品質提升」、「反事實選項生成」與「群組生成與難度控制」。其中，於「選擇題生成品質提升」研究主軸中，我們探索利用語言模型與知識圖譜架構來生成，將選擇題選項生成問題設定成為一Text2Text任務，輸入為問題題幹與正確答案所串接之TextSequence，而輸出為所生成之distractor串接之TextSequence。此外，我們也嘗試引入對比式學習來改善Text2Text生成模式之品質。我們也預期規劃探索利用ODQA方式來取得干擾選項。而於「反事實選項生成」研究主軸中，我們探索反事實陳述句之生成。我們預計嘗試基於MaskedLM取代重要名詞之反事實語句生成作法與基於知識圖譜知識三元組萃取改寫之作法。於第三個研究主軸中，我們則嘗試針對題組生成與難度可控制之干擾選項生成進行研究。MultipleChoice,Multi-此資料集特色在於為Multi-hop形式，答案需從複數文檔中推理而成；資料來自維基百科以及PubMed，PubMed為生物醫學論文的搜尋引擎，在兩個來源各自提供51,318(Wiki)和2,508(PubMed)筆問題。資料來自國高中英文考試，提供超過10萬筆問題以及2.8萬篇文本，題型上為英文閱讀測驗，會需要閱讀故事或對話紀錄，而針對該文本會有複數題目，每個題目此資料集的挑戰在於，內容來自於7個不同領域(新聞、小學自然科學、旅遊、小說故事等等)，為文本與題目提供多樣性，總計約1萬筆問題與800篇文本。MultipleChoice,Open-為Open-domain形式，並不提供文本，只有問題與答案；包含12,247筆問題、一個正確答案和四個干擾選項；題目類型為知識與常識題，來自於ConceptNet與同時包含兩種形式資料，填空題與Open-domain；填空題形式命名為QUASAR-S，其中資料來源自StackOverflow，提供約37K筆問題。Open-domain形式命名為QUASAR-T，提供約43K筆問題。來自StanfordUniversity的閱讀理解資料集，包含了超過100K筆問題和536篇對應的維基百科文本；延續了SQuAD1.1的題目外，[31]另外加入約50K筆無答案的題目，進一步測試機器能否在無對應答案時回答否，而MedQA是基於美國醫師執照考試(USMLE)所產生的資料集，其中問題皆為較為困難的專業醫學相關的知識，且包含了英文、簡體中文、繁體中文分別有約12K、34K、14K個問題集，這種專業的知識能使機器針對單一領域產生更具參考價值的distractor，目前此任務排名上最佳為ValentinLiévin團隊做出的Codex5-shot訓練機器學習模型進行閱讀理解(MachineReadingComprehensionTask)為最常見的深度學習自然語言任務。諸如，SQuAD[32]、RACE[21]、DROP[10]與HotPotQA[41]等資料集，皆是測驗/訓練機器學習模型於答案找尋、邏輯推理與摘要之能力。其中，許多機器閱讀理解任務採用多選題方式進行測驗。選擇題的自動答題與選擇題的自動生成為一體兩面之應用。因此具有選擇題樣態的資料集為我們選擇題自動生成研究之根本資料集，我們將目前所探知之相選擇題與干擾選項生成之相關研究可按應用類型可區隔為英文學習測驗與知識學習測驗。干擾選項生成研究於英文學習測驗應用，主要以自動生成閱讀理解與克漏字類型的考題為目標。閱讀理解之干擾選項生成與本研究計畫之知識類型選擇題之自動出題目標不同。因此本研究回顧中，我們不做此分類每一研究細部說明。我們將此類型之相關研究按不同象限做分類：(1)干擾選項層級之差異(生成字詞等級與句子等級之差異)象限，(2)題目類型(閱讀理解與克漏字測驗)，(3)生成方式(CandidateRanking與Generative)。我們按此三象限，將目前之相關研究摘要整理於Table5中。而知識學習測驗類型的選擇題生成，則於這一兩年逐漸吸引相關研究投入。我們認為與本研究計畫較為相關之研究有[22][40][3]，以下我們針對個別文獻討論如下。首先，與我們有相同目標者為[22]，也是針對特定知識主題生成選擇題。其主要作法Table5:現有閱讀測驗與克漏字干擾選項生成研究整理表為：先由人工給定一個主題，程式使用該主題名稱的維基百科頁面作為起始點(例如給定“體育”後搜尋體育相關維基百科)，保留熱門文章並取出文章的第一段擷取摘要，由此產生大量句子。並將這些句子依單純英文文法規則生成簡單5W問句與答案，例如who問句是將第一個動詞前的名詞作為正確答案並從句子中移除，what問句是將第一個夾在動詞間的名詞移除作為正確答案並從句子中移除，移除答案後的句子再帶入問題模板生成制式問句。生出五種問句後再依英文文法的詞與詞的相關性去決定哪個問句最適合。在干擾選項生成的部分，先隨機選取其他句子所產生的答案，再藉由語言模型去排序與問題的相關性，取前三高之預測為干擾選項。但本論文並未使用問句生成模型，採取之作法也與本計畫所規劃有所不同。於論文中[40]，則探討如何讓干擾選項更加多樣且與正確答案相差不遠(以具誘導性)。對此，創造了一款監督式分類任務來找出文章中屬於關鍵句子(及對產生干擾選項生成有用的句子)，作者在訓練之前會將訓練用的干擾選項和文章中的每個句子做比對，若該句子和干擾選項相似度達到一定程度，則該句子將成為文章中的關鍵句，透過這個方式在文章中能被修改成為新的干擾選項的句子將會達到多樣性的效果，且透過相似度對比，干擾選項也不會跟原始文章相差太遠從而提升干擾選項的品質。本研究之作法可供我們借鏡參考對關鍵文章/句選擇設計，用以提升干擾選項之多樣性。而與本計畫研究項目五「使用Open-DomainQuestionAnswer錯誤結果之干擾選項」走非生成想法相似之研究為[3]。其主要想法為作者發現於干擾選項資料集中，有許多干擾選項一再被使用。促使論文作者想為新的問題檢索以重複使用干擾選項。作者提出一個Transformer-baseddistractorretrievalmodels，該模型通過考慮問題的上下文來選擇干擾選項模型包含三個部分，分別為D-SIM模型、Q-SIM模型以及DQ-SIM模型。在D-SIM模型中使用mBERT計算同一個MCQ中問題和答案以及對應的干擾選項的向量，並計算他們之間的相似度。而在Q-SIM模型中作者假設不同問題共享一個或多個干擾選項或答案，代表問題可能在語意上相關，因此他們相關的干擾選項可以用作彼此的良好候選干擾選項，因此作者將訓練資料重新排序，使這些共享至少一個干擾選項或答案的問題聚集在一起，接著將資料送進Q-SIM模型計算不同的MCQ中問題之間的相似度。在DQ-SIM模型線性結合前兩個模型，從各自的優勢中獲益，透過基於分數和基於排名的合併策略計算來自D-SIM模型和Q-SIM模型的分數，並設計演算法使得高排名之干擾選項有較大的權重。重複使用現有資料集中之干擾選項為一創新想法，但仍與我們之ODQA做法有所不同。[OpenDomainQuestionAnswering相關研究]本計畫的選擇題生成架構主要基於兩元件:(1)文章萃取與(2)問句生成。目前我們所使用的文章萃取機制為傳統之BM25檢索。而近年來由於ODQA研究領域之發展，基於深度學習之DenseRetrieval技巧也紛紛於ODQA研究領域中所提出。我們預期DenseRetrieval技術也將於我們知識類型選擇題自動出題中擔任關鍵角色。茲簡要回顧現有ODQA之研究如下。ODQA架構中Retriver之重要性不言可喻。不同層級的語意檢索也影響著ODQA的效能。於[28]研究中，作者提出一個簡單而有效的pipeline系統，研究段落和句子級別語義檢索及其對下游任務的潛在影響，作法上是先檢索段落級別的段落，然後從上一步檢索後取得的段落中再去檢索句子級別的段落。該系統在事實驗證和Multi-hopQA方面進行評估，實驗結果顯示QA任務與事實驗證皆仰來上游檢索之結果。於[23]論文中，作者在ODQA架構的retreiver端提出了一個有效率的弱監督式學習的方法(InverseClozeTask)來對retreiver進行預訓練，給予模型完全無人工標記的維基百科原始資料，藉由論文中的方法將資料切分成固定長度的段落，並隨機選取其中的一個句子當作標籤，讓模型去學習如何從給定的句子去資料庫中找尋與之最相關的文章。在實驗數據中此方法訓練出的ODQA模型分數高於傳統的BM25檢索方法。而於DPR[16]論文中，作者為解決傳統BM25無法檢索語意相似段落的問題，提出了基於深度學習模型之DensePassageRetrieval，並提出使用in-batchnegative的技術，在密集向量的檢索取得極佳的效果改進。我們也預計引入中文DRP模型來提升目前題幹生成所需之而於GAR[27]研究中，作者們探討傳統基於BM25的檢索無法檢索沒有單詞重疊或語意相似的段落的問題，而基於密集向量的檢索方式在計算上又過於複雜，且會有資訊損失的問題。該論文透過預訓練語言模型自動生成與查詢語句相關的內容，可增加查詢語句的資訊量，隨後透過BM25進行檢索，該方法比起傳統BM25能檢索到更精確的段落，且執行效率也快於基於密集向量的檢索方式。透過GAR中的PLM可對查詢語句的資訊進行補充，原論文中是補充答案、相關段落及段落標題，可基於該手法讓PLM生成問題與原始答案的關聯，對這些關聯進行組合或刪除即可檢索與原答案有相似關係的查詢語句，可檢索到適合製作干於[19]，作者觀察到基於語言模型之檢索技術雖然效果很好，但比以往的方法需要極高的計算成本，特別是在計算每個query與document間的relevancescore時。本篇論文作者提出了ColBERT，引入lateinteraction架構，對query與document進行獨立的encoding，於模型輸出最終階段才進行相似度運算。而[18]中，則使用近一步基於ColBERT當基礎模型來進行retreiver端的訓練，使用了Relevance-guidedSupervision(RGS)的方法來訓練retreiver的模型。相較於傳統的retreiver訓練資料，傳統方法需人工標註的訓練資料，但作者認為人工標住資料往往存在偏見，進而影響模型的效能，所以在論文中提出了用訓練好的模型來找尋最適合的訓練資料，並把這些資料用於訓練下一個模型，使用這個方式來進行後續的訓練，最後得出最佳的模型。此方法訓練出的模型分數明顯高於目前opendomain的其他方式。在找尋訓練資料時，模型會先搜尋前k筆資料，並把前20筆資料中含有shortanswer的資料當成possitiveexample，而前20筆以外剩餘的資料中不含有shortanswer的資料當成negativeexample，此方法尋找的負面訓練資料交由reader進行預測能得到與正確答案相關的錯誤答案，如能改良ranking的方式，應用於干擾選項生成，預期將能提升干擾選項之品質。[ContrastiveLearning於自然語言生成相關研究]對比式學習始見於電腦視覺領域，其於影像應用上有著顯著的效果提升。因此，許多自然語言處理之研究者也將其引用至自然語言生成領域，諸如[11][7][35]。於[11]研究中，作者們提出了使用ContrastiveLearning的作法來學習SentenceEmbedding，他們使用BERT作為基礎模型，透過不同的dropoutmask使模型能用同樣的輸入產生不同的SentenceEmbedding，但它們之間的距離會是近的，也就可作為PositiveSample。然後再使用其他in-batch的句子作為NegativeSample，以達到非監督式訓練。而在有訓練資料的情況下，如NLI資料集，則可以將Eentailment視為PositiveSample，Contradiction以及其他in-batch的句子視為Negative而再基於[11]研究後，於[7]研究中，作者們提出加上了Discriminator來加強SentenceEmbedding的學習效果。具體而言，對每筆輸入來說，會透過SentenceEncoder產生一個SentenceEmbedding，同時也會隨機的Masking輸入句子其中的幾個Token，並透過一個fixed的Generator來還原MaskToken，而Discriminator要試著透過沒被修改過的SentenceEmbedding的資訊去辨認修改後的句子中的哪一個Token有被修改過，整個框架會透過SentenceEmbedding的ContrastiveLearningLoss以及Discriminator的ReplacedTokenDetection而與我們預計引入對比式學習來提升句子等級的干擾選項生成效果想法接近的研究也可見於[35]。該研究指出發現了以往autoregressive模型使用BeamSearch去決定要生成的文本時，會產生語意不自然以及沒意義的重複文字，儘管後續有人使用取樣以及修改ObjectiveFunction來嘗試解決這個問題，但這個方法時常會產生語意不連貫的問題。因此作者提出了使用ContrastiveLearningLoss加上原先的MaximumLikelihoodEstimationLoss來訓練模型，並且使用ContrastiveSearch來作為decodingmethod。[Counter-FactualSentenceGeneration相關研究]於Counter-FactualGeneration研究中，[44,33]為相關研究之先驅。主要想法是使用神經網路來生成反事實敘述，這個概念背後的想法是同時使用語言模型和各種採樣的策略來達成生成反事實敘述的效果。接續之前的成果，[39]提出了Polyjuice架構，可以控制輸出的反事實敘述的類型。另外一種生成干擾選項的研究是使用干擾選項生成(DistractorGeneration)的手法，干擾選項生成(DistractorGeneration)的目標是給予了一篇文章、問題和正確答案後，模型能夠生成似是而非的干擾選項。於研究[24]中，作者最先提出使用了神經網路來生成干擾選項，而研究[8]則延續了前者的成果，把重點放在生成複數的干擾選項並提出了一個負樣本學習策略(negativetrainingstrategy)來加強干擾選項生成。但這上述兩個研究主要都著眼於閱讀理解題型中干擾選項之生成，與本研究知識類型選擇題生成目標所有不同。而在關於AbductiveInferenceGeneration的相關研究中，於[30]中作者提出了故事改寫的任務，此任務目標是要改寫一段故事的敘述並且導致產生與原先不同的結局，與顯永的論文的方式的差異為故事改寫任務是根據給定的誤導推論來生成不同的結局，在MIG任務中是給定敘述句與其前後文來生成誤導推論。[2]研究中是讓模型根據上下文來推論中間適合加入的句子，此研究主要是產生正確的推論敘述，而在MIG任務[29]主要是產生相反的誤導推論敘述。於我們先前的研究[29]中，我們提出一個自然語言的誤導推論生成任務，任務步驟是當給定了一個陳述事實的句子並給定其上下文，將其定義為(x,y,z)與誤導推論ˆy，模型會生成與該事實相反的推論句子，由此可以讓模型產生出多選題似是而非的干擾選項。然而這樣任務與我們所設定之反事實出題仍有所不同。而與我們生成Counter-Factual陳述句想法接近的研究為：[46]。於該研究中，作者探討有別於過往的是非題，將文章中原始句子作為True的問句並通過反義詞替換關鍵字生成False問句，本篇作者通過使用一種新穎的masking-and-infilling策略來生成更靈活與複雜的問句。要被mask的關鍵字找法有很多種，像是SRL結果中的predicate，句子中連接詞(”that”、”when”、”since”)之後的部分或是數字等等，來找出值得提問的句子。Infilling的作法則是以BART為基底作為生成模型，將原始句子的關鍵字mask後，提供前後句子作為BART的上下文後產生句子填入。作者為了找值得提問的句子所提出的方法亦可用來作為多選題產生題目的答案選擇方法，而infilling的想法如果使用得宜，在提供出題範圍的前後文後，也能將欲出題句子挖空讓BART模型填補後產生選項。雖然與我們目標相似，但仍與我整合知識圖譜與語言模型為近年語言模型訓練熱門關注議題。於[42]研究中，作者們提出一CounterfactualGenerationControllableCounterfactualGenerationDistractorGenerationMisleadingInferenceGeneration種能將文本和知識圖譜的資訊進行深度雙向、自我監督的預訓練的方法，透過作者的方法可以將一些特定領域的KnowledgeBase資料集結合相關文獻去訓練語言知識模型，讓語言模型和知識圖譜之間的資訊互相交換，並且在回答特定領域問答的任務上，表現優於現有的語言模型和知識圖譜增強模型。我們想透過作者提供的預訓練模型，來做多選題選項生成，透過知識圖譜所額外提供的資訊，讓在考慮生成選項時，除了題目和答案的文字外，用這些文字從知識圖譜萃取相關的知識圖譜子圖幫助生成選項有更多參考資訊。知識圖譜於問句生成之研究也見於[5][13]。於[5]研究中，作者之想法為使用知識圖譜進行問題生成任務時，通常只會取出資料與關係的三元組合，並不能完整利用知識圖譜中豐富的資訊，因此作者基於GraphNeuralNetworks提出了BidirectionalGatedGraphNeuralNetwork(BiGGNN)，將知識圖譜中的node及edge編碼成向量，使得每個node都包含了周圍其他node的資訊，並對答案的node進行標註，隨後透過RNN進行問句生成，生成過程中RNN會決定生成新文字或直接複製某個node的資訊放進問題。我們應可借鏡此研究論文的想法，可將知識圖譜及答案的資訊完整編碼，可基於此特性，在生成問題時同時考慮答案附近的node，讓模型在建立問題的同時也能根據附近的node產生干擾選項。而於[13]研究中，則提出的ISEEQ架構，主要目的是希望從簡短的敘述句中去尋找更多訊息來反問使用者是否符合關於敘述句背後未提及的情況，在使用時，使用者只需輸入簡短的敘述句，模型會先從知識圖譜擴展並增加query的關鍵字，透過這些關鍵字模型就能retrieve更多相關的文章進行ranking，在從這些文章中使用QG模型產生一些原始query中未提及但可能相關背景知識，例如當使用者說道:“我感到疲勞且渾身無力”，ISEEQ能回覆:“請問病徵持續了多久?”，或是“你最近14天有出國過嗎?”這類可能與其他知識相關的問句。這種方式最大的貢獻及是讓機器學習到”反問”的能力，而非一味的透過query生成出結果。這樣的想法，或許可引用來提升題幹生成的精確度，舉例來說:有位教師想出關於日治時期的歷史題目，我們就能先藉由ISEEQ來詢問是要出與日治時期戰爭相關的題目或是與日治時期建設相關的題目，以此來取得更精確的題幹生成需求。[1]J.Araki,D.Rajagopal,S.Sankaranarayanan,S.Holm,Y.Yamakawa,andT.Mitamura.Gen-eratingquestionsandmultiple-choiceanswersusingsemanticanalysisoftexts.InProceedingsofCOLING2016,the26thInternationalConferenceonComputationalLinguistics:TechnicalPapers,pages1125–1136,2016.[2]C.Bhagavatula,R.L.Bras,C.Malaviya,K.Sakaguchi,A.Holtzman,H.Rashkin,D.Downey,S.W.-t.Yih,andY.Choi.Abductivecommonsensereasoning.arXivpreprintarXiv:1908.05739,[3]S.K.Bitew,A.Hadifar,L.Sterckx,J.Deleu,C.Develder,andT.Demeester.Learningtoreusedistractorstosupportmultiplechoicequestiongenerationineducation.IEEETransactionsonLearningTechnologies,2022.[4]Y.-H.ChanandY.-C.Fan.Arecurrentbert-basedmodelforquestiongeneration.InProceedingsofthe2ndWorkshoponMachineReadingforQuestionAnswering,pages154–162,2019.[5]Y.Chen,L.Wu,andM.J.Zaki.Towardsubgraphguidedknowledgegraphquestiongenerationwithgraphneuralnetworks.arXivpreprintarXiv:2004.06015,2020.[6]S.-H.Chiang,S.-C.Wang,andY.-C.Fan.Cdgp:Automaticclozedistractorgenerationbasedonpre-trainedlanugagemodel.InFindingsoftheAssociationforComputationalLinguistics:[7]Y.-S.Chuang,R.Dangovski,H.Luo,Y.Zhang,S.Chang,M.Soljac￿ic￿,S.-W.Li,W.-t.Yih,Y.Kim,andJ.Glass.Diffcse:Difference-basedcontrastivelearningforsentenceembeddings.arXivpreprintarXiv:2204.10298,2022.[8]H.-L.Chung,Y.-H.Chan,andY.-C.Fan.Abert-baseddistractorgenerationschemewithmulti-taskingandnegativeanswertrainingstrategies.InFindingsoftheAssociationforComputationalLinguistics:EMNLP2020,pages4390–4400,2020.[9]B.Dhingra,K.Mazaitis,andW.W.Cohen.Quasar:Datasetsforquestionansweringbysearchandreading.arXivpreprintarXiv:1707.03904,2017.[10]D.Dua,Y.Wang,P.Dasigi,G.Stanovsky,S.Singh,andM.Gardner.ingcomprehensionbenchmarkrequiringdiscretereasoningoverparagraphs.arXiv:1903.00161,2019.[11]T.Gao,X.Yao,andD.Chen.Simcse:Simplecontrastivelearningofsentenceembeddings.arXivpreprintarXiv:2104.08821,2021.[12]Y.Gao,L.Bing,P.Li,I.King,andM.R.Lyu.Generatingdistractorsforreadingcomprehen-sionquestionsfromrealexaminations.InProceedingsoftheAAAIConferenceonArtificialIntelligence,volume33,pages6423–6430,2019.[13]M.Gaur,K.Gunaratna,V.Srinivasan,andH.Jin.Iseeq:Informationseekingquestiongenerationusingdynamicmeta-informationretrievalandknowledgegraphs.InProceedingsoftheAAAIConferenceonArtificialIntelligence,volume36,pages10672–10680,2022.[14]Q.Guo,C.Kulkarni,A.Kittur,J.P.Bigham,andE.Brunskill.Questimator:Generatingknowl-edgeassessmentsforarbitrarytopics.InIJCAI-16:ProceedingsoftheAAAITwenty-FifthInternationalJointConferenceonArtificialIntelligence,2016.[15]D.Jin,E.Pan,N.Oufattole,W.-H.Weng,H.Fang,andP.Szolovits.Whatdiseasedoesthispa-tienthave?alarge-scaleopendomainquestionansweringdatasetfrommedicalexams.AppliedSciences,11(14):6421,2021.[16]V.Karpukhin,B.Og￿uz,S.Min,P.Lewis,L.Wu,S.Edunov,D.Chen,andW.-t.Yih.Densepassageretrievalforopen-domainquestionanswering.arXivpreprintarXiv:2004.04906,2020.[17]D.Khashabi,S.Chaturvedi,M.Roth,S.Upadhyay,andD.Roth.Lookingbeyondthesurface:Achallengesetforreadingcomprehensionovermultiplesentences.InProceedingsofthe2018ConferenceoftheNorthAmericanChapteroftheAssociationforComputationalLinguistics:HumanLanguageTechnologies,Volume1(LongPapers),pages252–262,2018.[18]O.Khattab,C.Potts,andM.Zaharia.Relevance-guidedsupervisionforopenqawithcolbert.TransactionsoftheAssociationforComputationalLinguistics,9:929–944,2021.[19]O.KhattabandM.Zaharia.Colbert:Efficientandeffectivepassagesearchviacontextualizedlateinteractionoverbert.InProceedingsofthe43rdInternationalACMSIGIRconferenceonresearchanddevelopmentinInformationRetrieval,pages39–48,2020.[20]G.Kumar,R.E.Banchs,andL.F.D’Haro.Revup:Automaticgap-fillquestiongenerationfromeducationaltexts.InProceedingsoftheTenthWorkshoponInnovativeUseofNLPforBuildingEducationalApplications,pages154–161,2015.[21]G.Lai,Q.Xie,H.Liu,Y.Yang,andE.Hovy.Race:Large-scalereadingcomprehensiondatasetfromexaminations.arXivpreprintarXiv:1704.04683,2017.[22]G.LeBerre,C.Cerisara,P.Langlais,andG.Lapalme.Unsupervisedmultiple-choicequestiongenerationforout-of-domainq&afine-tuning.InProceedingsofthe60thAnnualMeetingoftheAssociationforComputationalLinguistics(Volume2:ShortPapers),pages732–738,2022.[23]K.Lee,M.-W.Chang,andK.Toutanova.Latentretrievalforweaklysupervisedopendomainquestionanswering.arXivpreprintarXiv:1906.00300,2019.[24]C.Liang,X.Yang,N.Dave,D.Wham,B.Pursel,andC.L.Giles.Distractorgenerationformultiplechoicequestionsusinglearningtorank.InProceedingsofthethirteenthworkshoponinnovativeuseofNLPforbuildingeducationalapplications,pages284–290,2018.[25]C.Liang,X.Yang,D.Wham,B.Pursel,R.Passonneaur,andC.L.Giles.Distractorgenera-tionwithgenerativeadversarialnetsforautomaticallycreatingfill-in-the-blankquestions.InProceedingsoftheKnowledgeCaptureConference,pages1–4,2017.[26]V.Liévin,C.E.Hother,andO.Winther.Canlargelanguagemodelsreasonaboutmedicalquestions?arXivpreprintarXiv:2207.08143,2022.[27]Y.Mao,P.He,X.Liu,Y.Shen,J.Gao,J.Han,andW.Chen.Generation-augmentedretrievalforopen-domainquestionanswering.arXivpreprintarXiv:2009.08553,2020.[28]Y.Nie,S.Wang,andM.Bansal.Revealingtheimportanceofsemanticretrievalformachinereadingatscale.arXivpreprintarXiv:1909.08041,2019.[29]H.-Y.Peng,H.-L.Chung,Y.-H.Chan,andY.-C.Fan.Misleadinginferencegenerationviaproxi-malpolicyoptimization.InPacific-AsiaConferenceonKnowledgeDiscoveryandDataMining,pages497–509.Springer,2022.[30]L.Qin,A.Bosselut,A.Holtzman,C.Bhagavatula,E.Clark,andY.Choi.Counterfactualstoryreasoningandgeneration.arXivpreprintarXiv:1909.04076,2019.[31]P.Rajpurkar,R.Jia,andP.Liang.Knowwhatyoudon’tknow:Unanswerablequestionsforsquad.arXivpreprintarXiv:1806.03822,2018.[32]P.Rajpurkar,J.Zhang,K.Lopyrev,andP.Liang.Squad:100,000+questionsformachinecomprehensionoftext.arXivpreprintarXiv:1606.05250,2016.[33]S.Ren,Y.Deng,K.He,andW.Che.Generatingnaturallanguageadversarialexamplesthroughprobabilityweightedwordsaliency.InProceedingsofthe57thannualmeetingoftheassociationforcomputationallinguistics,pages1085–1097,2019.[34]S.RenandK.Q.Zhu.Knowledge-drivendistractorgenerationforcloze-stylemultiplechoicequestions.InProceedingsoftheAAAIConferenceonArtificialIntelligence,volume35,pages[35]Y.Su,T.Lan,Y.Wang,D.Yogatama,L.Kong,andN.Collier.Acontrastiveframeworkforneuraltextgeneration.arXivpreprintarXiv:2202.06417,2022.[36]A.Talmor,J.Herzig,N.Lourie,andJ.Berant.Commonsenseqa:Aquestionansweringchallengetargetingcommonsenseknowledge.arXivpreprintarXiv:1811.00937,2018.[37]J.Welbl,N.F.Liu,andM.Gardner.Crowdsourcingmultiplechoicesciencequestions.arXivpreprintarXiv:1707.06209,2017.[38]J.Welbl,P.Stenetorp,andS.Riedel.Constructingdatasetsformulti-hopreadingcomprehensionacrossdocuments.TransactionsoftheAssociationforComputationalLinguistics,6:287–302,[39]T.Wu,M.T.Ribeiro,J.Heer,andD.S.Weld.Polyjuice:Automated,general-purposecounter-factualgeneration.arXivpreprintarXiv:2101.00288,1(2),2021.[40]J.Xie,N.Peng,Y.Cai,T.Wang,andQ.Huang.Diversedistractorgenerationforconstruct-inghigh-qualitymultiplechoicequestions.IEEE/ACMTransactionsonAudio,Speech,andLanguageProcessing,30:280–291,2021.[41]Z.Yang,P.Qi,S.Zhang,Y.Bengio,W.W.Cohen,R.Salakhutdinov,andC.D.Manning.Hotpotqa:Adatasetfordiverse,explainablemulti-hopquestionanswering.arXiv:1809.09600,2018.[42]M.Yasunaga,A.Bosselut,H.Ren,X.Zhang,C.D.Manning,P.Liang,andJ.Leskovec.Deepbidirectionallanguage-knowledgegraphpretraining.arXivpreprintarXiv:2210.09338,2022.[43]C.Y.Yeung,J.S.Lee,andB.K.Tsou.Difficulty-awaredistractorgenerationforgap-fillitems.InProceedingsoftheThe17thAnnualWorkshopoftheAustralasianLanguageTechnologyAssociation,pages159–164,2019.[44]C.Zhang,Y.Sun,H.Chen,andJ.Wang.Generatingadequatedistractorsformultiple-choicequestions.arXivpreprintarXiv:2010.12658,2020.[45]X.Zhou,S.Luo,andY.Wu.Co-attentionhierarchicalnetwork:Generatingcoherentlongdistractorsforreadingcomprehension.InProceedingsoftheAAAIConferenceonArtificialIntelligence,volume34,pages9725–9732,2020.[46]B.Zou,P.Li,L.Pan,andA.Aw.Automatictrue/falsequestiongenerationforeducationalpur-pose.InProceedingsofthe17thWorkshoponInnovativeUseofNLPforBuildingEducationalApplications(BEA2022),pages61–70,2022.於本研究計畫中，我們目標基於自然語言生成技術進行選擇題自動出題研究。我們預期的展示情境為：給定一知識範圍(如農業文集)，我們可於系統直接輸入一組主題關鍵詞，而後自動生成關於該主題相關之選擇題題組(含字詞等級、句子等級、挑選是非類型之選擇題)。然而，如同前言背景中所言，現有技術離目標仍有許多待強化之處，對此我們擬定三大出題目標：字詞等級干擾選項、句子等級干擾選項與反事實干擾選項進行研究。針對此三大目標，我們擬定(1)基於知識圖譜之強化與(2)基於生成語言模型與對比學習之強化三個核心技術進行探討。研究技術項目與項目間之關係與進行規劃如Figure7所示。於第一年度中，我們規劃「考量題幹可用性與干擾選項間關聯之Retrieve-and-Generate架構優化」、「基於語言模型與知識圖譜之Text2Text干擾選項生成」與「基於Masked-LM之反事實干擾選項生成研究」進行研究。「考量題幹可用性與干擾選項間關聯之Retrieve-and-Generate架構優化」著眼於優化目前https://mcq-demo.nlpnchu.org之WordLevel之選擇題生成系統。希冀自動過濾相似或語意不正確題幹，並且考量干擾選項彼此間之關聯進行最終干擾選項集合之綜合挑選。「基於生成語言模型之Text2Text干擾選項生成模型」則考量Text2Text模式利用生成模型直接生成干擾選項序列，藉此嘗試生成句子等級之干擾選項。並進一步地探索結合知識圖譜與語言模型之干擾選項生成，希冀透過知識圖譜之整合來生成更有誘導性且合理之字詞等級干擾選項。此外，於「基於Masked-LM之反事實干擾選項生成研究」項目中，我們預計著手挑戰反事實干擾選項生成，我們嘗試利用透過NER擷取給定事實陳述句中之關鍵字並Masked-LM該關鍵字來填入機率較低之字詞，以為反事實語句生成之作法。我們預期第一年度工作完成後，將可優化現有單詞等級干擾選項之選擇題自動生成系統，並於第二年度中，我們則規劃「基於知識圖譜強化之反事實干擾選項生成」、「基於對比式學習之干擾選項生成模型強化」與「利用Open-DomainQuestionAnswering錯誤結果之干擾選項挑選」。於此年度中，進一步嘗試利用知識圖譜萃取三元組知識後進行反事實語句之生成，基本想法為透過萃取三元組後再進一步輸入語言模型，並希望語言模型適當地進行反事實語句的改寫。此一部分預計使用第一年度之「基於Masked-LM之反事實干擾選項生成研究」之基礎以為訓練資料。對於字詞等級的干擾選項，我們則探索利用Open-DomainQuestionAnswering模型所錯誤回報之答案當作干擾選項。我們希望透過此不同之想法來取得更多樣的干擾選項。再者，如前言所述，生成模型生成序列時常出現生成結果重複的狀況，對此我們嘗試利用對比式來改善生成模型這方面的缺點，以優化干擾選項之生成。第二年度工作項目完成後，我們可初步提供反事實干擾選項之選擇題生成、優化後之句子等級干擾選項生成與基於ODQA方式取得之字詞等級干擾選項生成。於第三年度中，我們則規劃「基於NodeCentrality考量主題涵蓋性之選擇題題組生成」與「生成難度可控之干擾選項生成研究」。如同前言所述，一個理想的選擇題出題系統應該考慮題組生成，整體題目之主題涵蓋性，對此我們也將嘗試基於知識圖譜涵蓋的方式考慮節點centrality重要度方式來進行題組之生成。同時我們也將嘗試題目難易度之控制生成研究。我們的想法是當自動生成成為可能後未來的出題應該是個人化模式；針對個人的學習狀況進行測驗，因此我們規劃「個人化與生成難度可控之干擾選項生成研究」進行探討。第三年度，我們也預計投入人力進行系統的實作開發與整合。以下茲針對擬訂之研究議題其問題挑戰與1.基於語言模型與知識圖譜之Text2Text干擾選項生成模型近年來，語言模型除了使用CandicateGeneratingandRanking架構的MaskedLM之外，使用Encoder-Decoder架構的Text2Text模型於生成任務上也有很好的成果，因此我們的想法是把干擾選項生成Model視為一個Text2Text任務，嘗試讓模型直接生成干擾選項。且使用Text2Text模型生成Distractor有別於用MaskedLM生成Distractor，我們希望模型能生成的Distractor有以下特性(1)非單詞之Distractor:目前在Distractor生成上多為單詞生成，要生成一段敘述/句子的干擾選項還沒有很好的做法。(2)特定domain之MCQ:目前的MCQ多為英文測驗，若想針對特定領域出選擇題，使用MaskedLM生成的Distractor會因語料庫限制而為達到上述的兩種目標需求，我們設Q為輸入之問句，A為輸入問句之答案，並將Q與A字串接字串當作輸入，訓練模型輸出一連串問句Q的干擾選項字串D=(d1,d2,d3)。模型架構如圖2所示，我們將「哪一種技術最適用於嚴重特殊傳染性肺炎之病原快速篩檢?」與「聚合￿連鎖反應」接在一起輸入至Text2Text模型，而模型生成了「重組DNA技術」、「電腦斷層掃描」與「組織培養」三個干擾選項，三個選項皆為醫療檢測方式且都不是單詞，符合如前言所述，我們也預計探索使用知識圖譜來強化生成干擾選項的能力。如Figure8的圖例，我們可以使用語言模型結合醫療知識圖譜，利用給定的文本從知識圖譜中萃取有包含這些topicentities以及相鄰的nodeentities的子圖，再分別將文本與子圖輸入至語言模型和GraphNeuralNetwork(GNN)，讓模型學習找出關鍵的資訊與強化network中重要entitynodes跟edges的權重，藉由GNN的額外資訊與語言模型能力生成更好的選項。Figure8為我們規劃做法的概觀。主要分兩階段進行：(1)關鍵字子圖與GNNEmbedding萃取與(2)GNNEmbedding與LanugageEmbedding間之FusionEncoder生成。具體而言，第一個階段是將從知識圖譜裡萃取相關的知識圖譜子圖。我們一開始先使用NERModel抓取題幹中的entities，再透過這些entities透過知識圖譜去尋找entitynodes以及所有跟entitynodes相鄰3-hop內的bridgenodes，將檢索出來的結果當成知識圖譜子圖，並在後續第二階段提供以Figure9的圖例，我們先將問題透過NERModel斷詞得到”白血病”、”發病”、”器官”與”發病原因”當作entities，再透過這些entities當作關鍵字透過Retriever從知識圖譜萃取Figure8:結合知識圖譜與語言模型生成之構想架構圖與這些詞相關的node和edge，如利用”白血病”與”器官”能找到”肝臟”、”骨髓”、”肺部”等等entities，我們能利用這些entities來當作後續的生成選項。第二個階段則將文本與第一階段得到的知識圖譜子圖分別輸入至語言模型與GNN進行訓練，訓練目標為將Input的[MASK]Token還原來產生干擾選項，先將語言模型的Input額外新增[MASK]Token，經過N層LMLayer做self-attention後，透過FusionEncoder再結合知識圖譜的資訊與原本的文本去生成選項，透過以上的手法，來強化干擾選項之生成。2.考量題幹可用性與干擾選項間關聯之Retrieve-and-Generate架構優化如同研究目的所述，本研究項目著眼於優化現有Retrieve-and-Generate生成架構。我們•文章檢索器過濾:在Retrieving-and-Generating方法中Retriever會檢索回傳含有主題字詞的文章，但有部分文章並不適合當作問句生成模型的輸入，那些文章含有太多的雜訊，我們想嘗試在這步驟先過濾一些文章。我們的想法是利用詞向量中的相似詞再次檢索Retriever回傳的文章，把過於不相關的文章濾掉，減少問句生成模型輸出過於複•生成題幹品質自動過濾：問句生成模型輸出的問句，會再交由使用者挑選，但為了更好的效果，我們想嘗試利用語言模型做初步的篩選，減少人工挑選時間。我們想嘗試利用ODQA模型來過濾問句，將問句生成模型輸出的問句輸入至問答模型後，根據問答模型的回答結果與信心分數來決定該問句是否合適當作MCQ。如Figure10所例，我們將生成之問句再次投入ODQA模型，透過答案選項之信心度來為問句生成過濾之基Figure10:Unanswerablequestionsolution•考量干擾選項間關聯之干擾選項挑選:如前言所述，一個可以提升選擇題生成品質￿考量干擾選項彼此之間之難易度。而在Retrieving-and-Generating方法中，生成干擾選項的方式為取出詞向量模型中與主題字詞相似之詞，使用相似度前三名當作干擾選項，因此有選項過於相近而造成問題太難的可能。為做改善，我們取出所有相似詞的向量與主題字詞向量計算相對距離，設定各個選項與答案的相似度範圍(距離範圍)，再從中隨機挑選相似詞，讓干擾選項彼此之間難易度有所區分以提升試題之鑑別度。•基於語言模型之干擾選項候選生成:雖然詞向量模型生成distractor過程迅速，但如果有主題字詞沒有被詞向量模型看過或有主題字詞一字多意，就無法生成適當的干擾選項。這個問題我們想嘗試使用MaskedLM來解決，我們將Retriever回傳的文章中含有主題字詞的段落記錄下來，移除主題字詞後輸入MaskedLM，再從MaskedLM輸出的候選字中選出distractor，使用這個方式生成的干擾選項不會被上述的缺點受限，也更Figure11:OptimizedRetrievingandGeneratingFramework3.基於Masked-LM之反事實干擾選項生成研究本研究項目為基於Masked-LM技巧來生成反事實干擾選項。我們需要先查找與事實相符之敘述，再修改成反事實干擾選項。我們預計之作法為：首先讓使用者先輸入考題關鍵字，再使用retriever查找相關文章，其目的為從相關文章中選取與事實相符之敘述。接著，將retriever檢索到的文章回傳給使用者，由使用者選取與事實相符之敘述。並按照以下之步驟進行Masked-LM反事實干擾選項生成。為了抽換專有名詞，我們先使用命名實體與事實相符之敘述中的所有專有名詞，以下圖為例”腎小管藉壓力差方式再吸收葡萄糖回微血管”，透過命名實體模型會萃取出專透過命名實體模型，我們會萃取出多個與事實相符之敘述中的專有名詞，我們需要選擇一個專有名詞行抽換。我們認為該專有名詞在相關文章中的重要程度越高，越高機率是考題中的重點。為了確保抽換專有名詞的品質，我們使用BM25來查看各個專有Figure12:AutoEncoder模型與AutoRegressive模型做MaskLM3.使用Maskedlanguagemodel模型進行改寫Maskedlanguagemodel，簡稱為MaskLM，該任務的會將輸入中的部分內容遮擋起來，模型會根據其他未被遮擋的內容，預測出被遮擋的內容或與其擁有相似語義的內容，現有的MaskLM模型分為兩種，分別為AutoEncoder框架的MaskLM模型與AutoRe-gressive框架的MaskLM模型，經典模型如BERT與BART。針對MaskLM任務，兩種模型進行預測的方式也有所不同，AutoEncoder框架的模型會預測出字，而AutoRe-gressive框架的模型會預測出字、詞或是句子，詳細範例如Figure12。根據抽換專有名詞的需求，我們將使用Autoregressive模型做MaskLM任務來抽換重要的專有名詞。然而模型預測出的內容眾多，有可能是原先遮擋的內容、遮擋內容的同義詞或是與遮擋內容擁有相似語義的內容。理想的情況下，模型預測的內容最高機率為原先遮擋的內容，若我們選擇原先遮擋的內容或是遮擋內容的同義詞進行抽換，會造成與事實相符之敘述未改變，便無法生成反事實干擾選項。為了避免此狀況發生，我們使用NLI任務中的換句話說(MRPC)任務來檢測其是否為干擾選項。換句話說任務主要在判斷兩句話的語意是否相同，為了避免MaskLM模型選用同義詞進行抽換，即模型生成的干擾選項仍是正確的，我們將與事實相符之敘述與生成之干擾選項作為模型的輸入，若模型認為兩句話具有相同語義，我們會讓MaskLM模型預4.基於知識圖譜強化之反事實干擾選項生成於本研究項目中，我們嘗試利用知識圖譜提供語言模型額外資訊達成反事實干擾選項生成。我們的構想如Figure4所示，包含以下步驟。首先我們會讓使用者先輸入考題內文，再將內文使用斷詞或NER模型萃取entity，利用這些entity於知識圖譜中查找相關子圖，子圖必須包含內文中所有的entitynodes以及所有跟entitynodes相鄰節點，以此為知識生成之基礎。接著我們將檢索到的子圖內的nodes與edges萃取成三元組，每筆三元組的結構是以node,edge,node組成。如同圖中所示，每一筆三元組描述一基本事實陳述，如導體可以3.將與事實相符之敘述修改為反事實干擾選項為了將與事實相符之敘述更改為反事實干擾選項，我們將前一步驟所萃取的三元組與考題內文當作作語言模型的輸入來生成反事實干擾選項。我們預計透過生成語言模型與輸入給定的問句和三元組來生成與問句情境相符合的反事實干擾選項，以FigurejQuery36308127470250942248_1676210083761為例，生成語言模型在給定關於與導體相關的題目敘述以及從知識圖譜萃取與導體相關的三元組後，模型藉由這些三元組推斷其中的關係，比如”導體可以導電->石墨是導體->筆芯是由石墨製成”，從上述的三元組推斷出”筆芯可以導電”的事實，也可以改寫成”筆芯不能導電”的反事實干擾選項。5.使用Open-DomainQuestionAnswering錯誤結果之干擾選項挑選研究如前言所述，我們預計使用ODQA模型所回答之錯誤結果來當作干擾選項(如Figure5)。以我們先前曾開發https://agri.nlpnchu.org/之農業問答系統為例，我們發現以上想法所面臨的挑於ODQA的架構下，可能會遇到ODQA正確回答所下達之問題，其回傳結果皆為可行之答案，也因此無法被使用來當干擾選項，對此，我們探索兩種作法來進行調整。–1.使用BM25，且刪除尋找出來文章的正確答案在使用BM25的情境下，因為是使用關鍵字比對，所以會找出許多跟正確答案相關的文章，我們將找到文章的正確答案刪除，這樣這個文章會變成絕對回答不出正確答案的文章，可是因為Reader對文字有一定的理解能力，並且具有關鍵字的文章中也很可能會有，他會從剩下可能的答案中，尋找一個最相近的選項，這樣就可以較容易生出干擾選項。如Figure13所示，我們將答案”2,4-D”從文章中直接刪除後，再交給Reader進行閱讀，藉此確保正確答案不會被回傳。Figure13:基於ODQA架構之Retriever弱化方法–2.使用DensePassageRetrieverDensePassageRetriever[16]，DensePassageRetriever可以將資料庫的文章轉為向量的方式表示，在搜尋的時候也將Query轉為向量，並且比對兩個向量的相似度，這樣就可以找出相似的文章，可是我們想說，可以利用DPR找尋文章相似度的這一點，理論上相似的文章跟Query算出來的相似度會是最高的，我們可以將相似度高的文章拿出來，並且查看有沒有答案出現在上面，如果沒有答案，我們就將他丟給Reader，因為我們覺得如我相似度高，就代表Retriever認為裡面有答案，可是裡面並沒有含有我們的正解，所以這篇文章就含Retriever認為是答案，但是實際上不是答案的文字，所以這時候我們將這篇文章傳給Reader，就會有比較高在我們使用baseline的情境中，有時候也會出現不相關的答案，例如在??中，問模型關於殺草劑的問題，會出現的人力除草，人力除草跟題目所問的殺草劑名稱是不相關的，所以我們需要用一個方法把他過濾掉。在這種情況下，我們如果加入正確答案當作Query一起輸入Retriever，就會增加模型找到正確passage的機率，對我們想找跟正確答案不相關但是相似度較高的Passage會產生反效果，所以我們採用的是從Reader輸出之後把他過濾掉的方法。當Reader從Retriever拿到Passage之後，我們會將正確答案去除，之後再將剩餘的答案丟入一個LanguageModel作判斷，如果他跟答案的距離過於遙遠，就放入NegativeList，如果是相關的就放入PositiveList，之後要挑相關答案的時候可以從PositiveList裡面挑選。6.基於對比式學習之干擾選項生成模型強化ContrastiveLearning的設計上需要考量如何去設定PositiveSample以及NegativeSample。在我們的目標中，因為不希望模型會重複的生成出相同意思的句子，所以我們預計使用同義的句子當作NegativeSample，而資料集中的干擾選項可以被當作PositiveSample。然而，由於我們的資料集中並沒有NagativeSample的選項，也就是同義於模型生成的選項。對此，我們的想法為透過迭代生成來產生我們需要的NegativeSample。具體而言，如Figure14所舉例，在DistractorGenerator的第一次生成中，我們會輸入Question以及Answer使模型生成出干擾選項。而在第二次迭代中，我們除了會輸入QuestionAnswerPair外，還會再加上在上一次迭代中生成的干擾選項，以提示模型先前已生成的Distractor的資訊。對於第二次迭代的輸出來說，第一次迭代輸出的Distractor，即為它的NegativeSample，因為DistractorGenerator必須要生成出與上一次迭代生成的Distractor意義相異的句子，所以它們之間的SentenceEmbedding不能太過接近。Figure14:IterationGenerater示意，在第一次的迭代中生成的Distractor會作為第二次迭代的輸入，以提示模型不要生成出類似的DistractorFigure15:對比式學習於干擾選項生成之預想架構我們所規劃之整體框架，如Figure15所示。我們使用多次迭代的方式來生成Distractor，對於第一次迭代來說，我們會從其他in-batch的data中取得Sentence當作NegativeSample。而在第二次以後的迭代來說，除了其他in-batch的Sentence以外，我們還會將此次迭代前所生成的Distractor加入我們的NegativeSample中。透過NegativeSample和模型輸出的Distractor的dotproduct可以計算出ContrastiveLoss，此外我們也會加入Distractor和Label計算的CrossEntropyLoss，來計算出我們的TotalLoss15。ContrastiveLoss定義如下:x為生成的distractor，p為資料集中的Distractor(Label)，N為NegativeSampleSet，N={n1,n2,...nj}，其中包括先前迭代生成的Distractor以及其他in-batch的選項。7.基於NodeCentrality考量主題涵蓋性之選擇題題組生成如同前言所述，本研究議題之目標為考量主題涵蓋性之選擇題題組生成。因此，我們預計根據使用者所指定的主題，按Figure16架構規劃來進行題組生成。1.透過知識圖譜子圖萃取來涵蓋所指定主題之內容：藉由知識圖譜上NodeCentrality方式來萃取關鍵字，以為題幹之生成。我們的基本想法為在知識圖譜上我們定位出與使用者輸入之主題關鍵字相關之結點後，我們可以試者套入PageRank或者其他圖形上計算NodeCentrality重要節點挑選，並以重要節點為題幹生成之主軸。2.透過上一步取得之重要答案片段後，進行問題之生成。並生成較多的問題數量以為CandidateQuestionPool，而後將問題將題組生成問題設定一成為一選擇最佳化議題(Optimization)：根據主題生成大量的單一題目，而後定義UtilityScore來計算個別題目組合後之優劣，來挑選最佳化的題組以為輸出。於本研究項目中，我們預計探索以下兩種生成難度控制方式。•調整Masked-LM比例之反事實選項難度控制：現有的Masked-LM模型會將重要的專有名詞進行遮擋，模型會透過未遮擋的內容對該專有名詞進行預測。理想的情況下，當我們僅對該專有名詞進行遮擋時，模型可成功地預測出遮擋的專有名詞或語義相近的專有名詞，這不僅是因為MaskLM模型夠強大，也是因為未遮擋的內容提供模型足夠多的資訊讓模型預測，若未遮擋的內容提供的資訊不夠多，則模型預測出的內容有可能會與遮擋的內容語義相差很多。我們根據Masked-LM模型的此特性對題目的難易度進行控制，若我們將多個專有名詞進行遮擋，再讓模型預測時，模型會因為提供的資訊不足，而預測出語義較不相近的專有名詞，即較不可能的答案。以下表為例，我們可以觀察到當[Mask]越多時，我們要判斷該事實是否正確就越形容易。[Mask]借壓力差吸收[Mask]回微血管[Mask]借壓力差吸收[Mask]回[Mask]•調整ODQA檢索文章之難度控制：由於我們的基礎架構為基於文章檢索(DocumentRetrieving)來取得包含使用者所給定的關鍵字之文章，並使用該文章與關鍵字為問題生成模型之輸入，來生成選擇題問句。我們若挑選較為冷門之文章進行生成，則預期可以生成較為困難之題幹。而就干擾選項而言，我們可藉由調整輸入給閱讀理解模型之文章來產生難度較高的錯誤。具體而言，給定一有包含正確答案A之文章C，我們先將答案片段A於文章D中刪除，而後將其輸入至閱讀理解模型，藉此產生錯誤之閱讀理解結果，以這樣的錯誤結果當Distractor預期將有較高之誘導能力。相類似的想法也可以套用為挑選Retriever認為相關度比較不高的文章以為閱讀理解模型之輸入。此外，就生成題目之廣度與深度而言，我們可藉由控制取得之文章來調配題組之廣度與深度。例如，若我們以單篇或少數篇文章進行生成，我們所生成的主題題組針對測驗內容而言，應該可產生知識性較為深入之試題。而若針對較多篇文章，則應可產生範圍較廣本研究計畫將以三年為期，共八項研究議題進行，各研究產出如上文所示，並整合各技術成為一選擇題自動生成系統，預期的展示情境為：我們可於系統直接輸入一知識範圍，系統將利用我們所開發之技術先進行自動關鍵語萃取，而後進行各種類型的選擇題出題並考量題組關聯性。我們也將彙整各研究議題中所發展之技術為一個成為公開原始碼專案，以為後續相關研究之用。並將計畫執行過程所累積的資料，公開希望能吸引更多研究者之投入。對於學術研究、國家發展及其他應用方面預期之貢獻自然語言生成已是未來深度學習領域之熱門研究重點。透過本研究計畫之投入我們將可進行先驅之探索，建立學生團隊與自然語言處理與生成之能力，並持續過去之閱讀測驗自動出題研究成果，以知識類型選擇題自動出題為主要系統開發目標為例，希冀能為國內外探索不同的自然語言生成應用服務。此外若可順利完成也可以成為一成功範例利用人工智慧輔助教師進行測驗的客製化與個人化學習。對比於題庫類型的方式，透過我們技術我們將可動態產生各式各樣不同之考題為每個人量身定做難度適中之考題。就此一觀點出發，我們預期本計畫除學術研究價值外，同時也兼具商業價值，也應可成為EdTech(教育科技)領域之一環。再者，疫情改變了學習與教學的模式。同步與非同步之線上教學已成為教學模式常態之一。線上教學帶來了好處，但也帶來了顧慮。其中，學習的專注度應是諸多顧慮中主要令人擔憂的一環。隔著螢幕學生學習是否專注？目前的教學內容學生是否已跟上進度？等等問題是線上教學學習專注度所在意的問題。對此，我們認為適時與即時地檢測學生學習成效，來提醒學生與教師目前之學習成效，應能適度緩解線上學習專注度之顧慮。因此，我們也常見於MOOCs課程於一個授課單元結束後適時地給予測驗(多數以選擇題測驗)，來確認學生學習成效。然而，人工設計試題耗費心力，一成不變全班同學皆相同試題也可能透過即時訊息(instantmessage)之分享而取得答案，造成檢測之無效。對此，我們認為隨著深度學習技術之快速演進，結合自然語言(NaturalLanguageGeneration)生成技術來進行自動出題技術已成為可能；透過自然語言生成技術自動化出題，產生多樣化的試題，來進行個人化的檢測，將有除了可以增進研讀論文、撰寫程式、架設系統以及撰寫論文的基本能力之外，參與本計畫之研究人員，將可以學習到深度學習與自然語言生成技術之基本觀念、實際應用以及相關的核心技術，在這些基礎之下，透過積極的研究與討論，將可以提升參與計畫人員思考問題與解決困難的重要能力。我們預期可獲得具體技能如下:(1)設計與建立一個深度學習模型之能力，(2)整合不同先進技術之能力，(3)自然語言生成技術之瞭解，(4)資訊檢索相關知識與軟體套件之暸解，(5)佈建深度學習模型於WebScale之能力。"}, "1676210977.3826022": {"單位": "", "標題": "(ㄧ)、研究計畫之背景與目的", "作者": "", "發布日": 0, "摘要": "", "全文": "自然語言生成(NaturalLanguageGeneration)將是未來深度學習領域之熱門研究議題。2022年年末ChatGPT語言模型所帶來的示範性與展示效果預期將吸引更多研究目光。而個人於中興大學之研究團隊近年於自然語言生成之耕耘已建立初步研究成果。在過去朝著自然語言生成方向前進，我們探索自動生成試題之研究方向前進。於前期研究計畫「基於深度學習語言模型之閱讀測驗自動化生成研究109-2221-E-005-058-MY3」中，我們著眼於基於深度學習模型進行閱讀測驗(ReadingComprehensionTesting)題型之自動出題。在計畫執行過程中，我們•Ying-HongChan,Yao-ChungFan:KeywordProvisionQuestionGenerationforFacilitatingEducationalReadingComprehensionPreparation.INLG(InternationalNaturalLanguageGen-erationConference)2022.•Ho-LamChung,Ying-HongChan,Yao-ChungFan:ABERT-basedDistractorGenerationSchemewithMulti-taskingandNegativeAnswerTrainingStrategies.EMNLP(EmpiricalMethodsinNaturalLanguageProcessing)Findings2020.•Shang-HsuanChiang,Ssu-ChengWang,Yao-ChungFan:CDGP:AutomaticClozeDistractorGenerationbasedonPre-trainedLanguageModel.EMNLP(EmpiricalMethodsinNaturalLanguageProcessing)Findings2022.•Hsien-YungPeng,Ho-LamChung,Ying-HongChan,Yao-ChungFan:MisleadingInferenceGenerationviaProximalPolicyOptimization.PAKDD(AdvancesinKnowledgeDiscoveryandDataMining-26thPacific-AsiaConference)2022此外我們也將前期計畫之自動出題研究成果進行實作，可於以下兩個網址進行實測。•問句生成QuestionGeneration：給定文章、指定片段，透過自然語言生成模型進行問句生成。目前系統可於https://classic.queratorai.com/進行實測。•克漏字干擾選項生成DistractorGeneration：此一部分針對給定之文章進行克漏字誘導選項生成。目前系統可於：https://cdgp-demo.nlpnchu.org/進行實測。基於過去之研究成果累積，本計劃延續自然語言生成技術於自動出題之研究成果，擬定知識類型選擇題(MultipleChoiceQuestion,MCQ)自動生成為目標。以四技二專統一測驗考試為例(如Table2所示)，知識類型選擇題將包含題幹與四個選項(其中包含正確答案與干擾選項)，主要針對某一知識範圍進行測驗。我們認為知識範圍選擇題之自動生成可分為兩部分：(1)問題題幹(Stem)生成與(2)干擾選項(Distractor)生成。題幹生成主要目標為選擇題問題本身之生成，而干擾選項則著眼於誘導選項之生成。知識類型選擇題為學習測驗最常見的測驗模式之一。對比於閱讀測驗生成，知識類型選擇題的自動生成，將可減輕教師準備個人化教學測驗之負擔，也為學生提供更有效率之學習檢測方式。以現有我們成果為基礎，一個基本之自動選擇題出題系統作法為(如圖1)所示：我們可利用資訊檢索技術(如BM25Retriever)，從給定之文集資料庫(目前使用2021中文維基百科所有中文文章)中檢索主題字詞相關文章後，結合前期計畫所開發之問句生成模型(QuestionGeneration)[4]生成題幹，而後再利用詞向量模型(例如word2vec模型)找出相近字詞以為干擾選項。我們將這樣的構想先行實作一系統(稱之為Retrieve-and-GeneratePipeline)，目前此系統可於https://mcq-demo.nlpnchu.org/使用。使用者可藉由輸入主題相關字詞，選擇題生成系統將自動生成輸入字詞相關之選擇題結果。舉例而言，我們以「骨髓、白血病、淋巴」為主題字詞輸入至系統。我們挑選較佳之成果顯示於Table1。自動生成選擇題AutomaticGeneratedMCQQ1:哪一種癌症種類的統稱中可以看出它會造成不正常白血球的大量增生?(a)白血病、(b)乳腺癌、(c)腎病、(d)帕金森氏症Q1:白血病的發病原因通常是發生在哪一個器官上?(a)肝臟、(b)淋巴、(c)甲狀腺、(d)骨髓Q:白血病是最常見的兒童癌症，其中80%的病例是慢性的哪一種性病變?(a)淋巴結、(b)微血管、(c)淋巴、(d)黏膜Table1:目前選擇題自動生成結果範例。目前系統可於https://mcq-demo.nlpnchu.org/。目前我們挑選較佳之生成結果顯示於此Table中。建議審查委員自行輸入關鍵字詞組，來觀察我們Figure1:Retrieve-and-Generate基本架構。本例中我們以”骨髓、白血病、淋巴”為關鍵詞進行文章檢索，而再使用”骨髓”為答案，分別生成問句與干擾選項。從展示之結果中，我們發現自動選擇題出題之可能性。但仔細觀察會發現現有系統離實際應用仍有空間須待努力。也因此觸發本期研究計畫之提出。具體而言，我們認為選擇題自•句子等級之干擾選項生成(Sentence-LevelDistractorGeneration)首先，關於干擾選項的部分，若仰賴詞向量模型來產生干擾選項，我們所能取得的干擾選項便僅限於單詞(Word-Level)等級。從統測試題中，我們發現許多題目之干擾選項屬於句子等級(Sentence-Level)。以Table2中Q3與Q4選擇題為例，我們可以看到選項皆為句子等級之描述，此為目前基於詞向量來產生干擾選項的做法所不能。•反事實干擾選項生成(Counter-FactualSentenceGeneration)我們觀察到選擇題題型中，有著下列何者為錯誤或正確類型之考題。其干擾選項是一個較長的描述句。以Table2為例，Q5與Q6為分別詢問下列選項何者正確/錯誤。此題型，為變像之是非題。除了句子等級生成外，核心技術牽涉的是反事實(Counter-Factual)句子之生成。如何生成此類型之句子以為干擾選項也將是研究重點所在。•選擇題生成品質提升(MCQQuestionQualityImprovement)此外，我們也看到retreive-and-generatepipeline的做法，所產生的題幹本身有時是一個過於空泛之問題(例如，下列何種疾病為兒童常見之癌症？)或者並非語意正確之語句(例如，中樞性的什麼症狀是缺乏抗利尿激素的?)。如何有效判斷生成不良之語句，減低人工進一步過濾/挑選題目之負擔也是需進行研究之議題。此外，基於詞向量之干擾選項作法也有待商榷;隨機從相近詞向量挑選distractor，是否能確保distractor之誘導品類型I:單詞等級的干擾選項Word-LevelDistractorsQ1：淋巴器官是人體防禦系統中重要的一環，可提供淋巴細胞發育或活化所需要的條件與狀態。下列何者是淋巴球進行防禦與活化的器官？(a)骨髓(b)胸腺(c)肝臟(d)脾臟Q2：健康人體感覺神經傳入大腦的訊息大部分經過轉接，(a)視丘(b)下視丘(c)中腦(d)橋腦類型II:句子等級的干擾選項Sentence-LevelDistractors(a)無就巢性(b)體型大(c)著肉多(d)耐粗食Q4：下列何者為「聯合國國際糧農組織」(FAO)成立最重要的目標及首要任務？(a)消除及戰勝飢餓(b)聯絡及交流農業資訊(c)自然環境的永續及保護(d)實現國際間的公平貿易類型III:句子等級且與事實相違背的干擾選項Counter-FactualDistractorsQ5：有關人類上皮細胞的細胞週期之敘述，下列何者正確？(c)後期，分離的染色分體往細胞中央移動(d)末期，細胞膜內凹且細胞質開始一分為二Q6：有關一健康成人泌尿系統功能的敘述，下列何者錯誤？(a)腎臟每日約產生濾液180公升及尿液1.5公升(b)腎小管藉壓力差方式再吸收葡萄糖回微血管(c)腎臟藉排尿量多寡來維持體內水分的恆定(d)若出現尿毒症狀時，可使用血液透析方式進行治療Table2:四技二專統一測驗選擇題類型概觀。Q1與Q2擷取自110學年度護理基礎生物考科，Q3與Q4110學年度農業群農業概論考科，Q5與Q6取自110學年度農業群基礎生物考科質？也許所挑選的distractor並不具誘導性，使得受測者很輕易地可挑選出正確答案，或者挑選到答案的同義詞，造成一題有多個正確答案。根據先前計畫執行訪談教師所知，於測驗時為考量受測全體鑑別度，教師出題時會考量干擾選項彼此之間之難易度；通常三個干擾選項中，會有一個離正確答案較遠之干擾選項(誘導性較低)，一個次之，一個最為困難最具有誘導性。藉此來有適當的鑑別度。目前我們直接使用詞向量之作法並未考量此設計。•主題題組生成(MCQQuestionGroupGeneration)目前我們的系統僅為單一關鍵字詞之出題。我們認為一個良好的自動出題應該是以題組為單位進行出題；使用者指定一主題知識，系統根據此一主題知識範圍進行5-10題之選擇題自動生成，題目之間也應該彼此考量。目前Retreive-and-Generatepipeline的做法，我們使用使用者給定的關鍵字為答案，進行題目之生成。然而比較好的做法應為將使用者給定的關鍵字集合為知識範圍之指定，系統根據該範圍自動挑選重要概念關鍵字進行答案挑選，而後再進行選擇題題組之生成。針對上述之研究挑戰，於本研究規劃中我們擬定下列八項研究議題進行探討。•研究項目ㄧ：基於語言模型與知識圖譜之Text2Text干擾選項生成模型(DistractorGenerationbasedonText2TextLanguageModelandKnowledgeGraph)Figure2:Text2Text干擾選項生成架構Figure3:基於Masked-LM之反事實干擾選項現有之Distractor生成研究多是基於CandicateGenerating-and-Ranking架構，先透過知識庫(如Wordnet)或者透過MaskedLM的方式取得候選字後，再適當進行候選詞特徵排序以為最終干擾選項。對此我們有不同之想法，我們預計嘗試把干擾選項生成設定為一個Text2Text任務；如圖2所例，預期的輸入為題幹Q與答案A之串接字串，透過ModelEncoder與Decoder轉化輸出為一連串之干擾選項字串“重組DNA技術[SEP]電腦斷層掃描[SEP]組織培養”。我們透過語言生成模型直接將輸入對比轉換為輸出。此外，透過此一Text2Text方式，生成句子等級的干擾選項也將為可能。再者，近年來語言模型研究趨勢除了將模型參數量與訓練資料持續增加外，引入知識圖譜來強化生成模型於常識理解與推理能力也成為研究社群的關注所在[5][13]。也因此，探索整合知識圖譜於選擇題自動生成也因此成為我們規劃之研究項目。希冀藉由整合知識圖譜來提升干擾選項之誘導性與題幹生成之正確性。•研究項目二：基於Masked-LM之反事實干擾選項生成研究(Counter-FactualSentenceGenerationbasedonMasked-LM)如同前言所述，“下列何者正確/錯誤”題目為常見之類型選擇題題型。對此，我們預計嘗試以下之作法。首先，我們假定可取得正確敘述之語句S。我們先針對語句S中的字詞進行命名實體辨識(NamedEntityRecognition,NER)，其後再利用語言模型針對所取得之命名實體進行WholeWordMasking(WWM)，填入機率較低之填空結果，藉此生成反事實描述之語句。如Figure4所例，我們將所給定之事實描述，藉由NER方式挑選”葡萄糖”一次進行Masked，再藉由語言模型推測該Masked字詞，並挑選機率相對較低之預測填入。藉此創造出反事實之陳述句。然而，如何確保不挑選到同義詞？要進行多少個字詞之Masked？如何自動從主題文章中挑選事實陳述句以進行改寫？等等•研究項目三：基於知識圖譜強化之反事實干擾選項生成(Counter-FactualSentenceGen-erationbasedonKnowledge-GraphAugmentedLanguageModel)針對反事實干擾選項生成之議題，除了研究項目二之Masked-LM作法外，我們另一個構想為利用知識圖譜來強化Counter-Factual語句之生成。如Figure4所示，我們基本想法為先萃取與題幹相關聯之知識圖譜子圖(LocalSubgraph)並取得知識三元組，以此輸入至語言模型中，訓練語言模型反向改寫三元組語句，以此為反事實干擾選項。然而，此模式之訓練資料該如何取得？如何從知識圖譜上萃取子圖，並生成合適三元組？如何引入GraphNeuralNetwork來萃取結構？為此研究項目需進行探討之工作內容。•研究項目四：基於對比式學習之干擾選項生成模型強化(DistractorGenerationModelbasedonContrastiveLearning)我們過去的研究經驗[8]與相關研究[35]皆指出當使用Generative語言模型來生成句子Figure4:基於知識圖譜強化之反事實干擾選項生成構想層級之輸出時，模型經常會產生語意之相同，但字面的結果。如Table3所示，選項A跟C雖然字面敘述上不同，但實際上都是描述同樣的意思。生成重複選項將導致選擇副交感神經興奮會導致使心跳加速，血壓上升Table3:模型生成的誤導選項重複出現的範例對此，我們預計探索對比式學習(ContrastiveLearning)[7][35]來提升生成品質。對比式學習於自然語言之應用中主要想法為透過選取PositiveSample以及NegativSample使得模型去學習到生成句子跟同義句子的相同以及跟異類句子的不同之處，這樣我們可以使用重複選項當作我們的NegativeSample，然後資料集中的干擾選項當作PostiveSample，期望模型可以學習避免生成出重複的誤導選項。因此，我們預計探索對比式•研究項目五：使用Open-DomainQuestionAnswering錯誤結果之干擾選項挑選研究(Us-ingIncorrectAnswersfromODQAasDistractors)而除了利用生成模型產生Distractors外，我們發現利用Open-DomainQuestionAnswer-ing(ODQA)模型所產生的錯誤結果是一個值得探討的干擾選項的取得方式。具體而言，現有的ODQA模型主要包含一個Retriever與一個Reader，Retriever負責檢索相關的文章，而Reader利用檢索出來的文章利用閱讀理解模型(ReadingComprehensionModel)來找尋答案片段。由於ODQA並非完美，因此也會有錯誤結果之回報。但我們發現錯誤結果與正確答案並非天差地遠，閱讀理解模型所回報之結果通常也與答案接以圖5為例，當我們下達Q：「下列何者為目前世界上應用最廣的殺草劑，其具備有生長素之特性，常用於闊葉雜草防治，亦為選擇性殺草劑的一種？」，ODQA系統將取得圖5中右半部之答案：「萌前除草劑、2,4-D、非選擇性除草劑、10殺丹粒劑」。其中，2,4-D為正確答案，餘者為錯誤答案。然而我們覺得這些錯誤答案，可以當作Q之干擾選項來使用。有別於基於生成模型研究項目，利用ODQA模型之錯誤答案來當作干擾選項應可提供不同思路進行干擾選項生成。這樣的想法乍看可行，但仍有若干技術點需進行探討。諸如，如何控制ODQA模型之錯誤？我們需要在Reader部分弱化Reader的能力，或者弱化Retriever能力，來製造ODQA的錯誤？如何從錯誤回報結果挑選干擾選項？這些皆是尚待深入研究之議題。也因此觸發本研究項目之規劃。Figure5:利用ODQA模型之錯誤答案來當作Distractors•研究項目六：基於NodeCentrality考量主題涵蓋性之選擇題題組生成(MCQGroupGenerationconsideringTopicCoveragebasedGraphNodeCentrality)上述所規劃的選擇題生成做法，主要針對單題進行生成。若要產生多題/題組考題，就是重複相同流程，獨立產生個別考題後，整合為一題組。然而，題組生成做法應當考慮題與題之間之關聯性與互補性。因此，我們認為比較好的做法應當為根據使用者所指定的主題，透過(1)知識圖譜子圖萃取來涵蓋所指定主題之內容與(2)透過主題重要概念萃取：藉由知識圖譜上NodeCentrality方式來萃取關鍵字，以為題幹之生成。此外，如何根據主題互補性，來挑選個別生成之題組，用以組成選擇題題組也是需進行•研究項目七：考量題幹可用性與干擾選項間關聯之Retrieve-and-Generate架構優化(Op-timizingRetrieve-and-GenerateFrameworkconsideringStemUtilityandDistractorCorrela-tion)本研究項目著眼於優化現有Retrieve-and-Generate架構設定。Retrieve-and-Generate架構雖然簡單，但我們可以發現其於單字等級的干擾選項生成已有一定的可用性。我們預計可優化之處為：(1)我們發現引用現有系統於題幹生成時，會有題幹生成結果不佳的狀況。對此，我們發現其中的一個原因為Retriever所檢索之文章品質不佳(文章過短或者與主題不相關)，導致不良之題幹生成。因此如何改善檢索機制來挑選較佳的文章用以進行生成，便是值得探討的課題。再者，題幹生成不佳也有部分原因是來自於問句生成模型本身之生成錯誤，如何有效過濾生成不佳的題幹也因此是一個可需進行研究之項目。(2)如同前言所述，干擾選項之挑選應該考量彼此間之關聯，而非獨立挑選。如何納入此觀點來進行干擾選項之生成也因此成為預計關注之議題。又再者，我們基於word2vec方式之字詞等級干擾選項挑選是否可以引入語言模型之詞向量來求取•研究項目八：生成難度可控之選擇題生成研究Difficulty-ControlledMCQGeneration發展自動化生成考題之可能願景為：針對個人化學習自對應生成難度適中之測驗，以增加學習與檢測之效率。因此，如何針對現有技術探討生成結果難度控制也因此成為一可探索之研究議題。由於我們的基礎架構為基於文章檢索來取得包含使用者所給定的關鍵字之文章，並使用該文章與關鍵字為問題生成模型之輸入，來生成選擇題問句。我們若挑選較為冷門之文章進行生成，則預期可以生成較為困難之題幹。而就干擾選項而言，若我們基於研究項目五「使用Open-DomainQuestionAnswering錯誤結果之干擾選項挑選研究」，利用ODQA來取得干擾選項之作法。我們可藉由調整輸入給閱讀理解模型之文章來產生難度較高的錯誤。具體而言，給定一有包含正確答案A之文章C，我們先將答案片段A於文章D中刪除，而後將其輸入至閱讀理解模型，藉此產生錯誤之閱讀理解結果，以這樣的錯誤結果當干擾選項預期將有較高之誘導能力。相類似的想法也可以套用為挑選Retriever認為相關度比較不高的文章以為閱讀理解模型之輸入。此外，就生成題目之廣度與深度而言，我們可藉由控制取得之文章來調配題組之廣度與深度。例如，若我們以單篇或少數篇文章進行生成，我們所生成的主題題組針對測驗內容而言，應該可產生知識性較為深入之試題。而若針對較多篇文章，則應可產生範圍較廣之測驗試題。上述之研究議題，乍看可行但仍有許多細節需進行探索，如上述研究背景目的所述，本研究計畫擬探索基於現有Retrieve-and-GeneratePipeline之選擇題自動生成架構進行強化。我們以三年為期，規劃在此目標下擬定八項關鍵技術進行研究探討。如Figure6所示，八個工作項目又可區分為三項主軸：「選擇題生成品質提升」、「反事實選項生成」與「群組生成與難度控制」。其中，於「選擇題生成品質提升」研究主軸中，我們探索利用語言模型與知識圖譜架構來生成，將選擇題選項生成問題設定成為一Text2Text任務，輸入為問題題幹與正確答案所串接之TextSequence，而輸出為所生成之distractor串接之TextSequence。此外，我們也嘗試引入對比式學習來改善Text2Text生成模式之品質。我們也預期規劃探索利用ODQA方式來取得干擾選項。而於「反事實選項生成」研究主軸中，我們探索反事實陳述句之生成。我們預計嘗試基於MaskedLM取代重要名詞之反事實語句生成作法與基於知識圖譜知識三元組萃取改寫之作法。於第三個研究主軸中，我們則嘗試針對題組生成與難度可控制之干擾選項生成進行研究。MultipleChoice,Multi-此資料集特色在於為Multi-hop形式，答案需從複數文檔中推理而成；資料來自維基百科以及PubMed，PubMed為生物醫學論文的搜尋引擎，在兩個來源各自提供51,318(Wiki)和2,508(PubMed)筆問題。資料來自國高中英文考試，提供超過10萬筆問題以及2.8萬篇文本，題型上為英文閱讀測驗，會需要閱讀故事或對話紀錄，而針對該文本會有複數題目，每個題目此資料集的挑戰在於，內容來自於7個不同領域(新聞、小學自然科學、旅遊、小說故事等等)，為文本與題目提供多樣性，總計約1萬筆問題與800篇文本。MultipleChoice,Open-為Open-domain形式，並不提供文本，只有問題與答案；包含12,247筆問題、一個正確答案和四個干擾選項；題目類型為知識與常識題，來自於ConceptNet與同時包含兩種形式資料，填空題與Open-domain；填空題形式命名為QUASAR-S，其中資料來源自StackOverflow，提供約37K筆問題。Open-domain形式命名為QUASAR-T，提供約43K筆問題。來自StanfordUniversity的閱讀理解資料集，包含了超過100K筆問題和536篇對應的維基百科文本；延續了SQuAD1.1的題目外，[31]另外加入約50K筆無答案的題目，進一步測試機器能否在無對應答案時回答否，而MedQA是基於美國醫師執照考試(USMLE)所產生的資料集，其中問題皆為較為困難的專業醫學相關的知識，且包含了英文、簡體中文、繁體中文分別有約12K、34K、14K個問題集，這種專業的知識能使機器針對單一領域產生更具參考價值的distractor，目前此任務排名上最佳為ValentinLiévin團隊做出的Codex5-shot訓練機器學習模型進行閱讀理解(MachineReadingComprehensionTask)為最常見的深度學習自然語言任務。諸如，SQuAD[32]、RACE[21]、DROP[10]與HotPotQA[41]等資料集，皆是測驗/訓練機器學習模型於答案找尋、邏輯推理與摘要之能力。其中，許多機器閱讀理解任務採用多選題方式進行測驗。選擇題的自動答題與選擇題的自動生成為一體兩面之應用。因此具有選擇題樣態的資料集為我們選擇題自動生成研究之根本資料集，我們將目前所探知之相選擇題與干擾選項生成之相關研究可按應用類型可區隔為英文學習測驗與知識學習測驗。干擾選項生成研究於英文學習測驗應用，主要以自動生成閱讀理解與克漏字類型的考題為目標。閱讀理解之干擾選項生成與本研究計畫之知識類型選擇題之自動出題目標不同。因此本研究回顧中，我們不做此分類每一研究細部說明。我們將此類型之相關研究按不同象限做分類：(1)干擾選項層級之差異(生成字詞等級與句子等級之差異)象限，(2)題目類型(閱讀理解與克漏字測驗)，(3)生成方式(CandidateRanking與Generative)。我們按此三象限，將目前之相關研究摘要整理於Table5中。而知識學習測驗類型的選擇題生成，則於這一兩年逐漸吸引相關研究投入。我們認為與本研究計畫較為相關之研究有[22][40][3]，以下我們針對個別文獻討論如下。首先，與我們有相同目標者為[22]，也是針對特定知識主題生成選擇題。其主要作法Table5:現有閱讀測驗與克漏字干擾選項生成研究整理表為：先由人工給定一個主題，程式使用該主題名稱的維基百科頁面作為起始點(例如給定“體育”後搜尋體育相關維基百科)，保留熱門文章並取出文章的第一段擷取摘要，由此產生大量句子。並將這些句子依單純英文文法規則生成簡單5W問句與答案，例如who問句是將第一個動詞前的名詞作為正確答案並從句子中移除，what問句是將第一個夾在動詞間的名詞移除作為正確答案並從句子中移除，移除答案後的句子再帶入問題模板生成制式問句。生出五種問句後再依英文文法的詞與詞的相關性去決定哪個問句最適合。在干擾選項生成的部分，先隨機選取其他句子所產生的答案，再藉由語言模型去排序與問題的相關性，取前三高之預測為干擾選項。但本論文並未使用問句生成模型，採取之作法也與本計畫所規劃有所不同。於論文中[40]，則探討如何讓干擾選項更加多樣且與正確答案相差不遠(以具誘導性)。對此，創造了一款監督式分類任務來找出文章中屬於關鍵句子(及對產生干擾選項生成有用的句子)，作者在訓練之前會將訓練用的干擾選項和文章中的每個句子做比對，若該句子和干擾選項相似度達到一定程度，則該句子將成為文章中的關鍵句，透過這個方式在文章中能被修改成為新的干擾選項的句子將會達到多樣性的效果，且透過相似度對比，干擾選項也不會跟原始文章相差太遠從而提升干擾選項的品質。本研究之作法可供我們借鏡參考對關鍵文章/句選擇設計，用以提升干擾選項之多樣性。而與本計畫研究項目五「使用Open-DomainQuestionAnswer錯誤結果之干擾選項」走非生成想法相似之研究為[3]。其主要想法為作者發現於干擾選項資料集中，有許多干擾選項一再被使用。促使論文作者想為新的問題檢索以重複使用干擾選項。作者提出一個Transformer-baseddistractorretrievalmodels，該模型通過考慮問題的上下文來選擇干擾選項模型包含三個部分，分別為D-SIM模型、Q-SIM模型以及DQ-SIM模型。在D-SIM模型中使用mBERT計算同一個MCQ中問題和答案以及對應的干擾選項的向量，並計算他們之間的相似度。而在Q-SIM模型中作者假設不同問題共享一個或多個干擾選項或答案，代表問題可能在語意上相關，因此他們相關的干擾選項可以用作彼此的良好候選干擾選項，因此作者將訓練資料重新排序，使這些共享至少一個干擾選項或答案的問題聚集在一起，接著將資料送進Q-SIM模型計算不同的MCQ中問題之間的相似度。在DQ-SIM模型線性結合前兩個模型，從各自的優勢中獲益，透過基於分數和基於排名的合併策略計算來自D-SIM模型和Q-SIM模型的分數，並設計演算法使得高排名之干擾選項有較大的權重。重複使用現有資料集中之干擾選項為一創新想法，但仍與我們之ODQA做法有所不同。[OpenDomainQuestionAnswering相關研究]本計畫的選擇題生成架構主要基於兩元件:(1)文章萃取與(2)問句生成。目前我們所使用的文章萃取機制為傳統之BM25檢索。而近年來由於ODQA研究領域之發展，基於深度學習之DenseRetrieval技巧也紛紛於ODQA研究領域中所提出。我們預期DenseRetrieval技術也將於我們知識類型選擇題自動出題中擔任關鍵角色。茲簡要回顧現有ODQA之研究如下。ODQA架構中Retriver之重要性不言可喻。不同層級的語意檢索也影響著ODQA的效能。於[28]研究中，作者提出一個簡單而有效的pipeline系統，研究段落和句子級別語義檢索及其對下游任務的潛在影響，作法上是先檢索段落級別的段落，然後從上一步檢索後取得的段落中再去檢索句子級別的段落。該系統在事實驗證和Multi-hopQA方面進行評估，實驗結果顯示QA任務與事實驗證皆仰來上游檢索之結果。於[23]論文中，作者在ODQA架構的retreiver端提出了一個有效率的弱監督式學習的方法(InverseClozeTask)來對retreiver進行預訓練，給予模型完全無人工標記的維基百科原始資料，藉由論文中的方法將資料切分成固定長度的段落，並隨機選取其中的一個句子當作標籤，讓模型去學習如何從給定的句子去資料庫中找尋與之最相關的文章。在實驗數據中此方法訓練出的ODQA模型分數高於傳統的BM25檢索方法。而於DPR[16]論文中，作者為解決傳統BM25無法檢索語意相似段落的問題，提出了基於深度學習模型之DensePassageRetrieval，並提出使用in-batchnegative的技術，在密集向量的檢索取得極佳的效果改進。我們也預計引入中文DRP模型來提升目前題幹生成所需之而於GAR[27]研究中，作者們探討傳統基於BM25的檢索無法檢索沒有單詞重疊或語意相似的段落的問題，而基於密集向量的檢索方式在計算上又過於複雜，且會有資訊損失的問題。該論文透過預訓練語言模型自動生成與查詢語句相關的內容，可增加查詢語句的資訊量，隨後透過BM25進行檢索，該方法比起傳統BM25能檢索到更精確的段落，且執行效率也快於基於密集向量的檢索方式。透過GAR中的PLM可對查詢語句的資訊進行補充，原論文中是補充答案、相關段落及段落標題，可基於該手法讓PLM生成問題與原始答案的關聯，對這些關聯進行組合或刪除即可檢索與原答案有相似關係的查詢語句，可檢索到適合製作干於[19]，作者觀察到基於語言模型之檢索技術雖然效果很好，但比以往的方法需要極高的計算成本，特別是在計算每個query與document間的relevancescore時。本篇論文作者提出了ColBERT，引入lateinteraction架構，對query與document進行獨立的encoding，於模型輸出最終階段才進行相似度運算。而[18]中，則使用近一步基於ColBERT當基礎模型來進行retreiver端的訓練，使用了Relevance-guidedSupervision(RGS)的方法來訓練retreiver的模型。相較於傳統的retreiver訓練資料，傳統方法需人工標註的訓練資料，但作者認為人工標住資料往往存在偏見，進而影響模型的效能，所以在論文中提出了用訓練好的模型來找尋最適合的訓練資料，並把這些資料用於訓練下一個模型，使用這個方式來進行後續的訓練，最後得出最佳的模型。此方法訓練出的模型分數明顯高於目前opendomain的其他方式。在找尋訓練資料時，模型會先搜尋前k筆資料，並把前20筆資料中含有shortanswer的資料當成possitiveexample，而前20筆以外剩餘的資料中不含有shortanswer的資料當成negativeexample，此方法尋找的負面訓練資料交由reader進行預測能得到與正確答案相關的錯誤答案，如能改良ranking的方式，應用於干擾選項生成，預期將能提升干擾選項之品質。[ContrastiveLearning於自然語言生成相關研究]對比式學習始見於電腦視覺領域，其於影像應用上有著顯著的效果提升。因此，許多自然語言處理之研究者也將其引用至自然語言生成領域，諸如[11][7][35]。於[11]研究中，作者們提出了使用ContrastiveLearning的作法來學習SentenceEmbedding，他們使用BERT作為基礎模型，透過不同的dropoutmask使模型能用同樣的輸入產生不同的SentenceEmbedding，但它們之間的距離會是近的，也就可作為PositiveSample。然後再使用其他in-batch的句子作為NegativeSample，以達到非監督式訓練。而在有訓練資料的情況下，如NLI資料集，則可以將Eentailment視為PositiveSample，Contradiction以及其他in-batch的句子視為Negative而再基於[11]研究後，於[7]研究中，作者們提出加上了Discriminator來加強SentenceEmbedding的學習效果。具體而言，對每筆輸入來說，會透過SentenceEncoder產生一個SentenceEmbedding，同時也會隨機的Masking輸入句子其中的幾個Token，並透過一個fixed的Generator來還原MaskToken，而Discriminator要試著透過沒被修改過的SentenceEmbedding的資訊去辨認修改後的句子中的哪一個Token有被修改過，整個框架會透過SentenceEmbedding的ContrastiveLearningLoss以及Discriminator的ReplacedTokenDetection而與我們預計引入對比式學習來提升句子等級的干擾選項生成效果想法接近的研究也可見於[35]。該研究指出發現了以往autoregressive模型使用BeamSearch去決定要生成的文本時，會產生語意不自然以及沒意義的重複文字，儘管後續有人使用取樣以及修改ObjectiveFunction來嘗試解決這個問題，但這個方法時常會產生語意不連貫的問題。因此作者提出了使用ContrastiveLearningLoss加上原先的MaximumLikelihoodEstimationLoss來訓練模型，並且使用ContrastiveSearch來作為decodingmethod。[Counter-FactualSentenceGeneration相關研究]於Counter-FactualGeneration研究中，[44,33]為相關研究之先驅。主要想法是使用神經網路來生成反事實敘述，這個概念背後的想法是同時使用語言模型和各種採樣的策略來達成生成反事實敘述的效果。接續之前的成果，[39]提出了Polyjuice架構，可以控制輸出的反事實敘述的類型。另外一種生成干擾選項的研究是使用干擾選項生成(DistractorGeneration)的手法，干擾選項生成(DistractorGeneration)的目標是給予了一篇文章、問題和正確答案後，模型能夠生成似是而非的干擾選項。於研究[24]中，作者最先提出使用了神經網路來生成干擾選項，而研究[8]則延續了前者的成果，把重點放在生成複數的干擾選項並提出了一個負樣本學習策略(negativetrainingstrategy)來加強干擾選項生成。但這上述兩個研究主要都著眼於閱讀理解題型中干擾選項之生成，與本研究知識類型選擇題生成目標所有不同。而在關於AbductiveInferenceGeneration的相關研究中，於[30]中作者提出了故事改寫的任務，此任務目標是要改寫一段故事的敘述並且導致產生與原先不同的結局，與顯永的論文的方式的差異為故事改寫任務是根據給定的誤導推論來生成不同的結局，在MIG任務中是給定敘述句與其前後文來生成誤導推論。[2]研究中是讓模型根據上下文來推論中間適合加入的句子，此研究主要是產生正確的推論敘述，而在MIG任務[29]主要是產生相反的誤導推論敘述。於我們先前的研究[29]中，我們提出一個自然語言的誤導推論生成任務，任務步驟是當給定了一個陳述事實的句子並給定其上下文，將其定義為(x,y,z)與誤導推論ˆy，模型會生成與該事實相反的推論句子，由此可以讓模型產生出多選題似是而非的干擾選項。然而這樣任務與我們所設定之反事實出題仍有所不同。而與我們生成Counter-Factual陳述句想法接近的研究為：[46]。於該研究中，作者探討有別於過往的是非題，將文章中原始句子作為True的問句並通過反義詞替換關鍵字生成False問句，本篇作者通過使用一種新穎的masking-and-infilling策略來生成更靈活與複雜的問句。要被mask的關鍵字找法有很多種，像是SRL結果中的predicate，句子中連接詞(”that”、”when”、”since”)之後的部分或是數字等等，來找出值得提問的句子。Infilling的作法則是以BART為基底作為生成模型，將原始句子的關鍵字mask後，提供前後句子作為BART的上下文後產生句子填入。作者為了找值得提問的句子所提出的方法亦可用來作為多選題產生題目的答案選擇方法，而infilling的想法如果使用得宜，在提供出題範圍的前後文後，也能將欲出題句子挖空讓BART模型填補後產生選項。雖然與我們目標相似，但仍與我整合知識圖譜與語言模型為近年語言模型訓練熱門關注議題。於[42]研究中，作者們提出一CounterfactualGenerationControllableCounterfactualGenerationDistractorGenerationMisleadingInferenceGeneration種能將文本和知識圖譜的資訊進行深度雙向、自我監督的預訓練的方法，透過作者的方法可以將一些特定領域的KnowledgeBase資料集結合相關文獻去訓練語言知識模型，讓語言模型和知識圖譜之間的資訊互相交換，並且在回答特定領域問答的任務上，表現優於現有的語言模型和知識圖譜增強模型。我們想透過作者提供的預訓練模型，來做多選題選項生成，透過知識圖譜所額外提供的資訊，讓在考慮生成選項時，除了題目和答案的文字外，用這些文字從知識圖譜萃取相關的知識圖譜子圖幫助生成選項有更多參考資訊。知識圖譜於問句生成之研究也見於[5][13]。於[5]研究中，作者之想法為使用知識圖譜進行問題生成任務時，通常只會取出資料與關係的三元組合，並不能完整利用知識圖譜中豐富的資訊，因此作者基於GraphNeuralNetworks提出了BidirectionalGatedGraphNeuralNetwork(BiGGNN)，將知識圖譜中的node及edge編碼成向量，使得每個node都包含了周圍其他node的資訊，並對答案的node進行標註，隨後透過RNN進行問句生成，生成過程中RNN會決定生成新文字或直接複製某個node的資訊放進問題。我們應可借鏡此研究論文的想法，可將知識圖譜及答案的資訊完整編碼，可基於此特性，在生成問題時同時考慮答案附近的node，讓模型在建立問題的同時也能根據附近的node產生干擾選項。而於[13]研究中，則提出的ISEEQ架構，主要目的是希望從簡短的敘述句中去尋找更多訊息來反問使用者是否符合關於敘述句背後未提及的情況，在使用時，使用者只需輸入簡短的敘述句，模型會先從知識圖譜擴展並增加query的關鍵字，透過這些關鍵字模型就能retrieve更多相關的文章進行ranking，在從這些文章中使用QG模型產生一些原始query中未提及但可能相關背景知識，例如當使用者說道:“我感到疲勞且渾身無力”，ISEEQ能回覆:“請問病徵持續了多久?”，或是“你最近14天有出國過嗎?”這類可能與其他知識相關的問句。這種方式最大的貢獻及是讓機器學習到”反問”的能力，而非一味的透過query生成出結果。這樣的想法，或許可引用來提升題幹生成的精確度，舉例來說:有位教師想出關於日治時期的歷史題目，我們就能先藉由ISEEQ來詢問是要出與日治時期戰爭相關的題目或是與日治時期建設相關的題目，以此來取得更精確的題幹生成需求。[1]J.Araki,D.Rajagopal,S.Sankaranarayanan,S.Holm,Y.Yamakawa,andT.Mitamura.Gen-eratingquestionsandmultiple-choiceanswersusingsemanticanalysisoftexts.InProceedingsofCOLING2016,the26thInternationalConferenceonComputationalLinguistics:TechnicalPapers,pages1125–1136,2016.[2]C.Bhagavatula,R.L.Bras,C.Malaviya,K.Sakaguchi,A.Holtzman,H.Rashkin,D.Downey,S.W.-t.Yih,andY.Choi.Abductivecommonsensereasoning.arXivpreprintarXiv:1908.05739,[3]S.K.Bitew,A.Hadifar,L.Sterckx,J.Deleu,C.Develder,andT.Demeester.Learningtoreusedistractorstosupportmultiplechoicequestiongenerationineducation.IEEETransactionsonLearningTechnologies,2022.[4]Y.-H.ChanandY.-C.Fan.Arecurrentbert-basedmodelforquestiongeneration.InProceedingsofthe2ndWorkshoponMachineReadingforQuestionAnswering,pages154–162,2019.[5]Y.Chen,L.Wu,andM.J.Zaki.Towardsubgraphguidedknowledgegraphquestiongenerationwithgraphneuralnetworks.arXivpreprintarXiv:2004.06015,2020.[6]S.-H.Chiang,S.-C.Wang,andY.-C.Fan.Cdgp:Automaticclozedistractorgenerationbasedonpre-trainedlanugagemodel.InFindingsoftheAssociationforComputationalLinguistics:[7]Y.-S.Chuang,R.Dangovski,H.Luo,Y.Zhang,S.Chang,M.Soljac￿ic￿,S.-W.Li,W.-t.Yih,Y.Kim,andJ.Glass.Diffcse:Difference-basedcontrastivelearningforsentenceembeddings.arXivpreprintarXiv:2204.10298,2022.[8]H.-L.Chung,Y.-H.Chan,andY.-C.Fan.Abert-baseddistractorgenerationschemewithmulti-taskingandnegativeanswertrainingstrategies.InFindingsoftheAssociationforComputationalLinguistics:EMNLP2020,pages4390–4400,2020.[9]B.Dhingra,K.Mazaitis,andW.W.Cohen.Quasar:Datasetsforquestionansweringbysearchandreading.arXivpreprintarXiv:1707.03904,2017.[10]D.Dua,Y.Wang,P.Dasigi,G.Stanovsky,S.Singh,andM.Gardner.ingcomprehensionbenchmarkrequiringdiscretereasoningoverparagraphs.arXiv:1903.00161,2019.[11]T.Gao,X.Yao,andD.Chen.Simcse:Simplecontrastivelearningofsentenceembeddings.arXivpreprintarXiv:2104.08821,2021.[12]Y.Gao,L.Bing,P.Li,I.King,andM.R.Lyu.Generatingdistractorsforreadingcomprehen-sionquestionsfromrealexaminations.InProceedingsoftheAAAIConferenceonArtificialIntelligence,volume33,pages6423–6430,2019.[13]M.Gaur,K.Gunaratna,V.Srinivasan,andH.Jin.Iseeq:Informationseekingquestiongenerationusingdynamicmeta-informationretrievalandknowledgegraphs.InProceedingsoftheAAAIConferenceonArtificialIntelligence,volume36,pages10672–10680,2022.[14]Q.Guo,C.Kulkarni,A.Kittur,J.P.Bigham,andE.Brunskill.Questimator:Generatingknowl-edgeassessmentsforarbitrarytopics.InIJCAI-16:ProceedingsoftheAAAITwenty-FifthInternationalJointConferenceonArtificialIntelligence,2016.[15]D.Jin,E.Pan,N.Oufattole,W.-H.Weng,H.Fang,andP.Szolovits.Whatdiseasedoesthispa-tienthave?alarge-scaleopendomainquestionansweringdatasetfrommedicalexams.AppliedSciences,11(14):6421,2021.[16]V.Karpukhin,B.Og￿uz,S.Min,P.Lewis,L.Wu,S.Edunov,D.Chen,andW.-t.Yih.Densepassageretrievalforopen-domainquestionanswering.arXivpreprintarXiv:2004.04906,2020.[17]D.Khashabi,S.Chaturvedi,M.Roth,S.Upadhyay,andD.Roth.Lookingbeyondthesurface:Achallengesetforreadingcomprehensionovermultiplesentences.InProceedingsofthe2018ConferenceoftheNorthAmericanChapteroftheAssociationforComputationalLinguistics:HumanLanguageTechnologies,Volume1(LongPapers),pages252–262,2018.[18]O.Khattab,C.Potts,andM.Zaharia.Relevance-guidedsupervisionforopenqawithcolbert.TransactionsoftheAssociationforComputationalLinguistics,9:929–944,2021.[19]O.KhattabandM.Zaharia.Colbert:Efficientandeffectivepassagesearchviacontextualizedlateinteractionoverbert.InProceedingsofthe43rdInternationalACMSIGIRconferenceonresearchanddevelopmentinInformationRetrieval,pages39–48,2020.[20]G.Kumar,R.E.Banchs,andL.F.D’Haro.Revup:Automaticgap-fillquestiongenerationfromeducationaltexts.InProceedingsoftheTenthWorkshoponInnovativeUseofNLPforBuildingEducationalApplications,pages154–161,2015.[21]G.Lai,Q.Xie,H.Liu,Y.Yang,andE.Hovy.Race:Large-scalereadingcomprehensiondatasetfromexaminations.arXivpreprintarXiv:1704.04683,2017.[22]G.LeBerre,C.Cerisara,P.Langlais,andG.Lapalme.Unsupervisedmultiple-choicequestiongenerationforout-of-domainq&afine-tuning.InProceedingsofthe60thAnnualMeetingoftheAssociationforComputationalLinguistics(Volume2:ShortPapers),pages732–738,2022.[23]K.Lee,M.-W.Chang,andK.Toutanova.Latentretrievalforweaklysupervisedopendomainquestionanswering.arXivpreprintarXiv:1906.00300,2019.[24]C.Liang,X.Yang,N.Dave,D.Wham,B.Pursel,andC.L.Giles.Distractorgenerationformultiplechoicequestionsusinglearningtorank.InProceedingsofthethirteenthworkshoponinnovativeuseofNLPforbuildingeducationalapplications,pages284–290,2018.[25]C.Liang,X.Yang,D.Wham,B.Pursel,R.Passonneaur,andC.L.Giles.Distractorgenera-tionwithgenerativeadversarialnetsforautomaticallycreatingfill-in-the-blankquestions.InProceedingsoftheKnowledgeCaptureConference,pages1–4,2017.[26]V.Liévin,C.E.Hother,andO.Winther.Canlargelanguagemodelsreasonaboutmedicalquestions?arXivpreprintarXiv:2207.08143,2022.[27]Y.Mao,P.He,X.Liu,Y.Shen,J.Gao,J.Han,andW.Chen.Generation-augmentedretrievalforopen-domainquestionanswering.arXivpreprintarXiv:2009.08553,2020.[28]Y.Nie,S.Wang,andM.Bansal.Revealingtheimportanceofsemanticretrievalformachinereadingatscale.arXivpreprintarXiv:1909.08041,2019.[29]H.-Y.Peng,H.-L.Chung,Y.-H.Chan,andY.-C.Fan.Misleadinginferencegenerationviaproxi-malpolicyoptimization.InPacific-AsiaConferenceonKnowledgeDiscoveryandDataMining,pages497–509.Springer,2022.[30]L.Qin,A.Bosselut,A.Holtzman,C.Bhagavatula,E.Clark,andY.Choi.Counterfactualstoryreasoningandgeneration.arXivpreprintarXiv:1909.04076,2019.[31]P.Rajpurkar,R.Jia,andP.Liang.Knowwhatyoudon’tknow:Unanswerablequestionsforsquad.arXivpreprintarXiv:1806.03822,2018.[32]P.Rajpurkar,J.Zhang,K.Lopyrev,andP.Liang.Squad:100,000+questionsformachinecomprehensionoftext.arXivpreprintarXiv:1606.05250,2016.[33]S.Ren,Y.Deng,K.He,andW.Che.Generatingnaturallanguageadversarialexamplesthroughprobabilityweightedwordsaliency.InProceedingsofthe57thannualmeetingoftheassociationforcomputationallinguistics,pages1085–1097,2019.[34]S.RenandK.Q.Zhu.Knowledge-drivendistractorgenerationforcloze-stylemultiplechoicequestions.InProceedingsoftheAAAIConferenceonArtificialIntelligence,volume35,pages[35]Y.Su,T.Lan,Y.Wang,D.Yogatama,L.Kong,andN.Collier.Acontrastiveframeworkforneuraltextgeneration.arXivpreprintarXiv:2202.06417,2022.[36]A.Talmor,J.Herzig,N.Lourie,andJ.Berant.Commonsenseqa:Aquestionansweringchallengetargetingcommonsenseknowledge.arXivpreprintarXiv:1811.00937,2018.[37]J.Welbl,N.F.Liu,andM.Gardner.Crowdsourcingmultiplechoicesciencequestions.arXivpreprintarXiv:1707.06209,2017.[38]J.Welbl,P.Stenetorp,andS.Riedel.Constructingdatasetsformulti-hopreadingcomprehensionacrossdocuments.TransactionsoftheAssociationforComputationalLinguistics,6:287–302,[39]T.Wu,M.T.Ribeiro,J.Heer,andD.S.Weld.Polyjuice:Automated,general-purposecounter-factualgeneration.arXivpreprintarXiv:2101.00288,1(2),2021.[40]J.Xie,N.Peng,Y.Cai,T.Wang,andQ.Huang.Diversedistractorgenerationforconstruct-inghigh-qualitymultiplechoicequestions.IEEE/ACMTransactionsonAudio,Speech,andLanguageProcessing,30:280–291,2021.[41]Z.Yang,P.Qi,S.Zhang,Y.Bengio,W.W.Cohen,R.Salakhutdinov,andC.D.Manning.Hotpotqa:Adatasetfordiverse,explainablemulti-hopquestionanswering.arXiv:1809.09600,2018.[42]M.Yasunaga,A.Bosselut,H.Ren,X.Zhang,C.D.Manning,P.Liang,andJ.Leskovec.Deepbidirectionallanguage-knowledgegraphpretraining.arXivpreprintarXiv:2210.09338,2022.[43]C.Y.Yeung,J.S.Lee,andB.K.Tsou.Difficulty-awaredistractorgenerationforgap-fillitems.InProceedingsoftheThe17thAnnualWorkshopoftheAustralasianLanguageTechnologyAssociation,pages159–164,2019.[44]C.Zhang,Y.Sun,H.Chen,andJ.Wang.Generatingadequatedistractorsformultiple-choicequestions.arXivpreprintarXiv:2010.12658,2020.[45]X.Zhou,S.Luo,andY.Wu.Co-attentionhierarchicalnetwork:Generatingcoherentlongdistractorsforreadingcomprehension.InProceedingsoftheAAAIConferenceonArtificialIntelligence,volume34,pages9725–9732,2020.[46]B.Zou,P.Li,L.Pan,andA.Aw.Automatictrue/falsequestiongenerationforeducationalpur-pose.InProceedingsofthe17thWorkshoponInnovativeUseofNLPforBuildingEducationalApplications(BEA2022),pages61–70,2022.於本研究計畫中，我們目標基於自然語言生成技術進行選擇題自動出題研究。我們預期的展示情境為：給定一知識範圍(如農業文集)，我們可於系統直接輸入一組主題關鍵詞，而後自動生成關於該主題相關之選擇題題組(含字詞等級、句子等級、挑選是非類型之選擇題)。然而，如同前言背景中所言，現有技術離目標仍有許多待強化之處，對此我們擬定三大出題目標：字詞等級干擾選項、句子等級干擾選項與反事實干擾選項進行研究。針對此三大目標，我們擬定(1)基於知識圖譜之強化與(2)基於生成語言模型與對比學習之強化三個核心技術進行探討。研究技術項目與項目間之關係與進行規劃如Figure7所示。於第一年度中，我們規劃「考量題幹可用性與干擾選項間關聯之Retrieve-and-Generate架構優化」、「基於語言模型與知識圖譜之Text2Text干擾選項生成」與「基於Masked-LM之反事實干擾選項生成研究」進行研究。「考量題幹可用性與干擾選項間關聯之Retrieve-and-Generate架構優化」著眼於優化目前https://mcq-demo.nlpnchu.org之WordLevel之選擇題生成系統。希冀自動過濾相似或語意不正確題幹，並且考量干擾選項彼此間之關聯進行最終干擾選項集合之綜合挑選。「基於生成語言模型之Text2Text干擾選項生成模型」則考量Text2Text模式利用生成模型直接生成干擾選項序列，藉此嘗試生成句子等級之干擾選項。並進一步地探索結合知識圖譜與語言模型之干擾選項生成，希冀透過知識圖譜之整合來生成更有誘導性且合理之字詞等級干擾選項。此外，於「基於Masked-LM之反事實干擾選項生成研究」項目中，我們預計著手挑戰反事實干擾選項生成，我們嘗試利用透過NER擷取給定事實陳述句中之關鍵字並Masked-LM該關鍵字來填入機率較低之字詞，以為反事實語句生成之作法。我們預期第一年度工作完成後，將可優化現有單詞等級干擾選項之選擇題自動生成系統，並於第二年度中，我們則規劃「基於知識圖譜強化之反事實干擾選項生成」、「基於對比式學習之干擾選項生成模型強化」與「利用Open-DomainQuestionAnswering錯誤結果之干擾選項挑選」。於此年度中，進一步嘗試利用知識圖譜萃取三元組知識後進行反事實語句之生成，基本想法為透過萃取三元組後再進一步輸入語言模型，並希望語言模型適當地進行反事實語句的改寫。此一部分預計使用第一年度之「基於Masked-LM之反事實干擾選項生成研究」之基礎以為訓練資料。對於字詞等級的干擾選項，我們則探索利用Open-DomainQuestionAnswering模型所錯誤回報之答案當作干擾選項。我們希望透過此不同之想法來取得更多樣的干擾選項。再者，如前言所述，生成模型生成序列時常出現生成結果重複的狀況，對此我們嘗試利用對比式來改善生成模型這方面的缺點，以優化干擾選項之生成。第二年度工作項目完成後，我們可初步提供反事實干擾選項之選擇題生成、優化後之句子等級干擾選項生成與基於ODQA方式取得之字詞等級干擾選項生成。於第三年度中，我們則規劃「基於NodeCentrality考量主題涵蓋性之選擇題題組生成」與「生成難度可控之干擾選項生成研究」。如同前言所述，一個理想的選擇題出題系統應該考慮題組生成，整體題目之主題涵蓋性，對此我們也將嘗試基於知識圖譜涵蓋的方式考慮節點centrality重要度方式來進行題組之生成。同時我們也將嘗試題目難易度之控制生成研究。我們的想法是當自動生成成為可能後未來的出題應該是個人化模式；針對個人的學習狀況進行測驗，因此我們規劃「個人化與生成難度可控之干擾選項生成研究」進行探討。第三年度，我們也預計投入人力進行系統的實作開發與整合。以下茲針對擬訂之研究議題其問題挑戰與1.基於語言模型與知識圖譜之Text2Text干擾選項生成模型近年來，語言模型除了使用CandicateGeneratingandRanking架構的MaskedLM之外，使用Encoder-Decoder架構的Text2Text模型於生成任務上也有很好的成果，因此我們的想法是把干擾選項生成Model視為一個Text2Text任務，嘗試讓模型直接生成干擾選項。且使用Text2Text模型生成Distractor有別於用MaskedLM生成Distractor，我們希望模型能生成的Distractor有以下特性(1)非單詞之Distractor:目前在Distractor生成上多為單詞生成，要生成一段敘述/句子的干擾選項還沒有很好的做法。(2)特定domain之MCQ:目前的MCQ多為英文測驗，若想針對特定領域出選擇題，使用MaskedLM生成的Distractor會因語料庫限制而為達到上述的兩種目標需求，我們設Q為輸入之問句，A為輸入問句之答案，並將Q與A字串接字串當作輸入，訓練模型輸出一連串問句Q的干擾選項字串D=(d1,d2,d3)。模型架構如圖2所示，我們將「哪一種技術最適用於嚴重特殊傳染性肺炎之病原快速篩檢?」與「聚合￿連鎖反應」接在一起輸入至Text2Text模型，而模型生成了「重組DNA技術」、「電腦斷層掃描」與「組織培養」三個干擾選項，三個選項皆為醫療檢測方式且都不是單詞，符合如前言所述，我們也預計探索使用知識圖譜來強化生成干擾選項的能力。如Figure8的圖例，我們可以使用語言模型結合醫療知識圖譜，利用給定的文本從知識圖譜中萃取有包含這些topicentities以及相鄰的nodeentities的子圖，再分別將文本與子圖輸入至語言模型和GraphNeuralNetwork(GNN)，讓模型學習找出關鍵的資訊與強化network中重要entitynodes跟edges的權重，藉由GNN的額外資訊與語言模型能力生成更好的選項。Figure8為我們規劃做法的概觀。主要分兩階段進行：(1)關鍵字子圖與GNNEmbedding萃取與(2)GNNEmbedding與LanugageEmbedding間之FusionEncoder生成。具體而言，第一個階段是將從知識圖譜裡萃取相關的知識圖譜子圖。我們一開始先使用NERModel抓取題幹中的entities，再透過這些entities透過知識圖譜去尋找entitynodes以及所有跟entitynodes相鄰3-hop內的bridgenodes，將檢索出來的結果當成知識圖譜子圖，並在後續第二階段提供以Figure9的圖例，我們先將問題透過NERModel斷詞得到”白血病”、”發病”、”器官”與”發病原因”當作entities，再透過這些entities當作關鍵字透過Retriever從知識圖譜萃取Figure8:結合知識圖譜與語言模型生成之構想架構圖與這些詞相關的node和edge，如利用”白血病”與”器官”能找到”肝臟”、”骨髓”、”肺部”等等entities，我們能利用這些entities來當作後續的生成選項。第二個階段則將文本與第一階段得到的知識圖譜子圖分別輸入至語言模型與GNN進行訓練，訓練目標為將Input的[MASK]Token還原來產生干擾選項，先將語言模型的Input額外新增[MASK]Token，經過N層LMLayer做self-attention後，透過FusionEncoder再結合知識圖譜的資訊與原本的文本去生成選項，透過以上的手法，來強化干擾選項之生成。2.考量題幹可用性與干擾選項間關聯之Retrieve-and-Generate架構優化如同研究目的所述，本研究項目著眼於優化現有Retrieve-and-Generate生成架構。我們•文章檢索器過濾:在Retrieving-and-Generating方法中Retriever會檢索回傳含有主題字詞的文章，但有部分文章並不適合當作問句生成模型的輸入，那些文章含有太多的雜訊，我們想嘗試在這步驟先過濾一些文章。我們的想法是利用詞向量中的相似詞再次檢索Retriever回傳的文章，把過於不相關的文章濾掉，減少問句生成模型輸出過於複•生成題幹品質自動過濾：問句生成模型輸出的問句，會再交由使用者挑選，但為了更好的效果，我們想嘗試利用語言模型做初步的篩選，減少人工挑選時間。我們想嘗試利用ODQA模型來過濾問句，將問句生成模型輸出的問句輸入至問答模型後，根據問答模型的回答結果與信心分數來決定該問句是否合適當作MCQ。如Figure10所例，我們將生成之問句再次投入ODQA模型，透過答案選項之信心度來為問句生成過濾之基Figure10:Unanswerablequestionsolution•考量干擾選項間關聯之干擾選項挑選:如前言所述，一個可以提升選擇題生成品質￿考量干擾選項彼此之間之難易度。而在Retrieving-and-Generating方法中，生成干擾選項的方式為取出詞向量模型中與主題字詞相似之詞，使用相似度前三名當作干擾選項，因此有選項過於相近而造成問題太難的可能。為做改善，我們取出所有相似詞的向量與主題字詞向量計算相對距離，設定各個選項與答案的相似度範圍(距離範圍)，再從中隨機挑選相似詞，讓干擾選項彼此之間難易度有所區分以提升試題之鑑別度。•基於語言模型之干擾選項候選生成:雖然詞向量模型生成distractor過程迅速，但如果有主題字詞沒有被詞向量模型看過或有主題字詞一字多意，就無法生成適當的干擾選項。這個問題我們想嘗試使用MaskedLM來解決，我們將Retriever回傳的文章中含有主題字詞的段落記錄下來，移除主題字詞後輸入MaskedLM，再從MaskedLM輸出的候選字中選出distractor，使用這個方式生成的干擾選項不會被上述的缺點受限，也更Figure11:OptimizedRetrievingandGeneratingFramework3.基於Masked-LM之反事實干擾選項生成研究本研究項目為基於Masked-LM技巧來生成反事實干擾選項。我們需要先查找與事實相符之敘述，再修改成反事實干擾選項。我們預計之作法為：首先讓使用者先輸入考題關鍵字，再使用retriever查找相關文章，其目的為從相關文章中選取與事實相符之敘述。接著，將retriever檢索到的文章回傳給使用者，由使用者選取與事實相符之敘述。並按照以下之步驟進行Masked-LM反事實干擾選項生成。為了抽換專有名詞，我們先使用命名實體與事實相符之敘述中的所有專有名詞，以下圖為例”腎小管藉壓力差方式再吸收葡萄糖回微血管”，透過命名實體模型會萃取出專透過命名實體模型，我們會萃取出多個與事實相符之敘述中的專有名詞，我們需要選擇一個專有名詞行抽換。我們認為該專有名詞在相關文章中的重要程度越高，越高機率是考題中的重點。為了確保抽換專有名詞的品質，我們使用BM25來查看各個專有Figure12:AutoEncoder模型與AutoRegressive模型做MaskLM3.使用Maskedlanguagemodel模型進行改寫Maskedlanguagemodel，簡稱為MaskLM，該任務的會將輸入中的部分內容遮擋起來，模型會根據其他未被遮擋的內容，預測出被遮擋的內容或與其擁有相似語義的內容，現有的MaskLM模型分為兩種，分別為AutoEncoder框架的MaskLM模型與AutoRe-gressive框架的MaskLM模型，經典模型如BERT與BART。針對MaskLM任務，兩種模型進行預測的方式也有所不同，AutoEncoder框架的模型會預測出字，而AutoRe-gressive框架的模型會預測出字、詞或是句子，詳細範例如Figure12。根據抽換專有名詞的需求，我們將使用Autoregressive模型做MaskLM任務來抽換重要的專有名詞。然而模型預測出的內容眾多，有可能是原先遮擋的內容、遮擋內容的同義詞或是與遮擋內容擁有相似語義的內容。理想的情況下，模型預測的內容最高機率為原先遮擋的內容，若我們選擇原先遮擋的內容或是遮擋內容的同義詞進行抽換，會造成與事實相符之敘述未改變，便無法生成反事實干擾選項。為了避免此狀況發生，我們使用NLI任務中的換句話說(MRPC)任務來檢測其是否為干擾選項。換句話說任務主要在判斷兩句話的語意是否相同，為了避免MaskLM模型選用同義詞進行抽換，即模型生成的干擾選項仍是正確的，我們將與事實相符之敘述與生成之干擾選項作為模型的輸入，若模型認為兩句話具有相同語義，我們會讓MaskLM模型預4.基於知識圖譜強化之反事實干擾選項生成於本研究項目中，我們嘗試利用知識圖譜提供語言模型額外資訊達成反事實干擾選項生成。我們的構想如Figure4所示，包含以下步驟。首先我們會讓使用者先輸入考題內文，再將內文使用斷詞或NER模型萃取entity，利用這些entity於知識圖譜中查找相關子圖，子圖必須包含內文中所有的entitynodes以及所有跟entitynodes相鄰節點，以此為知識生成之基礎。接著我們將檢索到的子圖內的nodes與edges萃取成三元組，每筆三元組的結構是以node,edge,node組成。如同圖中所示，每一筆三元組描述一基本事實陳述，如導體可以3.將與事實相符之敘述修改為反事實干擾選項為了將與事實相符之敘述更改為反事實干擾選項，我們將前一步驟所萃取的三元組與考題內文當作作語言模型的輸入來生成反事實干擾選項。我們預計透過生成語言模型與輸入給定的問句和三元組來生成與問句情境相符合的反事實干擾選項，以FigurejQuery363013002200944701103_1676210963929為例，生成語言模型在給定關於與導體相關的題目敘述以及從知識圖譜萃取與導體相關的三元組後，模型藉由這些三元組推斷其中的關係，比如”導體可以導電->石墨是導體->筆芯是由石墨製成”，從上述的三元組推斷出”筆芯可以導電”的事實，也可以改寫成”筆芯不能導電”的反事實干擾選項。5.使用Open-DomainQuestionAnswering錯誤結果之干擾選項挑選研究如前言所述，我們預計使用ODQA模型所回答之錯誤結果來當作干擾選項(如Figure5)。以我們先前曾開發https://agri.nlpnchu.org/之農業問答系統為例，我們發現以上想法所面臨的挑於ODQA的架構下，可能會遇到ODQA正確回答所下達之問題，其回傳結果皆為可行之答案，也因此無法被使用來當干擾選項，對此，我們探索兩種作法來進行調整。–1.使用BM25，且刪除尋找出來文章的正確答案在使用BM25的情境下，因為是使用關鍵字比對，所以會找出許多跟正確答案相關的文章，我們將找到文章的正確答案刪除，這樣這個文章會變成絕對回答不出正確答案的文章，可是因為Reader對文字有一定的理解能力，並且具有關鍵字的文章中也很可能會有，他會從剩下可能的答案中，尋找一個最相近的選項，這樣就可以較容易生出干擾選項。如Figure13所示，我們將答案”2,4-D”從文章中直接刪除後，再交給Reader進行閱讀，藉此確保正確答案不會被回傳。Figure13:基於ODQA架構之Retriever弱化方法–2.使用DensePassageRetrieverDensePassageRetriever[16]，DensePassageRetriever可以將資料庫的文章轉為向量的方式表示，在搜尋的時候也將Query轉為向量，並且比對兩個向量的相似度，這樣就可以找出相似的文章，可是我們想說，可以利用DPR找尋文章相似度的這一點，理論上相似的文章跟Query算出來的相似度會是最高的，我們可以將相似度高的文章拿出來，並且查看有沒有答案出現在上面，如果沒有答案，我們就將他丟給Reader，因為我們覺得如我相似度高，就代表Retriever認為裡面有答案，可是裡面並沒有含有我們的正解，所以這篇文章就含Retriever認為是答案，但是實際上不是答案的文字，所以這時候我們將這篇文章傳給Reader，就會有比較高在我們使用baseline的情境中，有時候也會出現不相關的答案，例如在??中，問模型關於殺草劑的問題，會出現的人力除草，人力除草跟題目所問的殺草劑名稱是不相關的，所以我們需要用一個方法把他過濾掉。在這種情況下，我們如果加入正確答案當作Query一起輸入Retriever，就會增加模型找到正確passage的機率，對我們想找跟正確答案不相關但是相似度較高的Passage會產生反效果，所以我們採用的是從Reader輸出之後把他過濾掉的方法。當Reader從Retriever拿到Passage之後，我們會將正確答案去除，之後再將剩餘的答案丟入一個LanguageModel作判斷，如果他跟答案的距離過於遙遠，就放入NegativeList，如果是相關的就放入PositiveList，之後要挑相關答案的時候可以從PositiveList裡面挑選。6.基於對比式學習之干擾選項生成模型強化ContrastiveLearning的設計上需要考量如何去設定PositiveSample以及NegativeSample。在我們的目標中，因為不希望模型會重複的生成出相同意思的句子，所以我們預計使用同義的句子當作NegativeSample，而資料集中的干擾選項可以被當作PositiveSample。然而，由於我們的資料集中並沒有NagativeSample的選項，也就是同義於模型生成的選項。對此，我們的想法為透過迭代生成來產生我們需要的NegativeSample。具體而言，如Figure14所舉例，在DistractorGenerator的第一次生成中，我們會輸入Question以及Answer使模型生成出干擾選項。而在第二次迭代中，我們除了會輸入QuestionAnswerPair外，還會再加上在上一次迭代中生成的干擾選項，以提示模型先前已生成的Distractor的資訊。對於第二次迭代的輸出來說，第一次迭代輸出的Distractor，即為它的NegativeSample，因為DistractorGenerator必須要生成出與上一次迭代生成的Distractor意義相異的句子，所以它們之間的SentenceEmbedding不能太過接近。Figure14:IterationGenerater示意，在第一次的迭代中生成的Distractor會作為第二次迭代的輸入，以提示模型不要生成出類似的DistractorFigure15:對比式學習於干擾選項生成之預想架構我們所規劃之整體框架，如Figure15所示。我們使用多次迭代的方式來生成Distractor，對於第一次迭代來說，我們會從其他in-batch的data中取得Sentence當作NegativeSample。而在第二次以後的迭代來說，除了其他in-batch的Sentence以外，我們還會將此次迭代前所生成的Distractor加入我們的NegativeSample中。透過NegativeSample和模型輸出的Distractor的dotproduct可以計算出ContrastiveLoss，此外我們也會加入Distractor和Label計算的CrossEntropyLoss，來計算出我們的TotalLoss15。ContrastiveLoss定義如下:x為生成的distractor，p為資料集中的Distractor(Label)，N為NegativeSampleSet，N={n1,n2,...nj}，其中包括先前迭代生成的Distractor以及其他in-batch的選項。7.基於NodeCentrality考量主題涵蓋性之選擇題題組生成如同前言所述，本研究議題之目標為考量主題涵蓋性之選擇題題組生成。因此，我們預計根據使用者所指定的主題，按Figure16架構規劃來進行題組生成。1.透過知識圖譜子圖萃取來涵蓋所指定主題之內容：藉由知識圖譜上NodeCentrality方式來萃取關鍵字，以為題幹之生成。我們的基本想法為在知識圖譜上我們定位出與使用者輸入之主題關鍵字相關之結點後，我們可以試者套入PageRank或者其他圖形上計算NodeCentrality重要節點挑選，並以重要節點為題幹生成之主軸。2.透過上一步取得之重要答案片段後，進行問題之生成。並生成較多的問題數量以為CandidateQuestionPool，而後將問題將題組生成問題設定一成為一選擇最佳化議題(Optimization)：根據主題生成大量的單一題目，而後定義UtilityScore來計算個別題目組合後之優劣，來挑選最佳化的題組以為輸出。於本研究項目中，我們預計探索以下兩種生成難度控制方式。•調整Masked-LM比例之反事實選項難度控制：現有的Masked-LM模型會將重要的專有名詞進行遮擋，模型會透過未遮擋的內容對該專有名詞進行預測。理想的情況下，當我們僅對該專有名詞進行遮擋時，模型可成功地預測出遮擋的專有名詞或語義相近的專有名詞，這不僅是因為MaskLM模型夠強大，也是因為未遮擋的內容提供模型足夠多的資訊讓模型預測，若未遮擋的內容提供的資訊不夠多，則模型預測出的內容有可能會與遮擋的內容語義相差很多。我們根據Masked-LM模型的此特性對題目的難易度進行控制，若我們將多個專有名詞進行遮擋，再讓模型預測時，模型會因為提供的資訊不足，而預測出語義較不相近的專有名詞，即較不可能的答案。以下表為例，我們可以觀察到當[Mask]越多時，我們要判斷該事實是否正確就越形容易。[Mask]借壓力差吸收[Mask]回微血管[Mask]借壓力差吸收[Mask]回[Mask]•調整ODQA檢索文章之難度控制：由於我們的基礎架構為基於文章檢索(DocumentRetrieving)來取得包含使用者所給定的關鍵字之文章，並使用該文章與關鍵字為問題生成模型之輸入，來生成選擇題問句。我們若挑選較為冷門之文章進行生成，則預期可以生成較為困難之題幹。而就干擾選項而言，我們可藉由調整輸入給閱讀理解模型之文章來產生難度較高的錯誤。具體而言，給定一有包含正確答案A之文章C，我們先將答案片段A於文章D中刪除，而後將其輸入至閱讀理解模型，藉此產生錯誤之閱讀理解結果，以這樣的錯誤結果當Distractor預期將有較高之誘導能力。相類似的想法也可以套用為挑選Retriever認為相關度比較不高的文章以為閱讀理解模型之輸入。此外，就生成題目之廣度與深度而言，我們可藉由控制取得之文章來調配題組之廣度與深度。例如，若我們以單篇或少數篇文章進行生成，我們所生成的主題題組針對測驗內容而言，應該可產生知識性較為深入之試題。而若針對較多篇文章，則應可產生範圍較廣本研究計畫將以三年為期，共八項研究議題進行，各研究產出如上文所示，並整合各技術成為一選擇題自動生成系統，預期的展示情境為：我們可於系統直接輸入一知識範圍，系統將利用我們所開發之技術先進行自動關鍵語萃取，而後進行各種類型的選擇題出題並考量題組關聯性。我們也將彙整各研究議題中所發展之技術為一個成為公開原始碼專案，以為後續相關研究之用。並將計畫執行過程所累積的資料，公開希望能吸引更多研究者之投入。對於學術研究、國家發展及其他應用方面預期之貢獻自然語言生成已是未來深度學習領域之熱門研究重點。透過本研究計畫之投入我們將可進行先驅之探索，建立學生團隊與自然語言處理與生成之能力，並持續過去之閱讀測驗自動出題研究成果，以知識類型選擇題自動出題為主要系統開發目標為例，希冀能為國內外探索不同的自然語言生成應用服務。此外若可順利完成也可以成為一成功範例利用人工智慧輔助教師進行測驗的客製化與個人化學習。對比於題庫類型的方式，透過我們技術我們將可動態產生各式各樣不同之考題為每個人量身定做難度適中之考題。就此一觀點出發，我們預期本計畫除學術研究價值外，同時也兼具商業價值，也應可成為EdTech(教育科技)領域之一環。再者，疫情改變了學習與教學的模式。同步與非同步之線上教學已成為教學模式常態之一。線上教學帶來了好處，但也帶來了顧慮。其中，學習的專注度應是諸多顧慮中主要令人擔憂的一環。隔著螢幕學生學習是否專注？目前的教學內容學生是否已跟上進度？等等問題是線上教學學習專注度所在意的問題。對此，我們認為適時與即時地檢測學生學習成效，來提醒學生與教師目前之學習成效，應能適度緩解線上學習專注度之顧慮。因此，我們也常見於MOOCs課程於一個授課單元結束後適時地給予測驗(多數以選擇題測驗)，來確認學生學習成效。然而，人工設計試題耗費心力，一成不變全班同學皆相同試題也可能透過即時訊息(instantmessage)之分享而取得答案，造成檢測之無效。對此，我們認為隨著深度學習技術之快速演進，結合自然語言(NaturalLanguageGeneration)生成技術來進行自動出題技術已成為可能；透過自然語言生成技術自動化出題，產生多樣化的試題，來進行個人化的檢測，將有除了可以增進研讀論文、撰寫程式、架設系統以及撰寫論文的基本能力之外，參與本計畫之研究人員，將可以學習到深度學習與自然語言生成技術之基本觀念、實際應用以及相關的核心技術，在這些基礎之下，透過積極的研究與討論，將可以提升參與計畫人員思考問題與解決困難的重要能力。我們預期可獲得具體技能如下:(1)設計與建立一個深度學習模型之能力，(2)整合不同先進技術之能力，(3)自然語言生成技術之瞭解，(4)資訊檢索相關知識與軟體套件之暸解，(5)佈建深度學習模型於WebScale之能力。"}, "1676211205.6049662": {"單位": "", "標題": "(ㄧ)、研究計畫之背景與目的", "作者": "", "發布日": 0, "摘要": "", "全文": "自然語言生成(NaturalLanguageGeneration)將是未來深度學習領域之熱門研究議題。2022年年末ChatGPT語言模型所帶來的示範性與展示效果預期將吸引更多研究目光。而個人於中興大學之研究團隊近年於自然語言生成之耕耘已建立初步研究成果。在過去朝著自然語言生成方向前進，我們探索自動生成試題之研究方向前進。於前期研究計畫「基於深度學習語言模型之閱讀測驗自動化生成研究109-2221-E-005-058-MY3」中，我們著眼於基於深度學習模型進行閱讀測驗(ReadingComprehensionTesting)題型之自動出題。在計畫執行過程中，我們•Ying-HongChan,Yao-ChungFan:KeywordProvisionQuestionGenerationforFacilitatingEducationalReadingComprehensionPreparation.INLG(InternationalNaturalLanguageGen-erationConference)2022.•Ho-LamChung,Ying-HongChan,Yao-ChungFan:ABERT-basedDistractorGenerationSchemewithMulti-taskingandNegativeAnswerTrainingStrategies.EMNLP(EmpiricalMethodsinNaturalLanguageProcessing)Findings2020.•Shang-HsuanChiang,Ssu-ChengWang,Yao-ChungFan:CDGP:AutomaticClozeDistractorGenerationbasedonPre-trainedLanguageModel.EMNLP(EmpiricalMethodsinNaturalLanguageProcessing)Findings2022.•Hsien-YungPeng,Ho-LamChung,Ying-HongChan,Yao-ChungFan:MisleadingInferenceGenerationviaProximalPolicyOptimization.PAKDD(AdvancesinKnowledgeDiscoveryandDataMining-26thPacific-AsiaConference)2022此外我們也將前期計畫之自動出題研究成果進行實作，可於以下兩個網址進行實測。•問句生成QuestionGeneration：給定文章、指定片段，透過自然語言生成模型進行問句生成。目前系統可於https://classic.queratorai.com/進行實測。•克漏字干擾選項生成DistractorGeneration：此一部分針對給定之文章進行克漏字誘導選項生成。目前系統可於：https://cdgp-demo.nlpnchu.org/進行實測。基於過去之研究成果累積，本計劃延續自然語言生成技術於自動出題之研究成果，擬定知識類型選擇題(MultipleChoiceQuestion,MCQ)自動生成為目標。以四技二專統一測驗考試為例(如Table2所示)，知識類型選擇題將包含題幹與四個選項(其中包含正確答案與干擾選項)，主要針對某一知識範圍進行測驗。我們認為知識範圍選擇題之自動生成可分為兩部分：(1)問題題幹(Stem)生成與(2)干擾選項(Distractor)生成。題幹生成主要目標為選擇題問題本身之生成，而干擾選項則著眼於誘導選項之生成。知識類型選擇題為學習測驗最常見的測驗模式之一。對比於閱讀測驗生成，知識類型選擇題的自動生成，將可減輕教師準備個人化教學測驗之負擔，也為學生提供更有效率之學習檢測方式。以現有我們成果為基礎，一個基本之自動選擇題出題系統作法為(如圖1)所示：我們可利用資訊檢索技術(如BM25Retriever)，從給定之文集資料庫(目前使用2021中文維基百科所有中文文章)中檢索主題字詞相關文章後，結合前期計畫所開發之問句生成模型(QuestionGeneration)[4]生成題幹，而後再利用詞向量模型(例如word2vec模型)找出相近字詞以為干擾選項。我們將這樣的構想先行實作一系統(稱之為Retrieve-and-GeneratePipeline)，目前此系統可於https://mcq-demo.nlpnchu.org/使用。使用者可藉由輸入主題相關字詞，選擇題生成系統將自動生成輸入字詞相關之選擇題結果。舉例而言，我們以「骨髓、白血病、淋巴」為主題字詞輸入至系統。我們挑選較佳之成果顯示於Table1。自動生成選擇題AutomaticGeneratedMCQQ1:哪一種癌症種類的統稱中可以看出它會造成不正常白血球的大量增生?(a)白血病、(b)乳腺癌、(c)腎病、(d)帕金森氏症Q1:白血病的發病原因通常是發生在哪一個器官上?(a)肝臟、(b)淋巴、(c)甲狀腺、(d)骨髓Q:白血病是最常見的兒童癌症，其中80%的病例是慢性的哪一種性病變?(a)淋巴結、(b)微血管、(c)淋巴、(d)黏膜Table1:目前選擇題自動生成結果範例。目前系統可於https://mcq-demo.nlpnchu.org/。目前我們挑選較佳之生成結果顯示於此Table中。建議審查委員自行輸入關鍵字詞組，來觀察我們Figure1:Retrieve-and-Generate基本架構。本例中我們以”骨髓、白血病、淋巴”為關鍵詞進行文章檢索，而再使用”骨髓”為答案，分別生成問句與干擾選項。從展示之結果中，我們發現自動選擇題出題之可能性。但仔細觀察會發現現有系統離實際應用仍有空間須待努力。也因此觸發本期研究計畫之提出。具體而言，我們認為選擇題自•句子等級之干擾選項生成(Sentence-LevelDistractorGeneration)首先，關於干擾選項的部分，若仰賴詞向量模型來產生干擾選項，我們所能取得的干擾選項便僅限於單詞(Word-Level)等級。從統測試題中，我們發現許多題目之干擾選項屬於句子等級(Sentence-Level)。以Table2中Q3與Q4選擇題為例，我們可以看到選項皆為句子等級之描述，此為目前基於詞向量來產生干擾選項的做法所不能。•反事實干擾選項生成(Counter-FactualSentenceGeneration)我們觀察到選擇題題型中，有著下列何者為錯誤或正確類型之考題。其干擾選項是一個較長的描述句。以Table2為例，Q5與Q6為分別詢問下列選項何者正確/錯誤。此題型，為變像之是非題。除了句子等級生成外，核心技術牽涉的是反事實(Counter-Factual)句子之生成。如何生成此類型之句子以為干擾選項也將是研究重點所在。•選擇題生成品質提升(MCQQuestionQualityImprovement)此外，我們也看到retreive-and-generatepipeline的做法，所產生的題幹本身有時是一個過於空泛之問題(例如，下列何種疾病為兒童常見之癌症？)或者並非語意正確之語句(例如，中樞性的什麼症狀是缺乏抗利尿激素的?)。如何有效判斷生成不良之語句，減低人工進一步過濾/挑選題目之負擔也是需進行研究之議題。此外，基於詞向量之干擾選項作法也有待商榷;隨機從相近詞向量挑選distractor，是否能確保distractor之誘導品類型I:單詞等級的干擾選項Word-LevelDistractorsQ1：淋巴器官是人體防禦系統中重要的一環，可提供淋巴細胞發育或活化所需要的條件與狀態。下列何者是淋巴球進行防禦與活化的器官？(a)骨髓(b)胸腺(c)肝臟(d)脾臟Q2：健康人體感覺神經傳入大腦的訊息大部分經過轉接，(a)視丘(b)下視丘(c)中腦(d)橋腦類型II:句子等級的干擾選項Sentence-LevelDistractors(a)無就巢性(b)體型大(c)著肉多(d)耐粗食Q4：下列何者為「聯合國國際糧農組織」(FAO)成立最重要的目標及首要任務？(a)消除及戰勝飢餓(b)聯絡及交流農業資訊(c)自然環境的永續及保護(d)實現國際間的公平貿易類型III:句子等級且與事實相違背的干擾選項Counter-FactualDistractorsQ5：有關人類上皮細胞的細胞週期之敘述，下列何者正確？(c)後期，分離的染色分體往細胞中央移動(d)末期，細胞膜內凹且細胞質開始一分為二Q6：有關一健康成人泌尿系統功能的敘述，下列何者錯誤？(a)腎臟每日約產生濾液180公升及尿液1.5公升(b)腎小管藉壓力差方式再吸收葡萄糖回微血管(c)腎臟藉排尿量多寡來維持體內水分的恆定(d)若出現尿毒症狀時，可使用血液透析方式進行治療Table2:四技二專統一測驗選擇題類型概觀。Q1與Q2擷取自110學年度護理基礎生物考科，Q3與Q4110學年度農業群農業概論考科，Q5與Q6取自110學年度農業群基礎生物考科質？也許所挑選的distractor並不具誘導性，使得受測者很輕易地可挑選出正確答案，或者挑選到答案的同義詞，造成一題有多個正確答案。根據先前計畫執行訪談教師所知，於測驗時為考量受測全體鑑別度，教師出題時會考量干擾選項彼此之間之難易度；通常三個干擾選項中，會有一個離正確答案較遠之干擾選項(誘導性較低)，一個次之，一個最為困難最具有誘導性。藉此來有適當的鑑別度。目前我們直接使用詞向量之作法並未考量此設計。•主題題組生成(MCQQuestionGroupGeneration)目前我們的系統僅為單一關鍵字詞之出題。我們認為一個良好的自動出題應該是以題組為單位進行出題；使用者指定一主題知識，系統根據此一主題知識範圍進行5-10題之選擇題自動生成，題目之間也應該彼此考量。目前Retreive-and-Generatepipeline的做法，我們使用使用者給定的關鍵字為答案，進行題目之生成。然而比較好的做法應為將使用者給定的關鍵字集合為知識範圍之指定，系統根據該範圍自動挑選重要概念關鍵字進行答案挑選，而後再進行選擇題題組之生成。針對上述之研究挑戰，於本研究規劃中我們擬定下列八項研究議題進行探討。•研究項目ㄧ：基於語言模型與知識圖譜之Text2Text干擾選項生成模型(DistractorGenerationbasedonText2TextLanguageModelandKnowledgeGraph)Figure2:Text2Text干擾選項生成架構Figure3:基於Masked-LM之反事實干擾選項現有之Distractor生成研究多是基於CandicateGenerating-and-Ranking架構，先透過知識庫(如Wordnet)或者透過MaskedLM的方式取得候選字後，再適當進行候選詞特徵排序以為最終干擾選項。對此我們有不同之想法，我們預計嘗試把干擾選項生成設定為一個Text2Text任務；如圖2所例，預期的輸入為題幹Q與答案A之串接字串，透過ModelEncoder與Decoder轉化輸出為一連串之干擾選項字串“重組DNA技術[SEP]電腦斷層掃描[SEP]組織培養”。我們透過語言生成模型直接將輸入對比轉換為輸出。此外，透過此一Text2Text方式，生成句子等級的干擾選項也將為可能。再者，近年來語言模型研究趨勢除了將模型參數量與訓練資料持續增加外，引入知識圖譜來強化生成模型於常識理解與推理能力也成為研究社群的關注所在[5][13]。也因此，探索整合知識圖譜於選擇題自動生成也因此成為我們規劃之研究項目。希冀藉由整合知識圖譜來提升干擾選項之誘導性與題幹生成之正確性。•研究項目二：基於Masked-LM之反事實干擾選項生成研究(Counter-FactualSentenceGenerationbasedonMasked-LM)如同前言所述，“下列何者正確/錯誤”題目為常見之類型選擇題題型。對此，我們預計嘗試以下之作法。首先，我們假定可取得正確敘述之語句S。我們先針對語句S中的字詞進行命名實體辨識(NamedEntityRecognition,NER)，其後再利用語言模型針對所取得之命名實體進行WholeWordMasking(WWM)，填入機率較低之填空結果，藉此生成反事實描述之語句。如Figure4所例，我們將所給定之事實描述，藉由NER方式挑選”葡萄糖”一次進行Masked，再藉由語言模型推測該Masked字詞，並挑選機率相對較低之預測填入。藉此創造出反事實之陳述句。然而，如何確保不挑選到同義詞？要進行多少個字詞之Masked？如何自動從主題文章中挑選事實陳述句以進行改寫？等等•研究項目三：基於知識圖譜強化之反事實干擾選項生成(Counter-FactualSentenceGen-erationbasedonKnowledge-GraphAugmentedLanguageModel)針對反事實干擾選項生成之議題，除了研究項目二之Masked-LM作法外，我們另一個構想為利用知識圖譜來強化Counter-Factual語句之生成。如Figure4所示，我們基本想法為先萃取與題幹相關聯之知識圖譜子圖(LocalSubgraph)並取得知識三元組，以此輸入至語言模型中，訓練語言模型反向改寫三元組語句，以此為反事實干擾選項。然而，此模式之訓練資料該如何取得？如何從知識圖譜上萃取子圖，並生成合適三元組？如何引入GraphNeuralNetwork來萃取結構？為此研究項目需進行探討之工作內容。•研究項目四：基於對比式學習之干擾選項生成模型強化(DistractorGenerationModelbasedonContrastiveLearning)我們過去的研究經驗[8]與相關研究[35]皆指出當使用Generative語言模型來生成句子Figure4:基於知識圖譜強化之反事實干擾選項生成構想層級之輸出時，模型經常會產生語意之相同，但字面的結果。如Table3所示，選項A跟C雖然字面敘述上不同，但實際上都是描述同樣的意思。生成重複選項將導致選擇副交感神經興奮會導致使心跳加速，血壓上升Table3:模型生成的誤導選項重複出現的範例對此，我們預計探索對比式學習(ContrastiveLearning)[7][35]來提升生成品質。對比式學習於自然語言之應用中主要想法為透過選取PositiveSample以及NegativSample使得模型去學習到生成句子跟同義句子的相同以及跟異類句子的不同之處，這樣我們可以使用重複選項當作我們的NegativeSample，然後資料集中的干擾選項當作PostiveSample，期望模型可以學習避免生成出重複的誤導選項。因此，我們預計探索對比式•研究項目五：使用Open-DomainQuestionAnswering錯誤結果之干擾選項挑選研究(Us-ingIncorrectAnswersfromODQAasDistractors)而除了利用生成模型產生Distractors外，我們發現利用Open-DomainQuestionAnswer-ing(ODQA)模型所產生的錯誤結果是一個值得探討的干擾選項的取得方式。具體而言，現有的ODQA模型主要包含一個Retriever與一個Reader，Retriever負責檢索相關的文章，而Reader利用檢索出來的文章利用閱讀理解模型(ReadingComprehensionModel)來找尋答案片段。由於ODQA並非完美，因此也會有錯誤結果之回報。但我們發現錯誤結果與正確答案並非天差地遠，閱讀理解模型所回報之結果通常也與答案接以圖5為例，當我們下達Q：「下列何者為目前世界上應用最廣的殺草劑，其具備有生長素之特性，常用於闊葉雜草防治，亦為選擇性殺草劑的一種？」，ODQA系統將取得圖5中右半部之答案：「萌前除草劑、2,4-D、非選擇性除草劑、10殺丹粒劑」。其中，2,4-D為正確答案，餘者為錯誤答案。然而我們覺得這些錯誤答案，可以當作Q之干擾選項來使用。有別於基於生成模型研究項目，利用ODQA模型之錯誤答案來當作干擾選項應可提供不同思路進行干擾選項生成。這樣的想法乍看可行，但仍有若干技術點需進行探討。諸如，如何控制ODQA模型之錯誤？我們需要在Reader部分弱化Reader的能力，或者弱化Retriever能力，來製造ODQA的錯誤？如何從錯誤回報結果挑選干擾選項？這些皆是尚待深入研究之議題。也因此觸發本研究項目之規劃。Figure5:利用ODQA模型之錯誤答案來當作Distractors•研究項目六：基於NodeCentrality考量主題涵蓋性之選擇題題組生成(MCQGroupGenerationconsideringTopicCoveragebasedGraphNodeCentrality)上述所規劃的選擇題生成做法，主要針對單題進行生成。若要產生多題/題組考題，就是重複相同流程，獨立產生個別考題後，整合為一題組。然而，題組生成做法應當考慮題與題之間之關聯性與互補性。因此，我們認為比較好的做法應當為根據使用者所指定的主題，透過(1)知識圖譜子圖萃取來涵蓋所指定主題之內容與(2)透過主題重要概念萃取：藉由知識圖譜上NodeCentrality方式來萃取關鍵字，以為題幹之生成。此外，如何根據主題互補性，來挑選個別生成之題組，用以組成選擇題題組也是需進行•研究項目七：考量題幹可用性與干擾選項間關聯之Retrieve-and-Generate架構優化(Op-timizingRetrieve-and-GenerateFrameworkconsideringStemUtilityandDistractorCorrela-tion)本研究項目著眼於優化現有Retrieve-and-Generate架構設定。Retrieve-and-Generate架構雖然簡單，但我們可以發現其於單字等級的干擾選項生成已有一定的可用性。我們預計可優化之處為：(1)我們發現引用現有系統於題幹生成時，會有題幹生成結果不佳的狀況。對此，我們發現其中的一個原因為Retriever所檢索之文章品質不佳(文章過短或者與主題不相關)，導致不良之題幹生成。因此如何改善檢索機制來挑選較佳的文章用以進行生成，便是值得探討的課題。再者，題幹生成不佳也有部分原因是來自於問句生成模型本身之生成錯誤，如何有效過濾生成不佳的題幹也因此是一個可需進行研究之項目。(2)如同前言所述，干擾選項之挑選應該考量彼此間之關聯，而非獨立挑選。如何納入此觀點來進行干擾選項之生成也因此成為預計關注之議題。又再者，我們基於word2vec方式之字詞等級干擾選項挑選是否可以引入語言模型之詞向量來求取•研究項目八：生成難度可控之選擇題生成研究Difficulty-ControlledMCQGeneration發展自動化生成考題之可能願景為：針對個人化學習自對應生成難度適中之測驗，以增加學習與檢測之效率。因此，如何針對現有技術探討生成結果難度控制也因此成為一可探索之研究議題。由於我們的基礎架構為基於文章檢索來取得包含使用者所給定的關鍵字之文章，並使用該文章與關鍵字為問題生成模型之輸入，來生成選擇題問句。我們若挑選較為冷門之文章進行生成，則預期可以生成較為困難之題幹。而就干擾選項而言，若我們基於研究項目五「使用Open-DomainQuestionAnswering錯誤結果之干擾選項挑選研究」，利用ODQA來取得干擾選項之作法。我們可藉由調整輸入給閱讀理解模型之文章來產生難度較高的錯誤。具體而言，給定一有包含正確答案A之文章C，我們先將答案片段A於文章D中刪除，而後將其輸入至閱讀理解模型，藉此產生錯誤之閱讀理解結果，以這樣的錯誤結果當干擾選項預期將有較高之誘導能力。相類似的想法也可以套用為挑選Retriever認為相關度比較不高的文章以為閱讀理解模型之輸入。此外，就生成題目之廣度與深度而言，我們可藉由控制取得之文章來調配題組之廣度與深度。例如，若我們以單篇或少數篇文章進行生成，我們所生成的主題題組針對測驗內容而言，應該可產生知識性較為深入之試題。而若針對較多篇文章，則應可產生範圍較廣之測驗試題。上述之研究議題，乍看可行但仍有許多細節需進行探索，如上述研究背景目的所述，本研究計畫擬探索基於現有Retrieve-and-GeneratePipeline之選擇題自動生成架構進行強化。我們以三年為期，規劃在此目標下擬定八項關鍵技術進行研究探討。如Figure6所示，八個工作項目又可區分為三項主軸：「選擇題生成品質提升」、「反事實選項生成」與「群組生成與難度控制」。其中，於「選擇題生成品質提升」研究主軸中，我們探索利用語言模型與知識圖譜架構來生成，將選擇題選項生成問題設定成為一Text2Text任務，輸入為問題題幹與正確答案所串接之TextSequence，而輸出為所生成之distractor串接之TextSequence。此外，我們也嘗試引入對比式學習來改善Text2Text生成模式之品質。我們也預期規劃探索利用ODQA方式來取得干擾選項。而於「反事實選項生成」研究主軸中，我們探索反事實陳述句之生成。我們預計嘗試基於MaskedLM取代重要名詞之反事實語句生成作法與基於知識圖譜知識三元組萃取改寫之作法。於第三個研究主軸中，我們則嘗試針對題組生成與難度可控制之干擾選項生成進行研究。MultipleChoice,Multi-此資料集特色在於為Multi-hop形式，答案需從複數文檔中推理而成；資料來自維基百科以及PubMed，PubMed為生物醫學論文的搜尋引擎，在兩個來源各自提供51,318(Wiki)和2,508(PubMed)筆問題。資料來自國高中英文考試，提供超過10萬筆問題以及2.8萬篇文本，題型上為英文閱讀測驗，會需要閱讀故事或對話紀錄，而針對該文本會有複數題目，每個題目此資料集的挑戰在於，內容來自於7個不同領域(新聞、小學自然科學、旅遊、小說故事等等)，為文本與題目提供多樣性，總計約1萬筆問題與800篇文本。MultipleChoice,Open-為Open-domain形式，並不提供文本，只有問題與答案；包含12,247筆問題、一個正確答案和四個干擾選項；題目類型為知識與常識題，來自於ConceptNet與同時包含兩種形式資料，填空題與Open-domain；填空題形式命名為QUASAR-S，其中資料來源自StackOverflow，提供約37K筆問題。Open-domain形式命名為QUASAR-T，提供約43K筆問題。來自StanfordUniversity的閱讀理解資料集，包含了超過100K筆問題和536篇對應的維基百科文本；延續了SQuAD1.1的題目外，[31]另外加入約50K筆無答案的題目，進一步測試機器能否在無對應答案時回答否，而MedQA是基於美國醫師執照考試(USMLE)所產生的資料集，其中問題皆為較為困難的專業醫學相關的知識，且包含了英文、簡體中文、繁體中文分別有約12K、34K、14K個問題集，這種專業的知識能使機器針對單一領域產生更具參考價值的distractor，目前此任務排名上最佳為ValentinLiévin團隊做出的Codex5-shot訓練機器學習模型進行閱讀理解(MachineReadingComprehensionTask)為最常見的深度學習自然語言任務。諸如，SQuAD[32]、RACE[21]、DROP[10]與HotPotQA[41]等資料集，皆是測驗/訓練機器學習模型於答案找尋、邏輯推理與摘要之能力。其中，許多機器閱讀理解任務採用多選題方式進行測驗。選擇題的自動答題與選擇題的自動生成為一體兩面之應用。因此具有選擇題樣態的資料集為我們選擇題自動生成研究之根本資料集，我們將目前所探知之相選擇題與干擾選項生成之相關研究可按應用類型可區隔為英文學習測驗與知識學習測驗。干擾選項生成研究於英文學習測驗應用，主要以自動生成閱讀理解與克漏字類型的考題為目標。閱讀理解之干擾選項生成與本研究計畫之知識類型選擇題之自動出題目標不同。因此本研究回顧中，我們不做此分類每一研究細部說明。我們將此類型之相關研究按不同象限做分類：(1)干擾選項層級之差異(生成字詞等級與句子等級之差異)象限，(2)題目類型(閱讀理解與克漏字測驗)，(3)生成方式(CandidateRanking與Generative)。我們按此三象限，將目前之相關研究摘要整理於Table5中。而知識學習測驗類型的選擇題生成，則於這一兩年逐漸吸引相關研究投入。我們認為與本研究計畫較為相關之研究有[22][40][3]，以下我們針對個別文獻討論如下。首先，與我們有相同目標者為[22]，也是針對特定知識主題生成選擇題。其主要作法Table5:現有閱讀測驗與克漏字干擾選項生成研究整理表為：先由人工給定一個主題，程式使用該主題名稱的維基百科頁面作為起始點(例如給定“體育”後搜尋體育相關維基百科)，保留熱門文章並取出文章的第一段擷取摘要，由此產生大量句子。並將這些句子依單純英文文法規則生成簡單5W問句與答案，例如who問句是將第一個動詞前的名詞作為正確答案並從句子中移除，what問句是將第一個夾在動詞間的名詞移除作為正確答案並從句子中移除，移除答案後的句子再帶入問題模板生成制式問句。生出五種問句後再依英文文法的詞與詞的相關性去決定哪個問句最適合。在干擾選項生成的部分，先隨機選取其他句子所產生的答案，再藉由語言模型去排序與問題的相關性，取前三高之預測為干擾選項。但本論文並未使用問句生成模型，採取之作法也與本計畫所規劃有所不同。於論文中[40]，則探討如何讓干擾選項更加多樣且與正確答案相差不遠(以具誘導性)。對此，創造了一款監督式分類任務來找出文章中屬於關鍵句子(及對產生干擾選項生成有用的句子)，作者在訓練之前會將訓練用的干擾選項和文章中的每個句子做比對，若該句子和干擾選項相似度達到一定程度，則該句子將成為文章中的關鍵句，透過這個方式在文章中能被修改成為新的干擾選項的句子將會達到多樣性的效果，且透過相似度對比，干擾選項也不會跟原始文章相差太遠從而提升干擾選項的品質。本研究之作法可供我們借鏡參考對關鍵文章/句選擇設計，用以提升干擾選項之多樣性。而與本計畫研究項目五「使用Open-DomainQuestionAnswer錯誤結果之干擾選項」走非生成想法相似之研究為[3]。其主要想法為作者發現於干擾選項資料集中，有許多干擾選項一再被使用。促使論文作者想為新的問題檢索以重複使用干擾選項。作者提出一個Transformer-baseddistractorretrievalmodels，該模型通過考慮問題的上下文來選擇干擾選項模型包含三個部分，分別為D-SIM模型、Q-SIM模型以及DQ-SIM模型。在D-SIM模型中使用mBERT計算同一個MCQ中問題和答案以及對應的干擾選項的向量，並計算他們之間的相似度。而在Q-SIM模型中作者假設不同問題共享一個或多個干擾選項或答案，代表問題可能在語意上相關，因此他們相關的干擾選項可以用作彼此的良好候選干擾選項，因此作者將訓練資料重新排序，使這些共享至少一個干擾選項或答案的問題聚集在一起，接著將資料送進Q-SIM模型計算不同的MCQ中問題之間的相似度。在DQ-SIM模型線性結合前兩個模型，從各自的優勢中獲益，透過基於分數和基於排名的合併策略計算來自D-SIM模型和Q-SIM模型的分數，並設計演算法使得高排名之干擾選項有較大的權重。重複使用現有資料集中之干擾選項為一創新想法，但仍與我們之ODQA做法有所不同。[OpenDomainQuestionAnswering相關研究]本計畫的選擇題生成架構主要基於兩元件:(1)文章萃取與(2)問句生成。目前我們所使用的文章萃取機制為傳統之BM25檢索。而近年來由於ODQA研究領域之發展，基於深度學習之DenseRetrieval技巧也紛紛於ODQA研究領域中所提出。我們預期DenseRetrieval技術也將於我們知識類型選擇題自動出題中擔任關鍵角色。茲簡要回顧現有ODQA之研究如下。ODQA架構中Retriver之重要性不言可喻。不同層級的語意檢索也影響著ODQA的效能。於[28]研究中，作者提出一個簡單而有效的pipeline系統，研究段落和句子級別語義檢索及其對下游任務的潛在影響，作法上是先檢索段落級別的段落，然後從上一步檢索後取得的段落中再去檢索句子級別的段落。該系統在事實驗證和Multi-hopQA方面進行評估，實驗結果顯示QA任務與事實驗證皆仰來上游檢索之結果。於[23]論文中，作者在ODQA架構的retreiver端提出了一個有效率的弱監督式學習的方法(InverseClozeTask)來對retreiver進行預訓練，給予模型完全無人工標記的維基百科原始資料，藉由論文中的方法將資料切分成固定長度的段落，並隨機選取其中的一個句子當作標籤，讓模型去學習如何從給定的句子去資料庫中找尋與之最相關的文章。在實驗數據中此方法訓練出的ODQA模型分數高於傳統的BM25檢索方法。而於DPR[16]論文中，作者為解決傳統BM25無法檢索語意相似段落的問題，提出了基於深度學習模型之DensePassageRetrieval，並提出使用in-batchnegative的技術，在密集向量的檢索取得極佳的效果改進。我們也預計引入中文DRP模型來提升目前題幹生成所需之而於GAR[27]研究中，作者們探討傳統基於BM25的檢索無法檢索沒有單詞重疊或語意相似的段落的問題，而基於密集向量的檢索方式在計算上又過於複雜，且會有資訊損失的問題。該論文透過預訓練語言模型自動生成與查詢語句相關的內容，可增加查詢語句的資訊量，隨後透過BM25進行檢索，該方法比起傳統BM25能檢索到更精確的段落，且執行效率也快於基於密集向量的檢索方式。透過GAR中的PLM可對查詢語句的資訊進行補充，原論文中是補充答案、相關段落及段落標題，可基於該手法讓PLM生成問題與原始答案的關聯，對這些關聯進行組合或刪除即可檢索與原答案有相似關係的查詢語句，可檢索到適合製作干於[19]，作者觀察到基於語言模型之檢索技術雖然效果很好，但比以往的方法需要極高的計算成本，特別是在計算每個query與document間的relevancescore時。本篇論文作者提出了ColBERT，引入lateinteraction架構，對query與document進行獨立的encoding，於模型輸出最終階段才進行相似度運算。而[18]中，則使用近一步基於ColBERT當基礎模型來進行retreiver端的訓練，使用了Relevance-guidedSupervision(RGS)的方法來訓練retreiver的模型。相較於傳統的retreiver訓練資料，傳統方法需人工標註的訓練資料，但作者認為人工標住資料往往存在偏見，進而影響模型的效能，所以在論文中提出了用訓練好的模型來找尋最適合的訓練資料，並把這些資料用於訓練下一個模型，使用這個方式來進行後續的訓練，最後得出最佳的模型。此方法訓練出的模型分數明顯高於目前opendomain的其他方式。在找尋訓練資料時，模型會先搜尋前k筆資料，並把前20筆資料中含有shortanswer的資料當成possitiveexample，而前20筆以外剩餘的資料中不含有shortanswer的資料當成negativeexample，此方法尋找的負面訓練資料交由reader進行預測能得到與正確答案相關的錯誤答案，如能改良ranking的方式，應用於干擾選項生成，預期將能提升干擾選項之品質。[ContrastiveLearning於自然語言生成相關研究]對比式學習始見於電腦視覺領域，其於影像應用上有著顯著的效果提升。因此，許多自然語言處理之研究者也將其引用至自然語言生成領域，諸如[11][7][35]。於[11]研究中，作者們提出了使用ContrastiveLearning的作法來學習SentenceEmbedding，他們使用BERT作為基礎模型，透過不同的dropoutmask使模型能用同樣的輸入產生不同的SentenceEmbedding，但它們之間的距離會是近的，也就可作為PositiveSample。然後再使用其他in-batch的句子作為NegativeSample，以達到非監督式訓練。而在有訓練資料的情況下，如NLI資料集，則可以將Eentailment視為PositiveSample，Contradiction以及其他in-batch的句子視為Negative而再基於[11]研究後，於[7]研究中，作者們提出加上了Discriminator來加強SentenceEmbedding的學習效果。具體而言，對每筆輸入來說，會透過SentenceEncoder產生一個SentenceEmbedding，同時也會隨機的Masking輸入句子其中的幾個Token，並透過一個fixed的Generator來還原MaskToken，而Discriminator要試著透過沒被修改過的SentenceEmbedding的資訊去辨認修改後的句子中的哪一個Token有被修改過，整個框架會透過SentenceEmbedding的ContrastiveLearningLoss以及Discriminator的ReplacedTokenDetection而與我們預計引入對比式學習來提升句子等級的干擾選項生成效果想法接近的研究也可見於[35]。該研究指出發現了以往autoregressive模型使用BeamSearch去決定要生成的文本時，會產生語意不自然以及沒意義的重複文字，儘管後續有人使用取樣以及修改ObjectiveFunction來嘗試解決這個問題，但這個方法時常會產生語意不連貫的問題。因此作者提出了使用ContrastiveLearningLoss加上原先的MaximumLikelihoodEstimationLoss來訓練模型，並且使用ContrastiveSearch來作為decodingmethod。[Counter-FactualSentenceGeneration相關研究]於Counter-FactualGeneration研究中，[44,33]為相關研究之先驅。主要想法是使用神經網路來生成反事實敘述，這個概念背後的想法是同時使用語言模型和各種採樣的策略來達成生成反事實敘述的效果。接續之前的成果，[39]提出了Polyjuice架構，可以控制輸出的反事實敘述的類型。另外一種生成干擾選項的研究是使用干擾選項生成(DistractorGeneration)的手法，干擾選項生成(DistractorGeneration)的目標是給予了一篇文章、問題和正確答案後，模型能夠生成似是而非的干擾選項。於研究[24]中，作者最先提出使用了神經網路來生成干擾選項，而研究[8]則延續了前者的成果，把重點放在生成複數的干擾選項並提出了一個負樣本學習策略(negativetrainingstrategy)來加強干擾選項生成。但這上述兩個研究主要都著眼於閱讀理解題型中干擾選項之生成，與本研究知識類型選擇題生成目標所有不同。而在關於AbductiveInferenceGeneration的相關研究中，於[30]中作者提出了故事改寫的任務，此任務目標是要改寫一段故事的敘述並且導致產生與原先不同的結局，與顯永的論文的方式的差異為故事改寫任務是根據給定的誤導推論來生成不同的結局，在MIG任務中是給定敘述句與其前後文來生成誤導推論。[2]研究中是讓模型根據上下文來推論中間適合加入的句子，此研究主要是產生正確的推論敘述，而在MIG任務[29]主要是產生相反的誤導推論敘述。於我們先前的研究[29]中，我們提出一個自然語言的誤導推論生成任務，任務步驟是當給定了一個陳述事實的句子並給定其上下文，將其定義為(x,y,z)與誤導推論ˆy，模型會生成與該事實相反的推論句子，由此可以讓模型產生出多選題似是而非的干擾選項。然而這樣任務與我們所設定之反事實出題仍有所不同。而與我們生成Counter-Factual陳述句想法接近的研究為：[46]。於該研究中，作者探討有別於過往的是非題，將文章中原始句子作為True的問句並通過反義詞替換關鍵字生成False問句，本篇作者通過使用一種新穎的masking-and-infilling策略來生成更靈活與複雜的問句。要被mask的關鍵字找法有很多種，像是SRL結果中的predicate，句子中連接詞(”that”、”when”、”since”)之後的部分或是數字等等，來找出值得提問的句子。Infilling的作法則是以BART為基底作為生成模型，將原始句子的關鍵字mask後，提供前後句子作為BART的上下文後產生句子填入。作者為了找值得提問的句子所提出的方法亦可用來作為多選題產生題目的答案選擇方法，而infilling的想法如果使用得宜，在提供出題範圍的前後文後，也能將欲出題句子挖空讓BART模型填補後產生選項。雖然與我們目標相似，但仍與我整合知識圖譜與語言模型為近年語言模型訓練熱門關注議題。於[42]研究中，作者們提出一CounterfactualGenerationControllableCounterfactualGenerationDistractorGenerationMisleadingInferenceGeneration種能將文本和知識圖譜的資訊進行深度雙向、自我監督的預訓練的方法，透過作者的方法可以將一些特定領域的KnowledgeBase資料集結合相關文獻去訓練語言知識模型，讓語言模型和知識圖譜之間的資訊互相交換，並且在回答特定領域問答的任務上，表現優於現有的語言模型和知識圖譜增強模型。我們想透過作者提供的預訓練模型，來做多選題選項生成，透過知識圖譜所額外提供的資訊，讓在考慮生成選項時，除了題目和答案的文字外，用這些文字從知識圖譜萃取相關的知識圖譜子圖幫助生成選項有更多參考資訊。知識圖譜於問句生成之研究也見於[5][13]。於[5]研究中，作者之想法為使用知識圖譜進行問題生成任務時，通常只會取出資料與關係的三元組合，並不能完整利用知識圖譜中豐富的資訊，因此作者基於GraphNeuralNetworks提出了BidirectionalGatedGraphNeuralNetwork(BiGGNN)，將知識圖譜中的node及edge編碼成向量，使得每個node都包含了周圍其他node的資訊，並對答案的node進行標註，隨後透過RNN進行問句生成，生成過程中RNN會決定生成新文字或直接複製某個node的資訊放進問題。我們應可借鏡此研究論文的想法，可將知識圖譜及答案的資訊完整編碼，可基於此特性，在生成問題時同時考慮答案附近的node，讓模型在建立問題的同時也能根據附近的node產生干擾選項。而於[13]研究中，則提出的ISEEQ架構，主要目的是希望從簡短的敘述句中去尋找更多訊息來反問使用者是否符合關於敘述句背後未提及的情況，在使用時，使用者只需輸入簡短的敘述句，模型會先從知識圖譜擴展並增加query的關鍵字，透過這些關鍵字模型就能retrieve更多相關的文章進行ranking，在從這些文章中使用QG模型產生一些原始query中未提及但可能相關背景知識，例如當使用者說道:“我感到疲勞且渾身無力”，ISEEQ能回覆:“請問病徵持續了多久?”，或是“你最近14天有出國過嗎?”這類可能與其他知識相關的問句。這種方式最大的貢獻及是讓機器學習到”反問”的能力，而非一味的透過query生成出結果。這樣的想法，或許可引用來提升題幹生成的精確度，舉例來說:有位教師想出關於日治時期的歷史題目，我們就能先藉由ISEEQ來詢問是要出與日治時期戰爭相關的題目或是與日治時期建設相關的題目，以此來取得更精確的題幹生成需求。[1]J.Araki,D.Rajagopal,S.Sankaranarayanan,S.Holm,Y.Yamakawa,andT.Mitamura.Gen-eratingquestionsandmultiple-choiceanswersusingsemanticanalysisoftexts.InProceedingsofCOLING2016,the26thInternationalConferenceonComputationalLinguistics:TechnicalPapers,pages1125–1136,2016.[2]C.Bhagavatula,R.L.Bras,C.Malaviya,K.Sakaguchi,A.Holtzman,H.Rashkin,D.Downey,S.W.-t.Yih,andY.Choi.Abductivecommonsensereasoning.arXivpreprintarXiv:1908.05739,[3]S.K.Bitew,A.Hadifar,L.Sterckx,J.Deleu,C.Develder,andT.Demeester.Learningtoreusedistractorstosupportmultiplechoicequestiongenerationineducation.IEEETransactionsonLearningTechnologies,2022.[4]Y.-H.ChanandY.-C.Fan.Arecurrentbert-basedmodelforquestiongeneration.InProceedingsofthe2ndWorkshoponMachineReadingforQuestionAnswering,pages154–162,2019.[5]Y.Chen,L.Wu,andM.J.Zaki.Towardsubgraphguidedknowledgegraphquestiongenerationwithgraphneuralnetworks.arXivpreprintarXiv:2004.06015,2020.[6]S.-H.Chiang,S.-C.Wang,andY.-C.Fan.Cdgp:Automaticclozedistractorgenerationbasedonpre-trainedlanugagemodel.InFindingsoftheAssociationforComputationalLinguistics:[7]Y.-S.Chuang,R.Dangovski,H.Luo,Y.Zhang,S.Chang,M.Soljac￿ic￿,S.-W.Li,W.-t.Yih,Y.Kim,andJ.Glass.Diffcse:Difference-basedcontrastivelearningforsentenceembeddings.arXivpreprintarXiv:2204.10298,2022.[8]H.-L.Chung,Y.-H.Chan,andY.-C.Fan.Abert-baseddistractorgenerationschemewithmulti-taskingandnegativeanswertrainingstrategies.InFindingsoftheAssociationforComputationalLinguistics:EMNLP2020,pages4390–4400,2020.[9]B.Dhingra,K.Mazaitis,andW.W.Cohen.Quasar:Datasetsforquestionansweringbysearchandreading.arXivpreprintarXiv:1707.03904,2017.[10]D.Dua,Y.Wang,P.Dasigi,G.Stanovsky,S.Singh,andM.Gardner.ingcomprehensionbenchmarkrequiringdiscretereasoningoverparagraphs.arXiv:1903.00161,2019.[11]T.Gao,X.Yao,andD.Chen.Simcse:Simplecontrastivelearningofsentenceembeddings.arXivpreprintarXiv:2104.08821,2021.[12]Y.Gao,L.Bing,P.Li,I.King,andM.R.Lyu.Generatingdistractorsforreadingcomprehen-sionquestionsfromrealexaminations.InProceedingsoftheAAAIConferenceonArtificialIntelligence,volume33,pages6423–6430,2019.[13]M.Gaur,K.Gunaratna,V.Srinivasan,andH.Jin.Iseeq:Informationseekingquestiongenerationusingdynamicmeta-informationretrievalandknowledgegraphs.InProceedingsoftheAAAIConferenceonArtificialIntelligence,volume36,pages10672–10680,2022.[14]Q.Guo,C.Kulkarni,A.Kittur,J.P.Bigham,andE.Brunskill.Questimator:Generatingknowl-edgeassessmentsforarbitrarytopics.InIJCAI-16:ProceedingsoftheAAAITwenty-FifthInternationalJointConferenceonArtificialIntelligence,2016.[15]D.Jin,E.Pan,N.Oufattole,W.-H.Weng,H.Fang,andP.Szolovits.Whatdiseasedoesthispa-tienthave?alarge-scaleopendomainquestionansweringdatasetfrommedicalexams.AppliedSciences,11(14):6421,2021.[16]V.Karpukhin,B.Og￿uz,S.Min,P.Lewis,L.Wu,S.Edunov,D.Chen,andW.-t.Yih.Densepassageretrievalforopen-domainquestionanswering.arXivpreprintarXiv:2004.04906,2020.[17]D.Khashabi,S.Chaturvedi,M.Roth,S.Upadhyay,andD.Roth.Lookingbeyondthesurface:Achallengesetforreadingcomprehensionovermultiplesentences.InProceedingsofthe2018ConferenceoftheNorthAmericanChapteroftheAssociationforComputationalLinguistics:HumanLanguageTechnologies,Volume1(LongPapers),pages252–262,2018.[18]O.Khattab,C.Potts,andM.Zaharia.Relevance-guidedsupervisionforopenqawithcolbert.TransactionsoftheAssociationforComputationalLinguistics,9:929–944,2021.[19]O.KhattabandM.Zaharia.Colbert:Efficientandeffectivepassagesearchviacontextualizedlateinteractionoverbert.InProceedingsofthe43rdInternationalACMSIGIRconferenceonresearchanddevelopmentinInformationRetrieval,pages39–48,2020.[20]G.Kumar,R.E.Banchs,andL.F.D’Haro.Revup:Automaticgap-fillquestiongenerationfromeducationaltexts.InProceedingsoftheTenthWorkshoponInnovativeUseofNLPforBuildingEducationalApplications,pages154–161,2015.[21]G.Lai,Q.Xie,H.Liu,Y.Yang,andE.Hovy.Race:Large-scalereadingcomprehensiondatasetfromexaminations.arXivpreprintarXiv:1704.04683,2017.[22]G.LeBerre,C.Cerisara,P.Langlais,andG.Lapalme.Unsupervisedmultiple-choicequestiongenerationforout-of-domainq&afine-tuning.InProceedingsofthe60thAnnualMeetingoftheAssociationforComputationalLinguistics(Volume2:ShortPapers),pages732–738,2022.[23]K.Lee,M.-W.Chang,andK.Toutanova.Latentretrievalforweaklysupervisedopendomainquestionanswering.arXivpreprintarXiv:1906.00300,2019.[24]C.Liang,X.Yang,N.Dave,D.Wham,B.Pursel,andC.L.Giles.Distractorgenerationformultiplechoicequestionsusinglearningtorank.InProceedingsofthethirteenthworkshoponinnovativeuseofNLPforbuildingeducationalapplications,pages284–290,2018.[25]C.Liang,X.Yang,D.Wham,B.Pursel,R.Passonneaur,andC.L.Giles.Distractorgenera-tionwithgenerativeadversarialnetsforautomaticallycreatingfill-in-the-blankquestions.InProceedingsoftheKnowledgeCaptureConference,pages1–4,2017.[26]V.Liévin,C.E.Hother,andO.Winther.Canlargelanguagemodelsreasonaboutmedicalquestions?arXivpreprintarXiv:2207.08143,2022.[27]Y.Mao,P.He,X.Liu,Y.Shen,J.Gao,J.Han,andW.Chen.Generation-augmentedretrievalforopen-domainquestionanswering.arXivpreprintarXiv:2009.08553,2020.[28]Y.Nie,S.Wang,andM.Bansal.Revealingtheimportanceofsemanticretrievalformachinereadingatscale.arXivpreprintarXiv:1909.08041,2019.[29]H.-Y.Peng,H.-L.Chung,Y.-H.Chan,andY.-C.Fan.Misleadinginferencegenerationviaproxi-malpolicyoptimization.InPacific-AsiaConferenceonKnowledgeDiscoveryandDataMining,pages497–509.Springer,2022.[30]L.Qin,A.Bosselut,A.Holtzman,C.Bhagavatula,E.Clark,andY.Choi.Counterfactualstoryreasoningandgeneration.arXivpreprintarXiv:1909.04076,2019.[31]P.Rajpurkar,R.Jia,andP.Liang.Knowwhatyoudon’tknow:Unanswerablequestionsforsquad.arXivpreprintarXiv:1806.03822,2018.[32]P.Rajpurkar,J.Zhang,K.Lopyrev,andP.Liang.Squad:100,000+questionsformachinecomprehensionoftext.arXivpreprintarXiv:1606.05250,2016.[33]S.Ren,Y.Deng,K.He,andW.Che.Generatingnaturallanguageadversarialexamplesthroughprobabilityweightedwordsaliency.InProceedingsofthe57thannualmeetingoftheassociationforcomputationallinguistics,pages1085–1097,2019.[34]S.RenandK.Q.Zhu.Knowledge-drivendistractorgenerationforcloze-stylemultiplechoicequestions.InProceedingsoftheAAAIConferenceonArtificialIntelligence,volume35,pages[35]Y.Su,T.Lan,Y.Wang,D.Yogatama,L.Kong,andN.Collier.Acontrastiveframeworkforneuraltextgeneration.arXivpreprintarXiv:2202.06417,2022.[36]A.Talmor,J.Herzig,N.Lourie,andJ.Berant.Commonsenseqa:Aquestionansweringchallengetargetingcommonsenseknowledge.arXivpreprintarXiv:1811.00937,2018.[37]J.Welbl,N.F.Liu,andM.Gardner.Crowdsourcingmultiplechoicesciencequestions.arXivpreprintarXiv:1707.06209,2017.[38]J.Welbl,P.Stenetorp,andS.Riedel.Constructingdatasetsformulti-hopreadingcomprehensionacrossdocuments.TransactionsoftheAssociationforComputationalLinguistics,6:287–302,[39]T.Wu,M.T.Ribeiro,J.Heer,andD.S.Weld.Polyjuice:Automated,general-purposecounter-factualgeneration.arXivpreprintarXiv:2101.00288,1(2),2021.[40]J.Xie,N.Peng,Y.Cai,T.Wang,andQ.Huang.Diversedistractorgenerationforconstruct-inghigh-qualitymultiplechoicequestions.IEEE/ACMTransactionsonAudio,Speech,andLanguageProcessing,30:280–291,2021.[41]Z.Yang,P.Qi,S.Zhang,Y.Bengio,W.W.Cohen,R.Salakhutdinov,andC.D.Manning.Hotpotqa:Adatasetfordiverse,explainablemulti-hopquestionanswering.arXiv:1809.09600,2018.[42]M.Yasunaga,A.Bosselut,H.Ren,X.Zhang,C.D.Manning,P.Liang,andJ.Leskovec.Deepbidirectionallanguage-knowledgegraphpretraining.arXivpreprintarXiv:2210.09338,2022.[43]C.Y.Yeung,J.S.Lee,andB.K.Tsou.Difficulty-awaredistractorgenerationforgap-fillitems.InProceedingsoftheThe17thAnnualWorkshopoftheAustralasianLanguageTechnologyAssociation,pages159–164,2019.[44]C.Zhang,Y.Sun,H.Chen,andJ.Wang.Generatingadequatedistractorsformultiple-choicequestions.arXivpreprintarXiv:2010.12658,2020.[45]X.Zhou,S.Luo,andY.Wu.Co-attentionhierarchicalnetwork:Generatingcoherentlongdistractorsforreadingcomprehension.InProceedingsoftheAAAIConferenceonArtificialIntelligence,volume34,pages9725–9732,2020.[46]B.Zou,P.Li,L.Pan,andA.Aw.Automatictrue/falsequestiongenerationforeducationalpur-pose.InProceedingsofthe17thWorkshoponInnovativeUseofNLPforBuildingEducationalApplications(BEA2022),pages61–70,2022.於本研究計畫中，我們目標基於自然語言生成技術進行選擇題自動出題研究。我們預期的展示情境為：給定一知識範圍(如農業文集)，我們可於系統直接輸入一組主題關鍵詞，而後自動生成關於該主題相關之選擇題題組(含字詞等級、句子等級、挑選是非類型之選擇題)。然而，如同前言背景中所言，現有技術離目標仍有許多待強化之處，對此我們擬定三大出題目標：字詞等級干擾選項、句子等級干擾選項與反事實干擾選項進行研究。針對此三大目標，我們擬定(1)基於知識圖譜之強化與(2)基於生成語言模型與對比學習之強化三個核心技術進行探討。研究技術項目與項目間之關係與進行規劃如Figure7所示。於第一年度中，我們規劃「考量題幹可用性與干擾選項間關聯之Retrieve-and-Generate架構優化」、「基於語言模型與知識圖譜之Text2Text干擾選項生成」與「基於Masked-LM之反事實干擾選項生成研究」進行研究。「考量題幹可用性與干擾選項間關聯之Retrieve-and-Generate架構優化」著眼於優化目前https://mcq-demo.nlpnchu.org之WordLevel之選擇題生成系統。希冀自動過濾相似或語意不正確題幹，並且考量干擾選項彼此間之關聯進行最終干擾選項集合之綜合挑選。「基於生成語言模型之Text2Text干擾選項生成模型」則考量Text2Text模式利用生成模型直接生成干擾選項序列，藉此嘗試生成句子等級之干擾選項。並進一步地探索結合知識圖譜與語言模型之干擾選項生成，希冀透過知識圖譜之整合來生成更有誘導性且合理之字詞等級干擾選項。此外，於「基於Masked-LM之反事實干擾選項生成研究」項目中，我們預計著手挑戰反事實干擾選項生成，我們嘗試利用透過NER擷取給定事實陳述句中之關鍵字並Masked-LM該關鍵字來填入機率較低之字詞，以為反事實語句生成之作法。我們預期第一年度工作完成後，將可優化現有單詞等級干擾選項之選擇題自動生成系統，並於第二年度中，我們則規劃「基於知識圖譜強化之反事實干擾選項生成」、「基於對比式學習之干擾選項生成模型強化」與「利用Open-DomainQuestionAnswering錯誤結果之干擾選項挑選」。於此年度中，進一步嘗試利用知識圖譜萃取三元組知識後進行反事實語句之生成，基本想法為透過萃取三元組後再進一步輸入語言模型，並希望語言模型適當地進行反事實語句的改寫。此一部分預計使用第一年度之「基於Masked-LM之反事實干擾選項生成研究」之基礎以為訓練資料。對於字詞等級的干擾選項，我們則探索利用Open-DomainQuestionAnswering模型所錯誤回報之答案當作干擾選項。我們希望透過此不同之想法來取得更多樣的干擾選項。再者，如前言所述，生成模型生成序列時常出現生成結果重複的狀況，對此我們嘗試利用對比式來改善生成模型這方面的缺點，以優化干擾選項之生成。第二年度工作項目完成後，我們可初步提供反事實干擾選項之選擇題生成、優化後之句子等級干擾選項生成與基於ODQA方式取得之字詞等級干擾選項生成。於第三年度中，我們則規劃「基於NodeCentrality考量主題涵蓋性之選擇題題組生成」與「生成難度可控之干擾選項生成研究」。如同前言所述，一個理想的選擇題出題系統應該考慮題組生成，整體題目之主題涵蓋性，對此我們也將嘗試基於知識圖譜涵蓋的方式考慮節點centrality重要度方式來進行題組之生成。同時我們也將嘗試題目難易度之控制生成研究。我們的想法是當自動生成成為可能後未來的出題應該是個人化模式；針對個人的學習狀況進行測驗，因此我們規劃「個人化與生成難度可控之干擾選項生成研究」進行探討。第三年度，我們也預計投入人力進行系統的實作開發與整合。以下茲針對擬訂之研究議題其問題挑戰與1.基於語言模型與知識圖譜之Text2Text干擾選項生成模型近年來，語言模型除了使用CandicateGeneratingandRanking架構的MaskedLM之外，使用Encoder-Decoder架構的Text2Text模型於生成任務上也有很好的成果，因此我們的想法是把干擾選項生成Model視為一個Text2Text任務，嘗試讓模型直接生成干擾選項。且使用Text2Text模型生成Distractor有別於用MaskedLM生成Distractor，我們希望模型能生成的Distractor有以下特性(1)非單詞之Distractor:目前在Distractor生成上多為單詞生成，要生成一段敘述/句子的干擾選項還沒有很好的做法。(2)特定domain之MCQ:目前的MCQ多為英文測驗，若想針對特定領域出選擇題，使用MaskedLM生成的Distractor會因語料庫限制而為達到上述的兩種目標需求，我們設Q為輸入之問句，A為輸入問句之答案，並將Q與A字串接字串當作輸入，訓練模型輸出一連串問句Q的干擾選項字串D=(d1,d2,d3)。模型架構如圖2所示，我們將「哪一種技術最適用於嚴重特殊傳染性肺炎之病原快速篩檢?」與「聚合￿連鎖反應」接在一起輸入至Text2Text模型，而模型生成了「重組DNA技術」、「電腦斷層掃描」與「組織培養」三個干擾選項，三個選項皆為醫療檢測方式且都不是單詞，符合如前言所述，我們也預計探索使用知識圖譜來強化生成干擾選項的能力。如Figure8的圖例，我們可以使用語言模型結合醫療知識圖譜，利用給定的文本從知識圖譜中萃取有包含這些topicentities以及相鄰的nodeentities的子圖，再分別將文本與子圖輸入至語言模型和GraphNeuralNetwork(GNN)，讓模型學習找出關鍵的資訊與強化network中重要entitynodes跟edges的權重，藉由GNN的額外資訊與語言模型能力生成更好的選項。Figure8為我們規劃做法的概觀。主要分兩階段進行：(1)關鍵字子圖與GNNEmbedding萃取與(2)GNNEmbedding與LanugageEmbedding間之FusionEncoder生成。具體而言，第一個階段是將從知識圖譜裡萃取相關的知識圖譜子圖。我們一開始先使用NERModel抓取題幹中的entities，再透過這些entities透過知識圖譜去尋找entitynodes以及所有跟entitynodes相鄰3-hop內的bridgenodes，將檢索出來的結果當成知識圖譜子圖，並在後續第二階段提供以Figure9的圖例，我們先將問題透過NERModel斷詞得到”白血病”、”發病”、”器官”與”發病原因”當作entities，再透過這些entities當作關鍵字透過Retriever從知識圖譜萃取Figure8:結合知識圖譜與語言模型生成之構想架構圖與這些詞相關的node和edge，如利用”白血病”與”器官”能找到”肝臟”、”骨髓”、”肺部”等等entities，我們能利用這些entities來當作後續的生成選項。第二個階段則將文本與第一階段得到的知識圖譜子圖分別輸入至語言模型與GNN進行訓練，訓練目標為將Input的[MASK]Token還原來產生干擾選項，先將語言模型的Input額外新增[MASK]Token，經過N層LMLayer做self-attention後，透過FusionEncoder再結合知識圖譜的資訊與原本的文本去生成選項，透過以上的手法，來強化干擾選項之生成。2.考量題幹可用性與干擾選項間關聯之Retrieve-and-Generate架構優化如同研究目的所述，本研究項目著眼於優化現有Retrieve-and-Generate生成架構。我們•文章檢索器過濾:在Retrieving-and-Generating方法中Retriever會檢索回傳含有主題字詞的文章，但有部分文章並不適合當作問句生成模型的輸入，那些文章含有太多的雜訊，我們想嘗試在這步驟先過濾一些文章。我們的想法是利用詞向量中的相似詞再次檢索Retriever回傳的文章，把過於不相關的文章濾掉，減少問句生成模型輸出過於複•生成題幹品質自動過濾：問句生成模型輸出的問句，會再交由使用者挑選，但為了更好的效果，我們想嘗試利用語言模型做初步的篩選，減少人工挑選時間。我們想嘗試利用ODQA模型來過濾問句，將問句生成模型輸出的問句輸入至問答模型後，根據問答模型的回答結果與信心分數來決定該問句是否合適當作MCQ。如Figure10所例，我們將生成之問句再次投入ODQA模型，透過答案選項之信心度來為問句生成過濾之基Figure10:Unanswerablequestionsolution•考量干擾選項間關聯之干擾選項挑選:如前言所述，一個可以提升選擇題生成品質￿考量干擾選項彼此之間之難易度。而在Retrieving-and-Generating方法中，生成干擾選項的方式為取出詞向量模型中與主題字詞相似之詞，使用相似度前三名當作干擾選項，因此有選項過於相近而造成問題太難的可能。為做改善，我們取出所有相似詞的向量與主題字詞向量計算相對距離，設定各個選項與答案的相似度範圍(距離範圍)，再從中隨機挑選相似詞，讓干擾選項彼此之間難易度有所區分以提升試題之鑑別度。•基於語言模型之干擾選項候選生成:雖然詞向量模型生成distractor過程迅速，但如果有主題字詞沒有被詞向量模型看過或有主題字詞一字多意，就無法生成適當的干擾選項。這個問題我們想嘗試使用MaskedLM來解決，我們將Retriever回傳的文章中含有主題字詞的段落記錄下來，移除主題字詞後輸入MaskedLM，再從MaskedLM輸出的候選字中選出distractor，使用這個方式生成的干擾選項不會被上述的缺點受限，也更Figure11:OptimizedRetrievingandGeneratingFramework3.基於Masked-LM之反事實干擾選項生成研究本研究項目為基於Masked-LM技巧來生成反事實干擾選項。我們需要先查找與事實相符之敘述，再修改成反事實干擾選項。我們預計之作法為：首先讓使用者先輸入考題關鍵字，再使用retriever查找相關文章，其目的為從相關文章中選取與事實相符之敘述。接著，將retriever檢索到的文章回傳給使用者，由使用者選取與事實相符之敘述。並按照以下之步驟進行Masked-LM反事實干擾選項生成。為了抽換專有名詞，我們先使用命名實體與事實相符之敘述中的所有專有名詞，以下圖為例”腎小管藉壓力差方式再吸收葡萄糖回微血管”，透過命名實體模型會萃取出專透過命名實體模型，我們會萃取出多個與事實相符之敘述中的專有名詞，我們需要選擇一個專有名詞行抽換。我們認為該專有名詞在相關文章中的重要程度越高，越高機率是考題中的重點。為了確保抽換專有名詞的品質，我們使用BM25來查看各個專有Figure12:AutoEncoder模型與AutoRegressive模型做MaskLM3.使用Maskedlanguagemodel模型進行改寫Maskedlanguagemodel，簡稱為MaskLM，該任務的會將輸入中的部分內容遮擋起來，模型會根據其他未被遮擋的內容，預測出被遮擋的內容或與其擁有相似語義的內容，現有的MaskLM模型分為兩種，分別為AutoEncoder框架的MaskLM模型與AutoRe-gressive框架的MaskLM模型，經典模型如BERT與BART。針對MaskLM任務，兩種模型進行預測的方式也有所不同，AutoEncoder框架的模型會預測出字，而AutoRe-gressive框架的模型會預測出字、詞或是句子，詳細範例如Figure12。根據抽換專有名詞的需求，我們將使用Autoregressive模型做MaskLM任務來抽換重要的專有名詞。然而模型預測出的內容眾多，有可能是原先遮擋的內容、遮擋內容的同義詞或是與遮擋內容擁有相似語義的內容。理想的情況下，模型預測的內容最高機率為原先遮擋的內容，若我們選擇原先遮擋的內容或是遮擋內容的同義詞進行抽換，會造成與事實相符之敘述未改變，便無法生成反事實干擾選項。為了避免此狀況發生，我們使用NLI任務中的換句話說(MRPC)任務來檢測其是否為干擾選項。換句話說任務主要在判斷兩句話的語意是否相同，為了避免MaskLM模型選用同義詞進行抽換，即模型生成的干擾選項仍是正確的，我們將與事實相符之敘述與生成之干擾選項作為模型的輸入，若模型認為兩句話具有相同語義，我們會讓MaskLM模型預4.基於知識圖譜強化之反事實干擾選項生成於本研究項目中，我們嘗試利用知識圖譜提供語言模型額外資訊達成反事實干擾選項生成。我們的構想如Figure4所示，包含以下步驟。首先我們會讓使用者先輸入考題內文，再將內文使用斷詞或NER模型萃取entity，利用這些entity於知識圖譜中查找相關子圖，子圖必須包含內文中所有的entitynodes以及所有跟entitynodes相鄰節點，以此為知識生成之基礎。接著我們將檢索到的子圖內的nodes與edges萃取成三元組，每筆三元組的結構是以node,edge,node組成。如同圖中所示，每一筆三元組描述一基本事實陳述，如導體可以3.將與事實相符之敘述修改為反事實干擾選項為了將與事實相符之敘述更改為反事實干擾選項，我們將前一步驟所萃取的三元組與考題內文當作作語言模型的輸入來生成反事實干擾選項。我們預計透過生成語言模型與輸入給定的問句和三元組來生成與問句情境相符合的反事實干擾選項，以FigurejQuery36307282536263739197_1676211194201為例，生成語言模型在給定關於與導體相關的題目敘述以及從知識圖譜萃取與導體相關的三元組後，模型藉由這些三元組推斷其中的關係，比如”導體可以導電->石墨是導體->筆芯是由石墨製成”，從上述的三元組推斷出”筆芯可以導電”的事實，也可以改寫成”筆芯不能導電”的反事實干擾選項。5.使用Open-DomainQuestionAnswering錯誤結果之干擾選項挑選研究如前言所述，我們預計使用ODQA模型所回答之錯誤結果來當作干擾選項(如Figure5)。以我們先前曾開發https://agri.nlpnchu.org/之農業問答系統為例，我們發現以上想法所面臨的挑於ODQA的架構下，可能會遇到ODQA正確回答所下達之問題，其回傳結果皆為可行之答案，也因此無法被使用來當干擾選項，對此，我們探索兩種作法來進行調整。–1.使用BM25，且刪除尋找出來文章的正確答案在使用BM25的情境下，因為是使用關鍵字比對，所以會找出許多跟正確答案相關的文章，我們將找到文章的正確答案刪除，這樣這個文章會變成絕對回答不出正確答案的文章，可是因為Reader對文字有一定的理解能力，並且具有關鍵字的文章中也很可能會有，他會從剩下可能的答案中，尋找一個最相近的選項，這樣就可以較容易生出干擾選項。如Figure13所示，我們將答案”2,4-D”從文章中直接刪除後，再交給Reader進行閱讀，藉此確保正確答案不會被回傳。Figure13:基於ODQA架構之Retriever弱化方法–2.使用DensePassageRetrieverDensePassageRetriever[16]，DensePassageRetriever可以將資料庫的文章轉為向量的方式表示，在搜尋的時候也將Query轉為向量，並且比對兩個向量的相似度，這樣就可以找出相似的文章，可是我們想說，可以利用DPR找尋文章相似度的這一點，理論上相似的文章跟Query算出來的相似度會是最高的，我們可以將相似度高的文章拿出來，並且查看有沒有答案出現在上面，如果沒有答案，我們就將他丟給Reader，因為我們覺得如我相似度高，就代表Retriever認為裡面有答案，可是裡面並沒有含有我們的正解，所以這篇文章就含Retriever認為是答案，但是實際上不是答案的文字，所以這時候我們將這篇文章傳給Reader，就會有比較高在我們使用baseline的情境中，有時候也會出現不相關的答案，例如在??中，問模型關於殺草劑的問題，會出現的人力除草，人力除草跟題目所問的殺草劑名稱是不相關的，所以我們需要用一個方法把他過濾掉。在這種情況下，我們如果加入正確答案當作Query一起輸入Retriever，就會增加模型找到正確passage的機率，對我們想找跟正確答案不相關但是相似度較高的Passage會產生反效果，所以我們採用的是從Reader輸出之後把他過濾掉的方法。當Reader從Retriever拿到Passage之後，我們會將正確答案去除，之後再將剩餘的答案丟入一個LanguageModel作判斷，如果他跟答案的距離過於遙遠，就放入NegativeList，如果是相關的就放入PositiveList，之後要挑相關答案的時候可以從PositiveList裡面挑選。6.基於對比式學習之干擾選項生成模型強化ContrastiveLearning的設計上需要考量如何去設定PositiveSample以及NegativeSample。在我們的目標中，因為不希望模型會重複的生成出相同意思的句子，所以我們預計使用同義的句子當作NegativeSample，而資料集中的干擾選項可以被當作PositiveSample。然而，由於我們的資料集中並沒有NagativeSample的選項，也就是同義於模型生成的選項。對此，我們的想法為透過迭代生成來產生我們需要的NegativeSample。具體而言，如Figure14所舉例，在DistractorGenerator的第一次生成中，我們會輸入Question以及Answer使模型生成出干擾選項。而在第二次迭代中，我們除了會輸入QuestionAnswerPair外，還會再加上在上一次迭代中生成的干擾選項，以提示模型先前已生成的Distractor的資訊。對於第二次迭代的輸出來說，第一次迭代輸出的Distractor，即為它的NegativeSample，因為DistractorGenerator必須要生成出與上一次迭代生成的Distractor意義相異的句子，所以它們之間的SentenceEmbedding不能太過接近。Figure14:IterationGenerater示意，在第一次的迭代中生成的Distractor會作為第二次迭代的輸入，以提示模型不要生成出類似的DistractorFigure15:對比式學習於干擾選項生成之預想架構我們所規劃之整體框架，如Figure15所示。我們使用多次迭代的方式來生成Distractor，對於第一次迭代來說，我們會從其他in-batch的data中取得Sentence當作NegativeSample。而在第二次以後的迭代來說，除了其他in-batch的Sentence以外，我們還會將此次迭代前所生成的Distractor加入我們的NegativeSample中。透過NegativeSample和模型輸出的Distractor的dotproduct可以計算出ContrastiveLoss，此外我們也會加入Distractor和Label計算的CrossEntropyLoss，來計算出我們的TotalLoss15。ContrastiveLoss定義如下:x為生成的distractor，p為資料集中的Distractor(Label)，N為NegativeSampleSet，N={n1,n2,...nj}，其中包括先前迭代生成的Distractor以及其他in-batch的選項。7.基於NodeCentrality考量主題涵蓋性之選擇題題組生成如同前言所述，本研究議題之目標為考量主題涵蓋性之選擇題題組生成。因此，我們預計根據使用者所指定的主題，按Figure16架構規劃來進行題組生成。1.透過知識圖譜子圖萃取來涵蓋所指定主題之內容：藉由知識圖譜上NodeCentrality方式來萃取關鍵字，以為題幹之生成。我們的基本想法為在知識圖譜上我們定位出與使用者輸入之主題關鍵字相關之結點後，我們可以試者套入PageRank或者其他圖形上計算NodeCentrality重要節點挑選，並以重要節點為題幹生成之主軸。2.透過上一步取得之重要答案片段後，進行問題之生成。並生成較多的問題數量以為CandidateQuestionPool，而後將問題將題組生成問題設定一成為一選擇最佳化議題(Optimization)：根據主題生成大量的單一題目，而後定義UtilityScore來計算個別題目組合後之優劣，來挑選最佳化的題組以為輸出。於本研究項目中，我們預計探索以下兩種生成難度控制方式。•調整Masked-LM比例之反事實選項難度控制：現有的Masked-LM模型會將重要的專有名詞進行遮擋，模型會透過未遮擋的內容對該專有名詞進行預測。理想的情況下，當我們僅對該專有名詞進行遮擋時，模型可成功地預測出遮擋的專有名詞或語義相近的專有名詞，這不僅是因為MaskLM模型夠強大，也是因為未遮擋的內容提供模型足夠多的資訊讓模型預測，若未遮擋的內容提供的資訊不夠多，則模型預測出的內容有可能會與遮擋的內容語義相差很多。我們根據Masked-LM模型的此特性對題目的難易度進行控制，若我們將多個專有名詞進行遮擋，再讓模型預測時，模型會因為提供的資訊不足，而預測出語義較不相近的專有名詞，即較不可能的答案。以下表為例，我們可以觀察到當[Mask]越多時，我們要判斷該事實是否正確就越形容易。[Mask]借壓力差吸收[Mask]回微血管[Mask]借壓力差吸收[Mask]回[Mask]•調整ODQA檢索文章之難度控制：由於我們的基礎架構為基於文章檢索(DocumentRetrieving)來取得包含使用者所給定的關鍵字之文章，並使用該文章與關鍵字為問題生成模型之輸入，來生成選擇題問句。我們若挑選較為冷門之文章進行生成，則預期可以生成較為困難之題幹。而就干擾選項而言，我們可藉由調整輸入給閱讀理解模型之文章來產生難度較高的錯誤。具體而言，給定一有包含正確答案A之文章C，我們先將答案片段A於文章D中刪除，而後將其輸入至閱讀理解模型，藉此產生錯誤之閱讀理解結果，以這樣的錯誤結果當Distractor預期將有較高之誘導能力。相類似的想法也可以套用為挑選Retriever認為相關度比較不高的文章以為閱讀理解模型之輸入。此外，就生成題目之廣度與深度而言，我們可藉由控制取得之文章來調配題組之廣度與深度。例如，若我們以單篇或少數篇文章進行生成，我們所生成的主題題組針對測驗內容而言，應該可產生知識性較為深入之試題。而若針對較多篇文章，則應可產生範圍較廣本研究計畫將以三年為期，共八項研究議題進行，各研究產出如上文所示，並整合各技術成為一選擇題自動生成系統，預期的展示情境為：我們可於系統直接輸入一知識範圍，系統將利用我們所開發之技術先進行自動關鍵語萃取，而後進行各種類型的選擇題出題並考量題組關聯性。我們也將彙整各研究議題中所發展之技術為一個成為公開原始碼專案，以為後續相關研究之用。並將計畫執行過程所累積的資料，公開希望能吸引更多研究者之投入。對於學術研究、國家發展及其他應用方面預期之貢獻自然語言生成已是未來深度學習領域之熱門研究重點。透過本研究計畫之投入我們將可進行先驅之探索，建立學生團隊與自然語言處理與生成之能力，並持續過去之閱讀測驗自動出題研究成果，以知識類型選擇題自動出題為主要系統開發目標為例，希冀能為國內外探索不同的自然語言生成應用服務。此外若可順利完成也可以成為一成功範例利用人工智慧輔助教師進行測驗的客製化與個人化學習。對比於題庫類型的方式，透過我們技術我們將可動態產生各式各樣不同之考題為每個人量身定做難度適中之考題。就此一觀點出發，我們預期本計畫除學術研究價值外，同時也兼具商業價值，也應可成為EdTech(教育科技)領域之一環。再者，疫情改變了學習與教學的模式。同步與非同步之線上教學已成為教學模式常態之一。線上教學帶來了好處，但也帶來了顧慮。其中，學習的專注度應是諸多顧慮中主要令人擔憂的一環。隔著螢幕學生學習是否專注？目前的教學內容學生是否已跟上進度？等等問題是線上教學學習專注度所在意的問題。對此，我們認為適時與即時地檢測學生學習成效，來提醒學生與教師目前之學習成效，應能適度緩解線上學習專注度之顧慮。因此，我們也常見於MOOCs課程於一個授課單元結束後適時地給予測驗(多數以選擇題測驗)，來確認學生學習成效。然而，人工設計試題耗費心力，一成不變全班同學皆相同試題也可能透過即時訊息(instantmessage)之分享而取得答案，造成檢測之無效。對此，我們認為隨著深度學習技術之快速演進，結合自然語言(NaturalLanguageGeneration)生成技術來進行自動出題技術已成為可能；透過自然語言生成技術自動化出題，產生多樣化的試題，來進行個人化的檢測，將有除了可以增進研讀論文、撰寫程式、架設系統以及撰寫論文的基本能力之外，參與本計畫之研究人員，將可以學習到深度學習與自然語言生成技術之基本觀念、實際應用以及相關的核心技術，在這些基礎之下，透過積極的研究與討論，將可以提升參與計畫人員思考問題與解決困難的重要能力。我們預期可獲得具體技能如下:(1)設計與建立一個深度學習模型之能力，(2)整合不同先進技術之能力，(3)自然語言生成技術之瞭解，(4)資訊檢索相關知識與軟體套件之暸解，(5)佈建深度學習模型於WebScale之能力。"}, "1676211373.044206": {"單位": "", "標題": "(ㄧ)、研究計畫之背景與目的", "作者": "", "發布日": 0, "摘要": "", "全文": "自然語言生成(NaturalLanguageGeneration)將是未來深度學習領域之熱門研究議題。2022年年末ChatGPT語言模型所帶來的示範性與展示效果預期將吸引更多研究目光。而個人於中興大學之研究團隊近年於自然語言生成之耕耘已建立初步研究成果。在過去朝著自然語言生成方向前進，我們探索自動生成試題之研究方向前進。於前期研究計畫「基於深度學習語言模型之閱讀測驗自動化生成研究109-2221-E-005-058-MY3」中，我們著眼於基於深度學習模型進行閱讀測驗(ReadingComprehensionTesting)題型之自動出題。在計畫執行過程中，我們•Ying-HongChan,Yao-ChungFan:KeywordProvisionQuestionGenerationforFacilitatingEducationalReadingComprehensionPreparation.INLG(InternationalNaturalLanguageGen-erationConference)2022.•Ho-LamChung,Ying-HongChan,Yao-ChungFan:ABERT-basedDistractorGenerationSchemewithMulti-taskingandNegativeAnswerTrainingStrategies.EMNLP(EmpiricalMethodsinNaturalLanguageProcessing)Findings2020.•Shang-HsuanChiang,Ssu-ChengWang,Yao-ChungFan:CDGP:AutomaticClozeDistractorGenerationbasedonPre-trainedLanguageModel.EMNLP(EmpiricalMethodsinNaturalLanguageProcessing)Findings2022.•Hsien-YungPeng,Ho-LamChung,Ying-HongChan,Yao-ChungFan:MisleadingInferenceGenerationviaProximalPolicyOptimization.PAKDD(AdvancesinKnowledgeDiscoveryandDataMining-26thPacific-AsiaConference)2022此外我們也將前期計畫之自動出題研究成果進行實作，可於以下兩個網址進行實測。•問句生成QuestionGeneration：給定文章、指定片段，透過自然語言生成模型進行問句生成。目前系統可於https://classic.queratorai.com/進行實測。•克漏字干擾選項生成DistractorGeneration：此一部分針對給定之文章進行克漏字誘導選項生成。目前系統可於：https://cdgp-demo.nlpnchu.org/進行實測。基於過去之研究成果累積，本計劃延續自然語言生成技術於自動出題之研究成果，擬定知識類型選擇題(MultipleChoiceQuestion,MCQ)自動生成為目標。以四技二專統一測驗考試為例(如Table2所示)，知識類型選擇題將包含題幹與四個選項(其中包含正確答案與干擾選項)，主要針對某一知識範圍進行測驗。我們認為知識範圍選擇題之自動生成可分為兩部分：(1)問題題幹(Stem)生成與(2)干擾選項(Distractor)生成。題幹生成主要目標為選擇題問題本身之生成，而干擾選項則著眼於誘導選項之生成。知識類型選擇題為學習測驗最常見的測驗模式之一。對比於閱讀測驗生成，知識類型選擇題的自動生成，將可減輕教師準備個人化教學測驗之負擔，也為學生提供更有效率之學習檢測方式。以現有我們成果為基礎，一個基本之自動選擇題出題系統作法為(如圖1)所示：我們可利用資訊檢索技術(如BM25Retriever)，從給定之文集資料庫(目前使用2021中文維基百科所有中文文章)中檢索主題字詞相關文章後，結合前期計畫所開發之問句生成模型(QuestionGeneration)[4]生成題幹，而後再利用詞向量模型(例如word2vec模型)找出相近字詞以為干擾選項。我們將這樣的構想先行實作一系統(稱之為Retrieve-and-GeneratePipeline)，目前此系統可於https://mcq-demo.nlpnchu.org/使用。使用者可藉由輸入主題相關字詞，選擇題生成系統將自動生成輸入字詞相關之選擇題結果。舉例而言，我們以「骨髓、白血病、淋巴」為主題字詞輸入至系統。我們挑選較佳之成果顯示於Table1。自動生成選擇題AutomaticGeneratedMCQQ1:哪一種癌症種類的統稱中可以看出它會造成不正常白血球的大量增生?(a)白血病、(b)乳腺癌、(c)腎病、(d)帕金森氏症Q1:白血病的發病原因通常是發生在哪一個器官上?(a)肝臟、(b)淋巴、(c)甲狀腺、(d)骨髓Q:白血病是最常見的兒童癌症，其中80%的病例是慢性的哪一種性病變?(a)淋巴結、(b)微血管、(c)淋巴、(d)黏膜Table1:目前選擇題自動生成結果範例。目前系統可於https://mcq-demo.nlpnchu.org/。目前我們挑選較佳之生成結果顯示於此Table中。建議審查委員自行輸入關鍵字詞組，來觀察我們Figure1:Retrieve-and-Generate基本架構。本例中我們以”骨髓、白血病、淋巴”為關鍵詞進行文章檢索，而再使用”骨髓”為答案，分別生成問句與干擾選項。從展示之結果中，我們發現自動選擇題出題之可能性。但仔細觀察會發現現有系統離實際應用仍有空間須待努力。也因此觸發本期研究計畫之提出。具體而言，我們認為選擇題自•句子等級之干擾選項生成(Sentence-LevelDistractorGeneration)首先，關於干擾選項的部分，若仰賴詞向量模型來產生干擾選項，我們所能取得的干擾選項便僅限於單詞(Word-Level)等級。從統測試題中，我們發現許多題目之干擾選項屬於句子等級(Sentence-Level)。以Table2中Q3與Q4選擇題為例，我們可以看到選項皆為句子等級之描述，此為目前基於詞向量來產生干擾選項的做法所不能。•反事實干擾選項生成(Counter-FactualSentenceGeneration)我們觀察到選擇題題型中，有著下列何者為錯誤或正確類型之考題。其干擾選項是一個較長的描述句。以Table2為例，Q5與Q6為分別詢問下列選項何者正確/錯誤。此題型，為變像之是非題。除了句子等級生成外，核心技術牽涉的是反事實(Counter-Factual)句子之生成。如何生成此類型之句子以為干擾選項也將是研究重點所在。•選擇題生成品質提升(MCQQuestionQualityImprovement)此外，我們也看到retreive-and-generatepipeline的做法，所產生的題幹本身有時是一個過於空泛之問題(例如，下列何種疾病為兒童常見之癌症？)或者並非語意正確之語句(例如，中樞性的什麼症狀是缺乏抗利尿激素的?)。如何有效判斷生成不良之語句，減低人工進一步過濾/挑選題目之負擔也是需進行研究之議題。此外，基於詞向量之干擾選項作法也有待商榷;隨機從相近詞向量挑選distractor，是否能確保distractor之誘導品類型I:單詞等級的干擾選項Word-LevelDistractorsQ1：淋巴器官是人體防禦系統中重要的一環，可提供淋巴細胞發育或活化所需要的條件與狀態。下列何者是淋巴球進行防禦與活化的器官？(a)骨髓(b)胸腺(c)肝臟(d)脾臟Q2：健康人體感覺神經傳入大腦的訊息大部分經過轉接，(a)視丘(b)下視丘(c)中腦(d)橋腦類型II:句子等級的干擾選項Sentence-LevelDistractors(a)無就巢性(b)體型大(c)著肉多(d)耐粗食Q4：下列何者為「聯合國國際糧農組織」(FAO)成立最重要的目標及首要任務？(a)消除及戰勝飢餓(b)聯絡及交流農業資訊(c)自然環境的永續及保護(d)實現國際間的公平貿易類型III:句子等級且與事實相違背的干擾選項Counter-FactualDistractorsQ5：有關人類上皮細胞的細胞週期之敘述，下列何者正確？(c)後期，分離的染色分體往細胞中央移動(d)末期，細胞膜內凹且細胞質開始一分為二Q6：有關一健康成人泌尿系統功能的敘述，下列何者錯誤？(a)腎臟每日約產生濾液180公升及尿液1.5公升(b)腎小管藉壓力差方式再吸收葡萄糖回微血管(c)腎臟藉排尿量多寡來維持體內水分的恆定(d)若出現尿毒症狀時，可使用血液透析方式進行治療Table2:四技二專統一測驗選擇題類型概觀。Q1與Q2擷取自110學年度護理基礎生物考科，Q3與Q4110學年度農業群農業概論考科，Q5與Q6取自110學年度農業群基礎生物考科質？也許所挑選的distractor並不具誘導性，使得受測者很輕易地可挑選出正確答案，或者挑選到答案的同義詞，造成一題有多個正確答案。根據先前計畫執行訪談教師所知，於測驗時為考量受測全體鑑別度，教師出題時會考量干擾選項彼此之間之難易度；通常三個干擾選項中，會有一個離正確答案較遠之干擾選項(誘導性較低)，一個次之，一個最為困難最具有誘導性。藉此來有適當的鑑別度。目前我們直接使用詞向量之作法並未考量此設計。•主題題組生成(MCQQuestionGroupGeneration)目前我們的系統僅為單一關鍵字詞之出題。我們認為一個良好的自動出題應該是以題組為單位進行出題；使用者指定一主題知識，系統根據此一主題知識範圍進行5-10題之選擇題自動生成，題目之間也應該彼此考量。目前Retreive-and-Generatepipeline的做法，我們使用使用者給定的關鍵字為答案，進行題目之生成。然而比較好的做法應為將使用者給定的關鍵字集合為知識範圍之指定，系統根據該範圍自動挑選重要概念關鍵字進行答案挑選，而後再進行選擇題題組之生成。針對上述之研究挑戰，於本研究規劃中我們擬定下列八項研究議題進行探討。•研究項目ㄧ：基於語言模型與知識圖譜之Text2Text干擾選項生成模型(DistractorGenerationbasedonText2TextLanguageModelandKnowledgeGraph)Figure2:Text2Text干擾選項生成架構Figure3:基於Masked-LM之反事實干擾選項現有之Distractor生成研究多是基於CandicateGenerating-and-Ranking架構，先透過知識庫(如Wordnet)或者透過MaskedLM的方式取得候選字後，再適當進行候選詞特徵排序以為最終干擾選項。對此我們有不同之想法，我們預計嘗試把干擾選項生成設定為一個Text2Text任務；如圖2所例，預期的輸入為題幹Q與答案A之串接字串，透過ModelEncoder與Decoder轉化輸出為一連串之干擾選項字串“重組DNA技術[SEP]電腦斷層掃描[SEP]組織培養”。我們透過語言生成模型直接將輸入對比轉換為輸出。此外，透過此一Text2Text方式，生成句子等級的干擾選項也將為可能。再者，近年來語言模型研究趨勢除了將模型參數量與訓練資料持續增加外，引入知識圖譜來強化生成模型於常識理解與推理能力也成為研究社群的關注所在[5][13]。也因此，探索整合知識圖譜於選擇題自動生成也因此成為我們規劃之研究項目。希冀藉由整合知識圖譜來提升干擾選項之誘導性與題幹生成之正確性。•研究項目二：基於Masked-LM之反事實干擾選項生成研究(Counter-FactualSentenceGenerationbasedonMasked-LM)如同前言所述，“下列何者正確/錯誤”題目為常見之類型選擇題題型。對此，我們預計嘗試以下之作法。首先，我們假定可取得正確敘述之語句S。我們先針對語句S中的字詞進行命名實體辨識(NamedEntityRecognition,NER)，其後再利用語言模型針對所取得之命名實體進行WholeWordMasking(WWM)，填入機率較低之填空結果，藉此生成反事實描述之語句。如Figure4所例，我們將所給定之事實描述，藉由NER方式挑選”葡萄糖”一次進行Masked，再藉由語言模型推測該Masked字詞，並挑選機率相對較低之預測填入。藉此創造出反事實之陳述句。然而，如何確保不挑選到同義詞？要進行多少個字詞之Masked？如何自動從主題文章中挑選事實陳述句以進行改寫？等等•研究項目三：基於知識圖譜強化之反事實干擾選項生成(Counter-FactualSentenceGen-erationbasedonKnowledge-GraphAugmentedLanguageModel)針對反事實干擾選項生成之議題，除了研究項目二之Masked-LM作法外，我們另一個構想為利用知識圖譜來強化Counter-Factual語句之生成。如Figure4所示，我們基本想法為先萃取與題幹相關聯之知識圖譜子圖(LocalSubgraph)並取得知識三元組，以此輸入至語言模型中，訓練語言模型反向改寫三元組語句，以此為反事實干擾選項。然而，此模式之訓練資料該如何取得？如何從知識圖譜上萃取子圖，並生成合適三元組？如何引入GraphNeuralNetwork來萃取結構？為此研究項目需進行探討之工作內容。•研究項目四：基於對比式學習之干擾選項生成模型強化(DistractorGenerationModelbasedonContrastiveLearning)我們過去的研究經驗[8]與相關研究[35]皆指出當使用Generative語言模型來生成句子Figure4:基於知識圖譜強化之反事實干擾選項生成構想層級之輸出時，模型經常會產生語意之相同，但字面的結果。如Table3所示，選項A跟C雖然字面敘述上不同，但實際上都是描述同樣的意思。生成重複選項將導致選擇副交感神經興奮會導致使心跳加速，血壓上升Table3:模型生成的誤導選項重複出現的範例對此，我們預計探索對比式學習(ContrastiveLearning)[7][35]來提升生成品質。對比式學習於自然語言之應用中主要想法為透過選取PositiveSample以及NegativSample使得模型去學習到生成句子跟同義句子的相同以及跟異類句子的不同之處，這樣我們可以使用重複選項當作我們的NegativeSample，然後資料集中的干擾選項當作PostiveSample，期望模型可以學習避免生成出重複的誤導選項。因此，我們預計探索對比式•研究項目五：使用Open-DomainQuestionAnswering錯誤結果之干擾選項挑選研究(Us-ingIncorrectAnswersfromODQAasDistractors)而除了利用生成模型產生Distractors外，我們發現利用Open-DomainQuestionAnswer-ing(ODQA)模型所產生的錯誤結果是一個值得探討的干擾選項的取得方式。具體而言，現有的ODQA模型主要包含一個Retriever與一個Reader，Retriever負責檢索相關的文章，而Reader利用檢索出來的文章利用閱讀理解模型(ReadingComprehensionModel)來找尋答案片段。由於ODQA並非完美，因此也會有錯誤結果之回報。但我們發現錯誤結果與正確答案並非天差地遠，閱讀理解模型所回報之結果通常也與答案接以圖5為例，當我們下達Q：「下列何者為目前世界上應用最廣的殺草劑，其具備有生長素之特性，常用於闊葉雜草防治，亦為選擇性殺草劑的一種？」，ODQA系統將取得圖5中右半部之答案：「萌前除草劑、2,4-D、非選擇性除草劑、10殺丹粒劑」。其中，2,4-D為正確答案，餘者為錯誤答案。然而我們覺得這些錯誤答案，可以當作Q之干擾選項來使用。有別於基於生成模型研究項目，利用ODQA模型之錯誤答案來當作干擾選項應可提供不同思路進行干擾選項生成。這樣的想法乍看可行，但仍有若干技術點需進行探討。諸如，如何控制ODQA模型之錯誤？我們需要在Reader部分弱化Reader的能力，或者弱化Retriever能力，來製造ODQA的錯誤？如何從錯誤回報結果挑選干擾選項？這些皆是尚待深入研究之議題。也因此觸發本研究項目之規劃。Figure5:利用ODQA模型之錯誤答案來當作Distractors•研究項目六：基於NodeCentrality考量主題涵蓋性之選擇題題組生成(MCQGroupGenerationconsideringTopicCoveragebasedGraphNodeCentrality)上述所規劃的選擇題生成做法，主要針對單題進行生成。若要產生多題/題組考題，就是重複相同流程，獨立產生個別考題後，整合為一題組。然而，題組生成做法應當考慮題與題之間之關聯性與互補性。因此，我們認為比較好的做法應當為根據使用者所指定的主題，透過(1)知識圖譜子圖萃取來涵蓋所指定主題之內容與(2)透過主題重要概念萃取：藉由知識圖譜上NodeCentrality方式來萃取關鍵字，以為題幹之生成。此外，如何根據主題互補性，來挑選個別生成之題組，用以組成選擇題題組也是需進行•研究項目七：考量題幹可用性與干擾選項間關聯之Retrieve-and-Generate架構優化(Op-timizingRetrieve-and-GenerateFrameworkconsideringStemUtilityandDistractorCorrela-tion)本研究項目著眼於優化現有Retrieve-and-Generate架構設定。Retrieve-and-Generate架構雖然簡單，但我們可以發現其於單字等級的干擾選項生成已有一定的可用性。我們預計可優化之處為：(1)我們發現引用現有系統於題幹生成時，會有題幹生成結果不佳的狀況。對此，我們發現其中的一個原因為Retriever所檢索之文章品質不佳(文章過短或者與主題不相關)，導致不良之題幹生成。因此如何改善檢索機制來挑選較佳的文章用以進行生成，便是值得探討的課題。再者，題幹生成不佳也有部分原因是來自於問句生成模型本身之生成錯誤，如何有效過濾生成不佳的題幹也因此是一個可需進行研究之項目。(2)如同前言所述，干擾選項之挑選應該考量彼此間之關聯，而非獨立挑選。如何納入此觀點來進行干擾選項之生成也因此成為預計關注之議題。又再者，我們基於word2vec方式之字詞等級干擾選項挑選是否可以引入語言模型之詞向量來求取•研究項目八：生成難度可控之選擇題生成研究Difficulty-ControlledMCQGeneration發展自動化生成考題之可能願景為：針對個人化學習自對應生成難度適中之測驗，以增加學習與檢測之效率。因此，如何針對現有技術探討生成結果難度控制也因此成為一可探索之研究議題。由於我們的基礎架構為基於文章檢索來取得包含使用者所給定的關鍵字之文章，並使用該文章與關鍵字為問題生成模型之輸入，來生成選擇題問句。我們若挑選較為冷門之文章進行生成，則預期可以生成較為困難之題幹。而就干擾選項而言，若我們基於研究項目五「使用Open-DomainQuestionAnswering錯誤結果之干擾選項挑選研究」，利用ODQA來取得干擾選項之作法。我們可藉由調整輸入給閱讀理解模型之文章來產生難度較高的錯誤。具體而言，給定一有包含正確答案A之文章C，我們先將答案片段A於文章D中刪除，而後將其輸入至閱讀理解模型，藉此產生錯誤之閱讀理解結果，以這樣的錯誤結果當干擾選項預期將有較高之誘導能力。相類似的想法也可以套用為挑選Retriever認為相關度比較不高的文章以為閱讀理解模型之輸入。此外，就生成題目之廣度與深度而言，我們可藉由控制取得之文章來調配題組之廣度與深度。例如，若我們以單篇或少數篇文章進行生成，我們所生成的主題題組針對測驗內容而言，應該可產生知識性較為深入之試題。而若針對較多篇文章，則應可產生範圍較廣之測驗試題。上述之研究議題，乍看可行但仍有許多細節需進行探索，如上述研究背景目的所述，本研究計畫擬探索基於現有Retrieve-and-GeneratePipeline之選擇題自動生成架構進行強化。我們以三年為期，規劃在此目標下擬定八項關鍵技術進行研究探討。如Figure6所示，八個工作項目又可區分為三項主軸：「選擇題生成品質提升」、「反事實選項生成」與「群組生成與難度控制」。其中，於「選擇題生成品質提升」研究主軸中，我們探索利用語言模型與知識圖譜架構來生成，將選擇題選項生成問題設定成為一Text2Text任務，輸入為問題題幹與正確答案所串接之TextSequence，而輸出為所生成之distractor串接之TextSequence。此外，我們也嘗試引入對比式學習來改善Text2Text生成模式之品質。我們也預期規劃探索利用ODQA方式來取得干擾選項。而於「反事實選項生成」研究主軸中，我們探索反事實陳述句之生成。我們預計嘗試基於MaskedLM取代重要名詞之反事實語句生成作法與基於知識圖譜知識三元組萃取改寫之作法。於第三個研究主軸中，我們則嘗試針對題組生成與難度可控制之干擾選項生成進行研究。MultipleChoice,Multi-此資料集特色在於為Multi-hop形式，答案需從複數文檔中推理而成；資料來自維基百科以及PubMed，PubMed為生物醫學論文的搜尋引擎，在兩個來源各自提供51,318(Wiki)和2,508(PubMed)筆問題。資料來自國高中英文考試，提供超過10萬筆問題以及2.8萬篇文本，題型上為英文閱讀測驗，會需要閱讀故事或對話紀錄，而針對該文本會有複數題目，每個題目此資料集的挑戰在於，內容來自於7個不同領域(新聞、小學自然科學、旅遊、小說故事等等)，為文本與題目提供多樣性，總計約1萬筆問題與800篇文本。MultipleChoice,Open-為Open-domain形式，並不提供文本，只有問題與答案；包含12,247筆問題、一個正確答案和四個干擾選項；題目類型為知識與常識題，來自於ConceptNet與同時包含兩種形式資料，填空題與Open-domain；填空題形式命名為QUASAR-S，其中資料來源自StackOverflow，提供約37K筆問題。Open-domain形式命名為QUASAR-T，提供約43K筆問題。來自StanfordUniversity的閱讀理解資料集，包含了超過100K筆問題和536篇對應的維基百科文本；延續了SQuAD1.1的題目外，[31]另外加入約50K筆無答案的題目，進一步測試機器能否在無對應答案時回答否，而MedQA是基於美國醫師執照考試(USMLE)所產生的資料集，其中問題皆為較為困難的專業醫學相關的知識，且包含了英文、簡體中文、繁體中文分別有約12K、34K、14K個問題集，這種專業的知識能使機器針對單一領域產生更具參考價值的distractor，目前此任務排名上最佳為ValentinLiévin團隊做出的Codex5-shot訓練機器學習模型進行閱讀理解(MachineReadingComprehensionTask)為最常見的深度學習自然語言任務。諸如，SQuAD[32]、RACE[21]、DROP[10]與HotPotQA[41]等資料集，皆是測驗/訓練機器學習模型於答案找尋、邏輯推理與摘要之能力。其中，許多機器閱讀理解任務採用多選題方式進行測驗。選擇題的自動答題與選擇題的自動生成為一體兩面之應用。因此具有選擇題樣態的資料集為我們選擇題自動生成研究之根本資料集，我們將目前所探知之相選擇題與干擾選項生成之相關研究可按應用類型可區隔為英文學習測驗與知識學習測驗。干擾選項生成研究於英文學習測驗應用，主要以自動生成閱讀理解與克漏字類型的考題為目標。閱讀理解之干擾選項生成與本研究計畫之知識類型選擇題之自動出題目標不同。因此本研究回顧中，我們不做此分類每一研究細部說明。我們將此類型之相關研究按不同象限做分類：(1)干擾選項層級之差異(生成字詞等級與句子等級之差異)象限，(2)題目類型(閱讀理解與克漏字測驗)，(3)生成方式(CandidateRanking與Generative)。我們按此三象限，將目前之相關研究摘要整理於Table5中。而知識學習測驗類型的選擇題生成，則於這一兩年逐漸吸引相關研究投入。我們認為與本研究計畫較為相關之研究有[22][40][3]，以下我們針對個別文獻討論如下。首先，與我們有相同目標者為[22]，也是針對特定知識主題生成選擇題。其主要作法Table5:現有閱讀測驗與克漏字干擾選項生成研究整理表為：先由人工給定一個主題，程式使用該主題名稱的維基百科頁面作為起始點(例如給定“體育”後搜尋體育相關維基百科)，保留熱門文章並取出文章的第一段擷取摘要，由此產生大量句子。並將這些句子依單純英文文法規則生成簡單5W問句與答案，例如who問句是將第一個動詞前的名詞作為正確答案並從句子中移除，what問句是將第一個夾在動詞間的名詞移除作為正確答案並從句子中移除，移除答案後的句子再帶入問題模板生成制式問句。生出五種問句後再依英文文法的詞與詞的相關性去決定哪個問句最適合。在干擾選項生成的部分，先隨機選取其他句子所產生的答案，再藉由語言模型去排序與問題的相關性，取前三高之預測為干擾選項。但本論文並未使用問句生成模型，採取之作法也與本計畫所規劃有所不同。於論文中[40]，則探討如何讓干擾選項更加多樣且與正確答案相差不遠(以具誘導性)。對此，創造了一款監督式分類任務來找出文章中屬於關鍵句子(及對產生干擾選項生成有用的句子)，作者在訓練之前會將訓練用的干擾選項和文章中的每個句子做比對，若該句子和干擾選項相似度達到一定程度，則該句子將成為文章中的關鍵句，透過這個方式在文章中能被修改成為新的干擾選項的句子將會達到多樣性的效果，且透過相似度對比，干擾選項也不會跟原始文章相差太遠從而提升干擾選項的品質。本研究之作法可供我們借鏡參考對關鍵文章/句選擇設計，用以提升干擾選項之多樣性。而與本計畫研究項目五「使用Open-DomainQuestionAnswer錯誤結果之干擾選項」走非生成想法相似之研究為[3]。其主要想法為作者發現於干擾選項資料集中，有許多干擾選項一再被使用。促使論文作者想為新的問題檢索以重複使用干擾選項。作者提出一個Transformer-baseddistractorretrievalmodels，該模型通過考慮問題的上下文來選擇干擾選項模型包含三個部分，分別為D-SIM模型、Q-SIM模型以及DQ-SIM模型。在D-SIM模型中使用mBERT計算同一個MCQ中問題和答案以及對應的干擾選項的向量，並計算他們之間的相似度。而在Q-SIM模型中作者假設不同問題共享一個或多個干擾選項或答案，代表問題可能在語意上相關，因此他們相關的干擾選項可以用作彼此的良好候選干擾選項，因此作者將訓練資料重新排序，使這些共享至少一個干擾選項或答案的問題聚集在一起，接著將資料送進Q-SIM模型計算不同的MCQ中問題之間的相似度。在DQ-SIM模型線性結合前兩個模型，從各自的優勢中獲益，透過基於分數和基於排名的合併策略計算來自D-SIM模型和Q-SIM模型的分數，並設計演算法使得高排名之干擾選項有較大的權重。重複使用現有資料集中之干擾選項為一創新想法，但仍與我們之ODQA做法有所不同。[OpenDomainQuestionAnswering相關研究]本計畫的選擇題生成架構主要基於兩元件:(1)文章萃取與(2)問句生成。目前我們所使用的文章萃取機制為傳統之BM25檢索。而近年來由於ODQA研究領域之發展，基於深度學習之DenseRetrieval技巧也紛紛於ODQA研究領域中所提出。我們預期DenseRetrieval技術也將於我們知識類型選擇題自動出題中擔任關鍵角色。茲簡要回顧現有ODQA之研究如下。ODQA架構中Retriver之重要性不言可喻。不同層級的語意檢索也影響著ODQA的效能。於[28]研究中，作者提出一個簡單而有效的pipeline系統，研究段落和句子級別語義檢索及其對下游任務的潛在影響，作法上是先檢索段落級別的段落，然後從上一步檢索後取得的段落中再去檢索句子級別的段落。該系統在事實驗證和Multi-hopQA方面進行評估，實驗結果顯示QA任務與事實驗證皆仰來上游檢索之結果。於[23]論文中，作者在ODQA架構的retreiver端提出了一個有效率的弱監督式學習的方法(InverseClozeTask)來對retreiver進行預訓練，給予模型完全無人工標記的維基百科原始資料，藉由論文中的方法將資料切分成固定長度的段落，並隨機選取其中的一個句子當作標籤，讓模型去學習如何從給定的句子去資料庫中找尋與之最相關的文章。在實驗數據中此方法訓練出的ODQA模型分數高於傳統的BM25檢索方法。而於DPR[16]論文中，作者為解決傳統BM25無法檢索語意相似段落的問題，提出了基於深度學習模型之DensePassageRetrieval，並提出使用in-batchnegative的技術，在密集向量的檢索取得極佳的效果改進。我們也預計引入中文DRP模型來提升目前題幹生成所需之而於GAR[27]研究中，作者們探討傳統基於BM25的檢索無法檢索沒有單詞重疊或語意相似的段落的問題，而基於密集向量的檢索方式在計算上又過於複雜，且會有資訊損失的問題。該論文透過預訓練語言模型自動生成與查詢語句相關的內容，可增加查詢語句的資訊量，隨後透過BM25進行檢索，該方法比起傳統BM25能檢索到更精確的段落，且執行效率也快於基於密集向量的檢索方式。透過GAR中的PLM可對查詢語句的資訊進行補充，原論文中是補充答案、相關段落及段落標題，可基於該手法讓PLM生成問題與原始答案的關聯，對這些關聯進行組合或刪除即可檢索與原答案有相似關係的查詢語句，可檢索到適合製作干於[19]，作者觀察到基於語言模型之檢索技術雖然效果很好，但比以往的方法需要極高的計算成本，特別是在計算每個query與document間的relevancescore時。本篇論文作者提出了ColBERT，引入lateinteraction架構，對query與document進行獨立的encoding，於模型輸出最終階段才進行相似度運算。而[18]中，則使用近一步基於ColBERT當基礎模型來進行retreiver端的訓練，使用了Relevance-guidedSupervision(RGS)的方法來訓練retreiver的模型。相較於傳統的retreiver訓練資料，傳統方法需人工標註的訓練資料，但作者認為人工標住資料往往存在偏見，進而影響模型的效能，所以在論文中提出了用訓練好的模型來找尋最適合的訓練資料，並把這些資料用於訓練下一個模型，使用這個方式來進行後續的訓練，最後得出最佳的模型。此方法訓練出的模型分數明顯高於目前opendomain的其他方式。在找尋訓練資料時，模型會先搜尋前k筆資料，並把前20筆資料中含有shortanswer的資料當成possitiveexample，而前20筆以外剩餘的資料中不含有shortanswer的資料當成negativeexample，此方法尋找的負面訓練資料交由reader進行預測能得到與正確答案相關的錯誤答案，如能改良ranking的方式，應用於干擾選項生成，預期將能提升干擾選項之品質。[ContrastiveLearning於自然語言生成相關研究]對比式學習始見於電腦視覺領域，其於影像應用上有著顯著的效果提升。因此，許多自然語言處理之研究者也將其引用至自然語言生成領域，諸如[11][7][35]。於[11]研究中，作者們提出了使用ContrastiveLearning的作法來學習SentenceEmbedding，他們使用BERT作為基礎模型，透過不同的dropoutmask使模型能用同樣的輸入產生不同的SentenceEmbedding，但它們之間的距離會是近的，也就可作為PositiveSample。然後再使用其他in-batch的句子作為NegativeSample，以達到非監督式訓練。而在有訓練資料的情況下，如NLI資料集，則可以將Eentailment視為PositiveSample，Contradiction以及其他in-batch的句子視為Negative而再基於[11]研究後，於[7]研究中，作者們提出加上了Discriminator來加強SentenceEmbedding的學習效果。具體而言，對每筆輸入來說，會透過SentenceEncoder產生一個SentenceEmbedding，同時也會隨機的Masking輸入句子其中的幾個Token，並透過一個fixed的Generator來還原MaskToken，而Discriminator要試著透過沒被修改過的SentenceEmbedding的資訊去辨認修改後的句子中的哪一個Token有被修改過，整個框架會透過SentenceEmbedding的ContrastiveLearningLoss以及Discriminator的ReplacedTokenDetection而與我們預計引入對比式學習來提升句子等級的干擾選項生成效果想法接近的研究也可見於[35]。該研究指出發現了以往autoregressive模型使用BeamSearch去決定要生成的文本時，會產生語意不自然以及沒意義的重複文字，儘管後續有人使用取樣以及修改ObjectiveFunction來嘗試解決這個問題，但這個方法時常會產生語意不連貫的問題。因此作者提出了使用ContrastiveLearningLoss加上原先的MaximumLikelihoodEstimationLoss來訓練模型，並且使用ContrastiveSearch來作為decodingmethod。[Counter-FactualSentenceGeneration相關研究]於Counter-FactualGeneration研究中，[44,33]為相關研究之先驅。主要想法是使用神經網路來生成反事實敘述，這個概念背後的想法是同時使用語言模型和各種採樣的策略來達成生成反事實敘述的效果。接續之前的成果，[39]提出了Polyjuice架構，可以控制輸出的反事實敘述的類型。另外一種生成干擾選項的研究是使用干擾選項生成(DistractorGeneration)的手法，干擾選項生成(DistractorGeneration)的目標是給予了一篇文章、問題和正確答案後，模型能夠生成似是而非的干擾選項。於研究[24]中，作者最先提出使用了神經網路來生成干擾選項，而研究[8]則延續了前者的成果，把重點放在生成複數的干擾選項並提出了一個負樣本學習策略(negativetrainingstrategy)來加強干擾選項生成。但這上述兩個研究主要都著眼於閱讀理解題型中干擾選項之生成，與本研究知識類型選擇題生成目標所有不同。而在關於AbductiveInferenceGeneration的相關研究中，於[30]中作者提出了故事改寫的任務，此任務目標是要改寫一段故事的敘述並且導致產生與原先不同的結局，與顯永的論文的方式的差異為故事改寫任務是根據給定的誤導推論來生成不同的結局，在MIG任務中是給定敘述句與其前後文來生成誤導推論。[2]研究中是讓模型根據上下文來推論中間適合加入的句子，此研究主要是產生正確的推論敘述，而在MIG任務[29]主要是產生相反的誤導推論敘述。於我們先前的研究[29]中，我們提出一個自然語言的誤導推論生成任務，任務步驟是當給定了一個陳述事實的句子並給定其上下文，將其定義為(x,y,z)與誤導推論ˆy，模型會生成與該事實相反的推論句子，由此可以讓模型產生出多選題似是而非的干擾選項。然而這樣任務與我們所設定之反事實出題仍有所不同。而與我們生成Counter-Factual陳述句想法接近的研究為：[46]。於該研究中，作者探討有別於過往的是非題，將文章中原始句子作為True的問句並通過反義詞替換關鍵字生成False問句，本篇作者通過使用一種新穎的masking-and-infilling策略來生成更靈活與複雜的問句。要被mask的關鍵字找法有很多種，像是SRL結果中的predicate，句子中連接詞(”that”、”when”、”since”)之後的部分或是數字等等，來找出值得提問的句子。Infilling的作法則是以BART為基底作為生成模型，將原始句子的關鍵字mask後，提供前後句子作為BART的上下文後產生句子填入。作者為了找值得提問的句子所提出的方法亦可用來作為多選題產生題目的答案選擇方法，而infilling的想法如果使用得宜，在提供出題範圍的前後文後，也能將欲出題句子挖空讓BART模型填補後產生選項。雖然與我們目標相似，但仍與我整合知識圖譜與語言模型為近年語言模型訓練熱門關注議題。於[42]研究中，作者們提出一CounterfactualGenerationControllableCounterfactualGenerationDistractorGenerationMisleadingInferenceGeneration種能將文本和知識圖譜的資訊進行深度雙向、自我監督的預訓練的方法，透過作者的方法可以將一些特定領域的KnowledgeBase資料集結合相關文獻去訓練語言知識模型，讓語言模型和知識圖譜之間的資訊互相交換，並且在回答特定領域問答的任務上，表現優於現有的語言模型和知識圖譜增強模型。我們想透過作者提供的預訓練模型，來做多選題選項生成，透過知識圖譜所額外提供的資訊，讓在考慮生成選項時，除了題目和答案的文字外，用這些文字從知識圖譜萃取相關的知識圖譜子圖幫助生成選項有更多參考資訊。知識圖譜於問句生成之研究也見於[5][13]。於[5]研究中，作者之想法為使用知識圖譜進行問題生成任務時，通常只會取出資料與關係的三元組合，並不能完整利用知識圖譜中豐富的資訊，因此作者基於GraphNeuralNetworks提出了BidirectionalGatedGraphNeuralNetwork(BiGGNN)，將知識圖譜中的node及edge編碼成向量，使得每個node都包含了周圍其他node的資訊，並對答案的node進行標註，隨後透過RNN進行問句生成，生成過程中RNN會決定生成新文字或直接複製某個node的資訊放進問題。我們應可借鏡此研究論文的想法，可將知識圖譜及答案的資訊完整編碼，可基於此特性，在生成問題時同時考慮答案附近的node，讓模型在建立問題的同時也能根據附近的node產生干擾選項。而於[13]研究中，則提出的ISEEQ架構，主要目的是希望從簡短的敘述句中去尋找更多訊息來反問使用者是否符合關於敘述句背後未提及的情況，在使用時，使用者只需輸入簡短的敘述句，模型會先從知識圖譜擴展並增加query的關鍵字，透過這些關鍵字模型就能retrieve更多相關的文章進行ranking，在從這些文章中使用QG模型產生一些原始query中未提及但可能相關背景知識，例如當使用者說道:“我感到疲勞且渾身無力”，ISEEQ能回覆:“請問病徵持續了多久?”，或是“你最近14天有出國過嗎?”這類可能與其他知識相關的問句。這種方式最大的貢獻及是讓機器學習到”反問”的能力，而非一味的透過query生成出結果。這樣的想法，或許可引用來提升題幹生成的精確度，舉例來說:有位教師想出關於日治時期的歷史題目，我們就能先藉由ISEEQ來詢問是要出與日治時期戰爭相關的題目或是與日治時期建設相關的題目，以此來取得更精確的題幹生成需求。[1]J.Araki,D.Rajagopal,S.Sankaranarayanan,S.Holm,Y.Yamakawa,andT.Mitamura.Gen-eratingquestionsandmultiple-choiceanswersusingsemanticanalysisoftexts.InProceedingsofCOLING2016,the26thInternationalConferenceonComputationalLinguistics:TechnicalPapers,pages1125–1136,2016.[2]C.Bhagavatula,R.L.Bras,C.Malaviya,K.Sakaguchi,A.Holtzman,H.Rashkin,D.Downey,S.W.-t.Yih,andY.Choi.Abductivecommonsensereasoning.arXivpreprintarXiv:1908.05739,[3]S.K.Bitew,A.Hadifar,L.Sterckx,J.Deleu,C.Develder,andT.Demeester.Learningtoreusedistractorstosupportmultiplechoicequestiongenerationineducation.IEEETransactionsonLearningTechnologies,2022.[4]Y.-H.ChanandY.-C.Fan.Arecurrentbert-basedmodelforquestiongeneration.InProceedingsofthe2ndWorkshoponMachineReadingforQuestionAnswering,pages154–162,2019.[5]Y.Chen,L.Wu,andM.J.Zaki.Towardsubgraphguidedknowledgegraphquestiongenerationwithgraphneuralnetworks.arXivpreprintarXiv:2004.06015,2020.[6]S.-H.Chiang,S.-C.Wang,andY.-C.Fan.Cdgp:Automaticclozedistractorgenerationbasedonpre-trainedlanugagemodel.InFindingsoftheAssociationforComputationalLinguistics:[7]Y.-S.Chuang,R.Dangovski,H.Luo,Y.Zhang,S.Chang,M.Soljac￿ic￿,S.-W.Li,W.-t.Yih,Y.Kim,andJ.Glass.Diffcse:Difference-basedcontrastivelearningforsentenceembeddings.arXivpreprintarXiv:2204.10298,2022.[8]H.-L.Chung,Y.-H.Chan,andY.-C.Fan.Abert-baseddistractorgenerationschemewithmulti-taskingandnegativeanswertrainingstrategies.InFindingsoftheAssociationforComputationalLinguistics:EMNLP2020,pages4390–4400,2020.[9]B.Dhingra,K.Mazaitis,andW.W.Cohen.Quasar:Datasetsforquestionansweringbysearchandreading.arXivpreprintarXiv:1707.03904,2017.[10]D.Dua,Y.Wang,P.Dasigi,G.Stanovsky,S.Singh,andM.Gardner.ingcomprehensionbenchmarkrequiringdiscretereasoningoverparagraphs.arXiv:1903.00161,2019.[11]T.Gao,X.Yao,andD.Chen.Simcse:Simplecontrastivelearningofsentenceembeddings.arXivpreprintarXiv:2104.08821,2021.[12]Y.Gao,L.Bing,P.Li,I.King,andM.R.Lyu.Generatingdistractorsforreadingcomprehen-sionquestionsfromrealexaminations.InProceedingsoftheAAAIConferenceonArtificialIntelligence,volume33,pages6423–6430,2019.[13]M.Gaur,K.Gunaratna,V.Srinivasan,andH.Jin.Iseeq:Informationseekingquestiongenerationusingdynamicmeta-informationretrievalandknowledgegraphs.InProceedingsoftheAAAIConferenceonArtificialIntelligence,volume36,pages10672–10680,2022.[14]Q.Guo,C.Kulkarni,A.Kittur,J.P.Bigham,andE.Brunskill.Questimator:Generatingknowl-edgeassessmentsforarbitrarytopics.InIJCAI-16:ProceedingsoftheAAAITwenty-FifthInternationalJointConferenceonArtificialIntelligence,2016.[15]D.Jin,E.Pan,N.Oufattole,W.-H.Weng,H.Fang,andP.Szolovits.Whatdiseasedoesthispa-tienthave?alarge-scaleopendomainquestionansweringdatasetfrommedicalexams.AppliedSciences,11(14):6421,2021.[16]V.Karpukhin,B.Og￿uz,S.Min,P.Lewis,L.Wu,S.Edunov,D.Chen,andW.-t.Yih.Densepassageretrievalforopen-domainquestionanswering.arXivpreprintarXiv:2004.04906,2020.[17]D.Khashabi,S.Chaturvedi,M.Roth,S.Upadhyay,andD.Roth.Lookingbeyondthesurface:Achallengesetforreadingcomprehensionovermultiplesentences.InProceedingsofthe2018ConferenceoftheNorthAmericanChapteroftheAssociationforComputationalLinguistics:HumanLanguageTechnologies,Volume1(LongPapers),pages252–262,2018.[18]O.Khattab,C.Potts,andM.Zaharia.Relevance-guidedsupervisionforopenqawithcolbert.TransactionsoftheAssociationforComputationalLinguistics,9:929–944,2021.[19]O.KhattabandM.Zaharia.Colbert:Efficientandeffectivepassagesearchviacontextualizedlateinteractionoverbert.InProceedingsofthe43rdInternationalACMSIGIRconferenceonresearchanddevelopmentinInformationRetrieval,pages39–48,2020.[20]G.Kumar,R.E.Banchs,andL.F.D’Haro.Revup:Automaticgap-fillquestiongenerationfromeducationaltexts.InProceedingsoftheTenthWorkshoponInnovativeUseofNLPforBuildingEducationalApplications,pages154–161,2015.[21]G.Lai,Q.Xie,H.Liu,Y.Yang,andE.Hovy.Race:Large-scalereadingcomprehensiondatasetfromexaminations.arXivpreprintarXiv:1704.04683,2017.[22]G.LeBerre,C.Cerisara,P.Langlais,andG.Lapalme.Unsupervisedmultiple-choicequestiongenerationforout-of-domainq&afine-tuning.InProceedingsofthe60thAnnualMeetingoftheAssociationforComputationalLinguistics(Volume2:ShortPapers),pages732–738,2022.[23]K.Lee,M.-W.Chang,andK.Toutanova.Latentretrievalforweaklysupervisedopendomainquestionanswering.arXivpreprintarXiv:1906.00300,2019.[24]C.Liang,X.Yang,N.Dave,D.Wham,B.Pursel,andC.L.Giles.Distractorgenerationformultiplechoicequestionsusinglearningtorank.InProceedingsofthethirteenthworkshoponinnovativeuseofNLPforbuildingeducationalapplications,pages284–290,2018.[25]C.Liang,X.Yang,D.Wham,B.Pursel,R.Passonneaur,andC.L.Giles.Distractorgenera-tionwithgenerativeadversarialnetsforautomaticallycreatingfill-in-the-blankquestions.InProceedingsoftheKnowledgeCaptureConference,pages1–4,2017.[26]V.Liévin,C.E.Hother,andO.Winther.Canlargelanguagemodelsreasonaboutmedicalquestions?arXivpreprintarXiv:2207.08143,2022.[27]Y.Mao,P.He,X.Liu,Y.Shen,J.Gao,J.Han,andW.Chen.Generation-augmentedretrievalforopen-domainquestionanswering.arXivpreprintarXiv:2009.08553,2020.[28]Y.Nie,S.Wang,andM.Bansal.Revealingtheimportanceofsemanticretrievalformachinereadingatscale.arXivpreprintarXiv:1909.08041,2019.[29]H.-Y.Peng,H.-L.Chung,Y.-H.Chan,andY.-C.Fan.Misleadinginferencegenerationviaproxi-malpolicyoptimization.InPacific-AsiaConferenceonKnowledgeDiscoveryandDataMining,pages497–509.Springer,2022.[30]L.Qin,A.Bosselut,A.Holtzman,C.Bhagavatula,E.Clark,andY.Choi.Counterfactualstoryreasoningandgeneration.arXivpreprintarXiv:1909.04076,2019.[31]P.Rajpurkar,R.Jia,andP.Liang.Knowwhatyoudon’tknow:Unanswerablequestionsforsquad.arXivpreprintarXiv:1806.03822,2018.[32]P.Rajpurkar,J.Zhang,K.Lopyrev,andP.Liang.Squad:100,000+questionsformachinecomprehensionoftext.arXivpreprintarXiv:1606.05250,2016.[33]S.Ren,Y.Deng,K.He,andW.Che.Generatingnaturallanguageadversarialexamplesthroughprobabilityweightedwordsaliency.InProceedingsofthe57thannualmeetingoftheassociationforcomputationallinguistics,pages1085–1097,2019.[34]S.RenandK.Q.Zhu.Knowledge-drivendistractorgenerationforcloze-stylemultiplechoicequestions.InProceedingsoftheAAAIConferenceonArtificialIntelligence,volume35,pages[35]Y.Su,T.Lan,Y.Wang,D.Yogatama,L.Kong,andN.Collier.Acontrastiveframeworkforneuraltextgeneration.arXivpreprintarXiv:2202.06417,2022.[36]A.Talmor,J.Herzig,N.Lourie,andJ.Berant.Commonsenseqa:Aquestionansweringchallengetargetingcommonsenseknowledge.arXivpreprintarXiv:1811.00937,2018.[37]J.Welbl,N.F.Liu,andM.Gardner.Crowdsourcingmultiplechoicesciencequestions.arXivpreprintarXiv:1707.06209,2017.[38]J.Welbl,P.Stenetorp,andS.Riedel.Constructingdatasetsformulti-hopreadingcomprehensionacrossdocuments.TransactionsoftheAssociationforComputationalLinguistics,6:287–302,[39]T.Wu,M.T.Ribeiro,J.Heer,andD.S.Weld.Polyjuice:Automated,general-purposecounter-factualgeneration.arXivpreprintarXiv:2101.00288,1(2),2021.[40]J.Xie,N.Peng,Y.Cai,T.Wang,andQ.Huang.Diversedistractorgenerationforconstruct-inghigh-qualitymultiplechoicequestions.IEEE/ACMTransactionsonAudio,Speech,andLanguageProcessing,30:280–291,2021.[41]Z.Yang,P.Qi,S.Zhang,Y.Bengio,W.W.Cohen,R.Salakhutdinov,andC.D.Manning.Hotpotqa:Adatasetfordiverse,explainablemulti-hopquestionanswering.arXiv:1809.09600,2018.[42]M.Yasunaga,A.Bosselut,H.Ren,X.Zhang,C.D.Manning,P.Liang,andJ.Leskovec.Deepbidirectionallanguage-knowledgegraphpretraining.arXivpreprintarXiv:2210.09338,2022.[43]C.Y.Yeung,J.S.Lee,andB.K.Tsou.Difficulty-awaredistractorgenerationforgap-fillitems.InProceedingsoftheThe17thAnnualWorkshopoftheAustralasianLanguageTechnologyAssociation,pages159–164,2019.[44]C.Zhang,Y.Sun,H.Chen,andJ.Wang.Generatingadequatedistractorsformultiple-choicequestions.arXivpreprintarXiv:2010.12658,2020.[45]X.Zhou,S.Luo,andY.Wu.Co-attentionhierarchicalnetwork:Generatingcoherentlongdistractorsforreadingcomprehension.InProceedingsoftheAAAIConferenceonArtificialIntelligence,volume34,pages9725–9732,2020.[46]B.Zou,P.Li,L.Pan,andA.Aw.Automatictrue/falsequestiongenerationforeducationalpur-pose.InProceedingsofthe17thWorkshoponInnovativeUseofNLPforBuildingEducationalApplications(BEA2022),pages61–70,2022.於本研究計畫中，我們目標基於自然語言生成技術進行選擇題自動出題研究。我們預期的展示情境為：給定一知識範圍(如農業文集)，我們可於系統直接輸入一組主題關鍵詞，而後自動生成關於該主題相關之選擇題題組(含字詞等級、句子等級、挑選是非類型之選擇題)。然而，如同前言背景中所言，現有技術離目標仍有許多待強化之處，對此我們擬定三大出題目標：字詞等級干擾選項、句子等級干擾選項與反事實干擾選項進行研究。針對此三大目標，我們擬定(1)基於知識圖譜之強化與(2)基於生成語言模型與對比學習之強化三個核心技術進行探討。研究技術項目與項目間之關係與進行規劃如Figure7所示。於第一年度中，我們規劃「考量題幹可用性與干擾選項間關聯之Retrieve-and-Generate架構優化」、「基於語言模型與知識圖譜之Text2Text干擾選項生成」與「基於Masked-LM之反事實干擾選項生成研究」進行研究。「考量題幹可用性與干擾選項間關聯之Retrieve-and-Generate架構優化」著眼於優化目前https://mcq-demo.nlpnchu.org之WordLevel之選擇題生成系統。希冀自動過濾相似或語意不正確題幹，並且考量干擾選項彼此間之關聯進行最終干擾選項集合之綜合挑選。「基於生成語言模型之Text2Text干擾選項生成模型」則考量Text2Text模式利用生成模型直接生成干擾選項序列，藉此嘗試生成句子等級之干擾選項。並進一步地探索結合知識圖譜與語言模型之干擾選項生成，希冀透過知識圖譜之整合來生成更有誘導性且合理之字詞等級干擾選項。此外，於「基於Masked-LM之反事實干擾選項生成研究」項目中，我們預計著手挑戰反事實干擾選項生成，我們嘗試利用透過NER擷取給定事實陳述句中之關鍵字並Masked-LM該關鍵字來填入機率較低之字詞，以為反事實語句生成之作法。我們預期第一年度工作完成後，將可優化現有單詞等級干擾選項之選擇題自動生成系統，並於第二年度中，我們則規劃「基於知識圖譜強化之反事實干擾選項生成」、「基於對比式學習之干擾選項生成模型強化」與「利用Open-DomainQuestionAnswering錯誤結果之干擾選項挑選」。於此年度中，進一步嘗試利用知識圖譜萃取三元組知識後進行反事實語句之生成，基本想法為透過萃取三元組後再進一步輸入語言模型，並希望語言模型適當地進行反事實語句的改寫。此一部分預計使用第一年度之「基於Masked-LM之反事實干擾選項生成研究」之基礎以為訓練資料。對於字詞等級的干擾選項，我們則探索利用Open-DomainQuestionAnswering模型所錯誤回報之答案當作干擾選項。我們希望透過此不同之想法來取得更多樣的干擾選項。再者，如前言所述，生成模型生成序列時常出現生成結果重複的狀況，對此我們嘗試利用對比式來改善生成模型這方面的缺點，以優化干擾選項之生成。第二年度工作項目完成後，我們可初步提供反事實干擾選項之選擇題生成、優化後之句子等級干擾選項生成與基於ODQA方式取得之字詞等級干擾選項生成。於第三年度中，我們則規劃「基於NodeCentrality考量主題涵蓋性之選擇題題組生成」與「生成難度可控之干擾選項生成研究」。如同前言所述，一個理想的選擇題出題系統應該考慮題組生成，整體題目之主題涵蓋性，對此我們也將嘗試基於知識圖譜涵蓋的方式考慮節點centrality重要度方式來進行題組之生成。同時我們也將嘗試題目難易度之控制生成研究。我們的想法是當自動生成成為可能後未來的出題應該是個人化模式；針對個人的學習狀況進行測驗，因此我們規劃「個人化與生成難度可控之干擾選項生成研究」進行探討。第三年度，我們也預計投入人力進行系統的實作開發與整合。以下茲針對擬訂之研究議題其問題挑戰與1.基於語言模型與知識圖譜之Text2Text干擾選項生成模型近年來，語言模型除了使用CandicateGeneratingandRanking架構的MaskedLM之外，使用Encoder-Decoder架構的Text2Text模型於生成任務上也有很好的成果，因此我們的想法是把干擾選項生成Model視為一個Text2Text任務，嘗試讓模型直接生成干擾選項。且使用Text2Text模型生成Distractor有別於用MaskedLM生成Distractor，我們希望模型能生成的Distractor有以下特性(1)非單詞之Distractor:目前在Distractor生成上多為單詞生成，要生成一段敘述/句子的干擾選項還沒有很好的做法。(2)特定domain之MCQ:目前的MCQ多為英文測驗，若想針對特定領域出選擇題，使用MaskedLM生成的Distractor會因語料庫限制而為達到上述的兩種目標需求，我們設Q為輸入之問句，A為輸入問句之答案，並將Q與A字串接字串當作輸入，訓練模型輸出一連串問句Q的干擾選項字串D=(d1,d2,d3)。模型架構如圖2所示，我們將「哪一種技術最適用於嚴重特殊傳染性肺炎之病原快速篩檢?」與「聚合￿連鎖反應」接在一起輸入至Text2Text模型，而模型生成了「重組DNA技術」、「電腦斷層掃描」與「組織培養」三個干擾選項，三個選項皆為醫療檢測方式且都不是單詞，符合如前言所述，我們也預計探索使用知識圖譜來強化生成干擾選項的能力。如Figure8的圖例，我們可以使用語言模型結合醫療知識圖譜，利用給定的文本從知識圖譜中萃取有包含這些topicentities以及相鄰的nodeentities的子圖，再分別將文本與子圖輸入至語言模型和GraphNeuralNetwork(GNN)，讓模型學習找出關鍵的資訊與強化network中重要entitynodes跟edges的權重，藉由GNN的額外資訊與語言模型能力生成更好的選項。Figure8為我們規劃做法的概觀。主要分兩階段進行：(1)關鍵字子圖與GNNEmbedding萃取與(2)GNNEmbedding與LanugageEmbedding間之FusionEncoder生成。具體而言，第一個階段是將從知識圖譜裡萃取相關的知識圖譜子圖。我們一開始先使用NERModel抓取題幹中的entities，再透過這些entities透過知識圖譜去尋找entitynodes以及所有跟entitynodes相鄰3-hop內的bridgenodes，將檢索出來的結果當成知識圖譜子圖，並在後續第二階段提供以Figure9的圖例，我們先將問題透過NERModel斷詞得到”白血病”、”發病”、”器官”與”發病原因”當作entities，再透過這些entities當作關鍵字透過Retriever從知識圖譜萃取Figure8:結合知識圖譜與語言模型生成之構想架構圖與這些詞相關的node和edge，如利用”白血病”與”器官”能找到”肝臟”、”骨髓”、”肺部”等等entities，我們能利用這些entities來當作後續的生成選項。第二個階段則將文本與第一階段得到的知識圖譜子圖分別輸入至語言模型與GNN進行訓練，訓練目標為將Input的[MASK]Token還原來產生干擾選項，先將語言模型的Input額外新增[MASK]Token，經過N層LMLayer做self-attention後，透過FusionEncoder再結合知識圖譜的資訊與原本的文本去生成選項，透過以上的手法，來強化干擾選項之生成。2.考量題幹可用性與干擾選項間關聯之Retrieve-and-Generate架構優化如同研究目的所述，本研究項目著眼於優化現有Retrieve-and-Generate生成架構。我們•文章檢索器過濾:在Retrieving-and-Generating方法中Retriever會檢索回傳含有主題字詞的文章，但有部分文章並不適合當作問句生成模型的輸入，那些文章含有太多的雜訊，我們想嘗試在這步驟先過濾一些文章。我們的想法是利用詞向量中的相似詞再次檢索Retriever回傳的文章，把過於不相關的文章濾掉，減少問句生成模型輸出過於複•生成題幹品質自動過濾：問句生成模型輸出的問句，會再交由使用者挑選，但為了更好的效果，我們想嘗試利用語言模型做初步的篩選，減少人工挑選時間。我們想嘗試利用ODQA模型來過濾問句，將問句生成模型輸出的問句輸入至問答模型後，根據問答模型的回答結果與信心分數來決定該問句是否合適當作MCQ。如Figure10所例，我們將生成之問句再次投入ODQA模型，透過答案選項之信心度來為問句生成過濾之基Figure10:Unanswerablequestionsolution•考量干擾選項間關聯之干擾選項挑選:如前言所述，一個可以提升選擇題生成品質￿考量干擾選項彼此之間之難易度。而在Retrieving-and-Generating方法中，生成干擾選項的方式為取出詞向量模型中與主題字詞相似之詞，使用相似度前三名當作干擾選項，因此有選項過於相近而造成問題太難的可能。為做改善，我們取出所有相似詞的向量與主題字詞向量計算相對距離，設定各個選項與答案的相似度範圍(距離範圍)，再從中隨機挑選相似詞，讓干擾選項彼此之間難易度有所區分以提升試題之鑑別度。•基於語言模型之干擾選項候選生成:雖然詞向量模型生成distractor過程迅速，但如果有主題字詞沒有被詞向量模型看過或有主題字詞一字多意，就無法生成適當的干擾選項。這個問題我們想嘗試使用MaskedLM來解決，我們將Retriever回傳的文章中含有主題字詞的段落記錄下來，移除主題字詞後輸入MaskedLM，再從MaskedLM輸出的候選字中選出distractor，使用這個方式生成的干擾選項不會被上述的缺點受限，也更Figure11:OptimizedRetrievingandGeneratingFramework3.基於Masked-LM之反事實干擾選項生成研究本研究項目為基於Masked-LM技巧來生成反事實干擾選項。我們需要先查找與事實相符之敘述，再修改成反事實干擾選項。我們預計之作法為：首先讓使用者先輸入考題關鍵字，再使用retriever查找相關文章，其目的為從相關文章中選取與事實相符之敘述。接著，將retriever檢索到的文章回傳給使用者，由使用者選取與事實相符之敘述。並按照以下之步驟進行Masked-LM反事實干擾選項生成。為了抽換專有名詞，我們先使用命名實體與事實相符之敘述中的所有專有名詞，以下圖為例”腎小管藉壓力差方式再吸收葡萄糖回微血管”，透過命名實體模型會萃取出專透過命名實體模型，我們會萃取出多個與事實相符之敘述中的專有名詞，我們需要選擇一個專有名詞行抽換。我們認為該專有名詞在相關文章中的重要程度越高，越高機率是考題中的重點。為了確保抽換專有名詞的品質，我們使用BM25來查看各個專有Figure12:AutoEncoder模型與AutoRegressive模型做MaskLM3.使用Maskedlanguagemodel模型進行改寫Maskedlanguagemodel，簡稱為MaskLM，該任務的會將輸入中的部分內容遮擋起來，模型會根據其他未被遮擋的內容，預測出被遮擋的內容或與其擁有相似語義的內容，現有的MaskLM模型分為兩種，分別為AutoEncoder框架的MaskLM模型與AutoRe-gressive框架的MaskLM模型，經典模型如BERT與BART。針對MaskLM任務，兩種模型進行預測的方式也有所不同，AutoEncoder框架的模型會預測出字，而AutoRe-gressive框架的模型會預測出字、詞或是句子，詳細範例如Figure12。根據抽換專有名詞的需求，我們將使用Autoregressive模型做MaskLM任務來抽換重要的專有名詞。然而模型預測出的內容眾多，有可能是原先遮擋的內容、遮擋內容的同義詞或是與遮擋內容擁有相似語義的內容。理想的情況下，模型預測的內容最高機率為原先遮擋的內容，若我們選擇原先遮擋的內容或是遮擋內容的同義詞進行抽換，會造成與事實相符之敘述未改變，便無法生成反事實干擾選項。為了避免此狀況發生，我們使用NLI任務中的換句話說(MRPC)任務來檢測其是否為干擾選項。換句話說任務主要在判斷兩句話的語意是否相同，為了避免MaskLM模型選用同義詞進行抽換，即模型生成的干擾選項仍是正確的，我們將與事實相符之敘述與生成之干擾選項作為模型的輸入，若模型認為兩句話具有相同語義，我們會讓MaskLM模型預4.基於知識圖譜強化之反事實干擾選項生成於本研究項目中，我們嘗試利用知識圖譜提供語言模型額外資訊達成反事實干擾選項生成。我們的構想如Figure4所示，包含以下步驟。首先我們會讓使用者先輸入考題內文，再將內文使用斷詞或NER模型萃取entity，利用這些entity於知識圖譜中查找相關子圖，子圖必須包含內文中所有的entitynodes以及所有跟entitynodes相鄰節點，以此為知識生成之基礎。接著我們將檢索到的子圖內的nodes與edges萃取成三元組，每筆三元組的結構是以node,edge,node組成。如同圖中所示，每一筆三元組描述一基本事實陳述，如導體可以3.將與事實相符之敘述修改為反事實干擾選項為了將與事實相符之敘述更改為反事實干擾選項，我們將前一步驟所萃取的三元組與考題內文當作作語言模型的輸入來生成反事實干擾選項。我們預計透過生成語言模型與輸入給定的問句和三元組來生成與問句情境相符合的反事實干擾選項，以FigurejQuery36306122693278001712_1676211361329為例，生成語言模型在給定關於與導體相關的題目敘述以及從知識圖譜萃取與導體相關的三元組後，模型藉由這些三元組推斷其中的關係，比如”導體可以導電->石墨是導體->筆芯是由石墨製成”，從上述的三元組推斷出”筆芯可以導電”的事實，也可以改寫成”筆芯不能導電”的反事實干擾選項。5.使用Open-DomainQuestionAnswering錯誤結果之干擾選項挑選研究如前言所述，我們預計使用ODQA模型所回答之錯誤結果來當作干擾選項(如Figure5)。以我們先前曾開發https://agri.nlpnchu.org/之農業問答系統為例，我們發現以上想法所面臨的挑於ODQA的架構下，可能會遇到ODQA正確回答所下達之問題，其回傳結果皆為可行之答案，也因此無法被使用來當干擾選項，對此，我們探索兩種作法來進行調整。–1.使用BM25，且刪除尋找出來文章的正確答案在使用BM25的情境下，因為是使用關鍵字比對，所以會找出許多跟正確答案相關的文章，我們將找到文章的正確答案刪除，這樣這個文章會變成絕對回答不出正確答案的文章，可是因為Reader對文字有一定的理解能力，並且具有關鍵字的文章中也很可能會有，他會從剩下可能的答案中，尋找一個最相近的選項，這樣就可以較容易生出干擾選項。如Figure13所示，我們將答案”2,4-D”從文章中直接刪除後，再交給Reader進行閱讀，藉此確保正確答案不會被回傳。Figure13:基於ODQA架構之Retriever弱化方法–2.使用DensePassageRetrieverDensePassageRetriever[16]，DensePassageRetriever可以將資料庫的文章轉為向量的方式表示，在搜尋的時候也將Query轉為向量，並且比對兩個向量的相似度，這樣就可以找出相似的文章，可是我們想說，可以利用DPR找尋文章相似度的這一點，理論上相似的文章跟Query算出來的相似度會是最高的，我們可以將相似度高的文章拿出來，並且查看有沒有答案出現在上面，如果沒有答案，我們就將他丟給Reader，因為我們覺得如我相似度高，就代表Retriever認為裡面有答案，可是裡面並沒有含有我們的正解，所以這篇文章就含Retriever認為是答案，但是實際上不是答案的文字，所以這時候我們將這篇文章傳給Reader，就會有比較高在我們使用baseline的情境中，有時候也會出現不相關的答案，例如在??中，問模型關於殺草劑的問題，會出現的人力除草，人力除草跟題目所問的殺草劑名稱是不相關的，所以我們需要用一個方法把他過濾掉。在這種情況下，我們如果加入正確答案當作Query一起輸入Retriever，就會增加模型找到正確passage的機率，對我們想找跟正確答案不相關但是相似度較高的Passage會產生反效果，所以我們採用的是從Reader輸出之後把他過濾掉的方法。當Reader從Retriever拿到Passage之後，我們會將正確答案去除，之後再將剩餘的答案丟入一個LanguageModel作判斷，如果他跟答案的距離過於遙遠，就放入NegativeList，如果是相關的就放入PositiveList，之後要挑相關答案的時候可以從PositiveList裡面挑選。6.基於對比式學習之干擾選項生成模型強化ContrastiveLearning的設計上需要考量如何去設定PositiveSample以及NegativeSample。在我們的目標中，因為不希望模型會重複的生成出相同意思的句子，所以我們預計使用同義的句子當作NegativeSample，而資料集中的干擾選項可以被當作PositiveSample。然而，由於我們的資料集中並沒有NagativeSample的選項，也就是同義於模型生成的選項。對此，我們的想法為透過迭代生成來產生我們需要的NegativeSample。具體而言，如Figure14所舉例，在DistractorGenerator的第一次生成中，我們會輸入Question以及Answer使模型生成出干擾選項。而在第二次迭代中，我們除了會輸入QuestionAnswerPair外，還會再加上在上一次迭代中生成的干擾選項，以提示模型先前已生成的Distractor的資訊。對於第二次迭代的輸出來說，第一次迭代輸出的Distractor，即為它的NegativeSample，因為DistractorGenerator必須要生成出與上一次迭代生成的Distractor意義相異的句子，所以它們之間的SentenceEmbedding不能太過接近。Figure14:IterationGenerater示意，在第一次的迭代中生成的Distractor會作為第二次迭代的輸入，以提示模型不要生成出類似的DistractorFigure15:對比式學習於干擾選項生成之預想架構我們所規劃之整體框架，如Figure15所示。我們使用多次迭代的方式來生成Distractor，對於第一次迭代來說，我們會從其他in-batch的data中取得Sentence當作NegativeSample。而在第二次以後的迭代來說，除了其他in-batch的Sentence以外，我們還會將此次迭代前所生成的Distractor加入我們的NegativeSample中。透過NegativeSample和模型輸出的Distractor的dotproduct可以計算出ContrastiveLoss，此外我們也會加入Distractor和Label計算的CrossEntropyLoss，來計算出我們的TotalLoss15。ContrastiveLoss定義如下:x為生成的distractor，p為資料集中的Distractor(Label)，N為NegativeSampleSet，N={n1,n2,...nj}，其中包括先前迭代生成的Distractor以及其他in-batch的選項。7.基於NodeCentrality考量主題涵蓋性之選擇題題組生成如同前言所述，本研究議題之目標為考量主題涵蓋性之選擇題題組生成。因此，我們預計根據使用者所指定的主題，按Figure16架構規劃來進行題組生成。1.透過知識圖譜子圖萃取來涵蓋所指定主題之內容：藉由知識圖譜上NodeCentrality方式來萃取關鍵字，以為題幹之生成。我們的基本想法為在知識圖譜上我們定位出與使用者輸入之主題關鍵字相關之結點後，我們可以試者套入PageRank或者其他圖形上計算NodeCentrality重要節點挑選，並以重要節點為題幹生成之主軸。2.透過上一步取得之重要答案片段後，進行問題之生成。並生成較多的問題數量以為CandidateQuestionPool，而後將問題將題組生成問題設定一成為一選擇最佳化議題(Optimization)：根據主題生成大量的單一題目，而後定義UtilityScore來計算個別題目組合後之優劣，來挑選最佳化的題組以為輸出。於本研究項目中，我們預計探索以下兩種生成難度控制方式。•調整Masked-LM比例之反事實選項難度控制：現有的Masked-LM模型會將重要的專有名詞進行遮擋，模型會透過未遮擋的內容對該專有名詞進行預測。理想的情況下，當我們僅對該專有名詞進行遮擋時，模型可成功地預測出遮擋的專有名詞或語義相近的專有名詞，這不僅是因為MaskLM模型夠強大，也是因為未遮擋的內容提供模型足夠多的資訊讓模型預測，若未遮擋的內容提供的資訊不夠多，則模型預測出的內容有可能會與遮擋的內容語義相差很多。我們根據Masked-LM模型的此特性對題目的難易度進行控制，若我們將多個專有名詞進行遮擋，再讓模型預測時，模型會因為提供的資訊不足，而預測出語義較不相近的專有名詞，即較不可能的答案。以下表為例，我們可以觀察到當[Mask]越多時，我們要判斷該事實是否正確就越形容易。[Mask]借壓力差吸收[Mask]回微血管[Mask]借壓力差吸收[Mask]回[Mask]•調整ODQA檢索文章之難度控制：由於我們的基礎架構為基於文章檢索(DocumentRetrieving)來取得包含使用者所給定的關鍵字之文章，並使用該文章與關鍵字為問題生成模型之輸入，來生成選擇題問句。我們若挑選較為冷門之文章進行生成，則預期可以生成較為困難之題幹。而就干擾選項而言，我們可藉由調整輸入給閱讀理解模型之文章來產生難度較高的錯誤。具體而言，給定一有包含正確答案A之文章C，我們先將答案片段A於文章D中刪除，而後將其輸入至閱讀理解模型，藉此產生錯誤之閱讀理解結果，以這樣的錯誤結果當Distractor預期將有較高之誘導能力。相類似的想法也可以套用為挑選Retriever認為相關度比較不高的文章以為閱讀理解模型之輸入。此外，就生成題目之廣度與深度而言，我們可藉由控制取得之文章來調配題組之廣度與深度。例如，若我們以單篇或少數篇文章進行生成，我們所生成的主題題組針對測驗內容而言，應該可產生知識性較為深入之試題。而若針對較多篇文章，則應可產生範圍較廣本研究計畫將以三年為期，共八項研究議題進行，各研究產出如上文所示，並整合各技術成為一選擇題自動生成系統，預期的展示情境為：我們可於系統直接輸入一知識範圍，系統將利用我們所開發之技術先進行自動關鍵語萃取，而後進行各種類型的選擇題出題並考量題組關聯性。我們也將彙整各研究議題中所發展之技術為一個成為公開原始碼專案，以為後續相關研究之用。並將計畫執行過程所累積的資料，公開希望能吸引更多研究者之投入。對於學術研究、國家發展及其他應用方面預期之貢獻自然語言生成已是未來深度學習領域之熱門研究重點。透過本研究計畫之投入我們將可進行先驅之探索，建立學生團隊與自然語言處理與生成之能力，並持續過去之閱讀測驗自動出題研究成果，以知識類型選擇題自動出題為主要系統開發目標為例，希冀能為國內外探索不同的自然語言生成應用服務。此外若可順利完成也可以成為一成功範例利用人工智慧輔助教師進行測驗的客製化與個人化學習。對比於題庫類型的方式，透過我們技術我們將可動態產生各式各樣不同之考題為每個人量身定做難度適中之考題。就此一觀點出發，我們預期本計畫除學術研究價值外，同時也兼具商業價值，也應可成為EdTech(教育科技)領域之一環。再者，疫情改變了學習與教學的模式。同步與非同步之線上教學已成為教學模式常態之一。線上教學帶來了好處，但也帶來了顧慮。其中，學習的專注度應是諸多顧慮中主要令人擔憂的一環。隔著螢幕學生學習是否專注？目前的教學內容學生是否已跟上進度？等等問題是線上教學學習專注度所在意的問題。對此，我們認為適時與即時地檢測學生學習成效，來提醒學生與教師目前之學習成效，應能適度緩解線上學習專注度之顧慮。因此，我們也常見於MOOCs課程於一個授課單元結束後適時地給予測驗(多數以選擇題測驗)，來確認學生學習成效。然而，人工設計試題耗費心力，一成不變全班同學皆相同試題也可能透過即時訊息(instantmessage)之分享而取得答案，造成檢測之無效。對此，我們認為隨著深度學習技術之快速演進，結合自然語言(NaturalLanguageGeneration)生成技術來進行自動出題技術已成為可能；透過自然語言生成技術自動化出題，產生多樣化的試題，來進行個人化的檢測，將有除了可以增進研讀論文、撰寫程式、架設系統以及撰寫論文的基本能力之外，參與本計畫之研究人員，將可以學習到深度學習與自然語言生成技術之基本觀念、實際應用以及相關的核心技術，在這些基礎之下，透過積極的研究與討論，將可以提升參與計畫人員思考問題與解決困難的重要能力。我們預期可獲得具體技能如下:(1)設計與建立一個深度學習模型之能力，(2)整合不同先進技術之能力，(3)自然語言生成技術之瞭解，(4)資訊檢索相關知識與軟體套件之暸解，(5)佈建深度學習模型於WebScale之能力。"}, "1676211401.714697": {"單位": "", "標題": "1324234", "作者": "", "發布日": 0, "摘要": "", "全文": "rhbsrhwsrh:ndgsnsegnan"}, "1676211422.082183": {"單位": "", "標題": "1121121", "作者": "", "發布日": 0, "摘要": "", "全文": "'rgbwrgwgwgw:rsgvsvr"}, "1676211498.0740092": {"單位": "", "標題": "研究計畫之背景與目的", "作者": "", "發布日": 0, "摘要": "", "全文": "自然語言生成(NaturalLanguageGeneration)將是未來深度學習領域之熱門研究議題。2022年年末ChatGPT語言模型所帶來的示範性與展示效果預期將吸引更多研究目光。而個人於中興大學之研究團隊近年於自然語言生成之耕耘已建立初步研究成果。在過去朝著自然語言生成方向前進，我們探索自動生成試題之研究方向前進。於前期研究計畫「基於深度學習語言模型之閱讀測驗自動化生成研究109-2221-E-005-058-MY3」中，我們著眼於基於深度學習模型進行閱讀測驗(ReadingComprehensionTesting)題型之自動出題。在計畫執行過程中，我們•Ying-HongChan,Yao-ChungFan:KeywordProvisionQuestionGenerationforFacilitatingEducationalReadingComprehensionPreparation.INLG(InternationalNaturalLanguageGen-erationConference)2022.•Ho-LamChung,Ying-HongChan,Yao-ChungFan:ABERT-basedDistractorGenerationSchemewithMulti-taskingandNegativeAnswerTrainingStrategies.EMNLP(EmpiricalMethodsinNaturalLanguageProcessing)Findings2020.•Shang-HsuanChiang,Ssu-ChengWang,Yao-ChungFan:CDGP:AutomaticClozeDistractorGenerationbasedonPre-trainedLanguageModel.EMNLP(EmpiricalMethodsinNaturalLanguageProcessing)Findings2022.•Hsien-YungPeng,Ho-LamChung,Ying-HongChan,Yao-ChungFan:MisleadingInferenceGenerationviaProximalPolicyOptimization.PAKDD(AdvancesinKnowledgeDiscoveryandDataMining-26thPacific-AsiaConference)2022此外我們也將前期計畫之自動出題研究成果進行實作，可於以下兩個網址進行實測。•問句生成QuestionGeneration：給定文章、指定片段，透過自然語言生成模型進行問句生成。目前系統可於https://classic.queratorai.com/進行實測。•克漏字干擾選項生成DistractorGeneration：此一部分針對給定之文章進行克漏字誘導選項生成。目前系統可於：https://cdgp-demo.nlpnchu.org/進行實測。基於過去之研究成果累積，本計劃延續自然語言生成技術於自動出題之研究成果，擬定知識類型選擇題(MultipleChoiceQuestion,MCQ)自動生成為目標。以四技二專統一測驗考試為例(如Table2所示)，知識類型選擇題將包含題幹與四個選項(其中包含正確答案與干擾選項)，主要針對某一知識範圍進行測驗。我們認為知識範圍選擇題之自動生成可分為兩部分：(1)問題題幹(Stem)生成與(2)干擾選項(Distractor)生成。題幹生成主要目標為選擇題問題本身之生成，而干擾選項則著眼於誘導選項之生成。知識類型選擇題為學習測驗最常見的測驗模式之一。對比於閱讀測驗生成，知識類型選擇題的自動生成，將可減輕教師準備個人化教學測驗之負擔，也為學生提供更有效率之學習檢測方式。以現有我們成果為基礎，一個基本之自動選擇題出題系統作法為(如圖1)所示：我們可利用資訊檢索技術(如BM25Retriever)，從給定之文集資料庫(目前使用2021中文維基百科所有中文文章)中檢索主題字詞相關文章後，結合前期計畫所開發之問句生成模型(QuestionGeneration)[4]生成題幹，而後再利用詞向量模型(例如word2vec模型)找出相近字詞以為干擾選項。我們將這樣的構想先行實作一系統(稱之為Retrieve-and-GeneratePipeline)，目前此系統可於https://mcq-demo.nlpnchu.org/使用。使用者可藉由輸入主題相關字詞，選擇題生成系統將自動生成輸入字詞相關之選擇題結果。舉例而言，我們以「骨髓、白血病、淋巴」為主題字詞輸入至系統。我們挑選較佳之成果顯示於Table1。自動生成選擇題AutomaticGeneratedMCQQ1:哪一種癌症種類的統稱中可以看出它會造成不正常白血球的大量增生?(a)白血病、(b)乳腺癌、(c)腎病、(d)帕金森氏症Q1:白血病的發病原因通常是發生在哪一個器官上?(a)肝臟、(b)淋巴、(c)甲狀腺、(d)骨髓Q:白血病是最常見的兒童癌症，其中80%的病例是慢性的哪一種性病變?(a)淋巴結、(b)微血管、(c)淋巴、(d)黏膜Table1:目前選擇題自動生成結果範例。目前系統可於https://mcq-demo.nlpnchu.org/。目前我們挑選較佳之生成結果顯示於此Table中。建議審查委員自行輸入關鍵字詞組，來觀察我們Figure1:Retrieve-and-Generate基本架構。本例中我們以”骨髓、白血病、淋巴”為關鍵詞進行文章檢索，而再使用”骨髓”為答案，分別生成問句與干擾選項。從展示之結果中，我們發現自動選擇題出題之可能性。但仔細觀察會發現現有系統離實際應用仍有空間須待努力。也因此觸發本期研究計畫之提出。具體而言，我們認為選擇題自•句子等級之干擾選項生成(Sentence-LevelDistractorGeneration)首先，關於干擾選項的部分，若仰賴詞向量模型來產生干擾選項，我們所能取得的干擾選項便僅限於單詞(Word-Level)等級。從統測試題中，我們發現許多題目之干擾選項屬於句子等級(Sentence-Level)。以Table2中Q3與Q4選擇題為例，我們可以看到選項皆為句子等級之描述，此為目前基於詞向量來產生干擾選項的做法所不能。•反事實干擾選項生成(Counter-FactualSentenceGeneration)我們觀察到選擇題題型中，有著下列何者為錯誤或正確類型之考題。其干擾選項是一個較長的描述句。以Table2為例，Q5與Q6為分別詢問下列選項何者正確/錯誤。此題型，為變像之是非題。除了句子等級生成外，核心技術牽涉的是反事實(Counter-Factual)句子之生成。如何生成此類型之句子以為干擾選項也將是研究重點所在。•選擇題生成品質提升(MCQQuestionQualityImprovement)此外，我們也看到retreive-and-generatepipeline的做法，所產生的題幹本身有時是一個過於空泛之問題(例如，下列何種疾病為兒童常見之癌症？)或者並非語意正確之語句(例如，中樞性的什麼症狀是缺乏抗利尿激素的?)。如何有效判斷生成不良之語句，減低人工進一步過濾/挑選題目之負擔也是需進行研究之議題。此外，基於詞向量之干擾選項作法也有待商榷;隨機從相近詞向量挑選distractor，是否能確保distractor之誘導品類型I:單詞等級的干擾選項Word-LevelDistractorsQ1：淋巴器官是人體防禦系統中重要的一環，可提供淋巴細胞發育或活化所需要的條件與狀態。下列何者是淋巴球進行防禦與活化的器官？(a)骨髓(b)胸腺(c)肝臟(d)脾臟Q2：健康人體感覺神經傳入大腦的訊息大部分經過轉接，(a)視丘(b)下視丘(c)中腦(d)橋腦類型II:句子等級的干擾選項Sentence-LevelDistractors(a)無就巢性(b)體型大(c)著肉多(d)耐粗食Q4：下列何者為「聯合國國際糧農組織」(FAO)成立最重要的目標及首要任務？(a)消除及戰勝飢餓(b)聯絡及交流農業資訊(c)自然環境的永續及保護(d)實現國際間的公平貿易類型III:句子等級且與事實相違背的干擾選項Counter-FactualDistractorsQ5：有關人類上皮細胞的細胞週期之敘述，下列何者正確？(c)後期，分離的染色分體往細胞中央移動(d)末期，細胞膜內凹且細胞質開始一分為二Q6：有關一健康成人泌尿系統功能的敘述，下列何者錯誤？(a)腎臟每日約產生濾液180公升及尿液1.5公升(b)腎小管藉壓力差方式再吸收葡萄糖回微血管(c)腎臟藉排尿量多寡來維持體內水分的恆定(d)若出現尿毒症狀時，可使用血液透析方式進行治療Table2:四技二專統一測驗選擇題類型概觀。Q1與Q2擷取自110學年度護理基礎生物考科，Q3與Q4110學年度農業群農業概論考科，Q5與Q6取自110學年度農業群基礎生物考科質？也許所挑選的distractor並不具誘導性，使得受測者很輕易地可挑選出正確答案，或者挑選到答案的同義詞，造成一題有多個正確答案。根據先前計畫執行訪談教師所知，於測驗時為考量受測全體鑑別度，教師出題時會考量干擾選項彼此之間之難易度；通常三個干擾選項中，會有一個離正確答案較遠之干擾選項(誘導性較低)，一個次之，一個最為困難最具有誘導性。藉此來有適當的鑑別度。目前我們直接使用詞向量之作法並未考量此設計。•主題題組生成(MCQQuestionGroupGeneration)目前我們的系統僅為單一關鍵字詞之出題。我們認為一個良好的自動出題應該是以題組為單位進行出題；使用者指定一主題知識，系統根據此一主題知識範圍進行5-10題之選擇題自動生成，題目之間也應該彼此考量。目前Retreive-and-Generatepipeline的做法，我們使用使用者給定的關鍵字為答案，進行題目之生成。然而比較好的做法應為將使用者給定的關鍵字集合為知識範圍之指定，系統根據該範圍自動挑選重要概念關鍵字進行答案挑選，而後再進行選擇題題組之生成。針對上述之研究挑戰，於本研究規劃中我們擬定下列八項研究議題進行探討。•研究項目ㄧ：基於語言模型與知識圖譜之Text2Text干擾選項生成模型(DistractorGenerationbasedonText2TextLanguageModelandKnowledgeGraph)Figure2:Text2Text干擾選項生成架構Figure3:基於Masked-LM之反事實干擾選項現有之Distractor生成研究多是基於CandicateGenerating-and-Ranking架構，先透過知識庫(如Wordnet)或者透過MaskedLM的方式取得候選字後，再適當進行候選詞特徵排序以為最終干擾選項。對此我們有不同之想法，我們預計嘗試把干擾選項生成設定為一個Text2Text任務；如圖2所例，預期的輸入為題幹Q與答案A之串接字串，透過ModelEncoder與Decoder轉化輸出為一連串之干擾選項字串“重組DNA技術[SEP]電腦斷層掃描[SEP]組織培養”。我們透過語言生成模型直接將輸入對比轉換為輸出。此外，透過此一Text2Text方式，生成句子等級的干擾選項也將為可能。再者，近年來語言模型研究趨勢除了將模型參數量與訓練資料持續增加外，引入知識圖譜來強化生成模型於常識理解與推理能力也成為研究社群的關注所在[5][13]。也因此，探索整合知識圖譜於選擇題自動生成也因此成為我們規劃之研究項目。希冀藉由整合知識圖譜來提升干擾選項之誘導性與題幹生成之正確性。•研究項目二：基於Masked-LM之反事實干擾選項生成研究(Counter-FactualSentenceGenerationbasedonMasked-LM)如同前言所述，“下列何者正確/錯誤”題目為常見之類型選擇題題型。對此，我們預計嘗試以下之作法。首先，我們假定可取得正確敘述之語句S。我們先針對語句S中的字詞進行命名實體辨識(NamedEntityRecognition,NER)，其後再利用語言模型針對所取得之命名實體進行WholeWordMasking(WWM)，填入機率較低之填空結果，藉此生成反事實描述之語句。如Figure4所例，我們將所給定之事實描述，藉由NER方式挑選”葡萄糖”一次進行Masked，再藉由語言模型推測該Masked字詞，並挑選機率相對較低之預測填入。藉此創造出反事實之陳述句。然而，如何確保不挑選到同義詞？要進行多少個字詞之Masked？如何自動從主題文章中挑選事實陳述句以進行改寫？等等•研究項目三：基於知識圖譜強化之反事實干擾選項生成(Counter-FactualSentenceGen-erationbasedonKnowledge-GraphAugmentedLanguageModel)針對反事實干擾選項生成之議題，除了研究項目二之Masked-LM作法外，我們另一個構想為利用知識圖譜來強化Counter-Factual語句之生成。如Figure4所示，我們基本想法為先萃取與題幹相關聯之知識圖譜子圖(LocalSubgraph)並取得知識三元組，以此輸入至語言模型中，訓練語言模型反向改寫三元組語句，以此為反事實干擾選項。然而，此模式之訓練資料該如何取得？如何從知識圖譜上萃取子圖，並生成合適三元組？如何引入GraphNeuralNetwork來萃取結構？為此研究項目需進行探討之工作內容。•研究項目四：基於對比式學習之干擾選項生成模型強化(DistractorGenerationModelbasedonContrastiveLearning)我們過去的研究經驗[8]與相關研究[35]皆指出當使用Generative語言模型來生成句子Figure4:基於知識圖譜強化之反事實干擾選項生成構想層級之輸出時，模型經常會產生語意之相同，但字面的結果。如Table3所示，選項A跟C雖然字面敘述上不同，但實際上都是描述同樣的意思。生成重複選項將導致選擇副交感神經興奮會導致使心跳加速，血壓上升Table3:模型生成的誤導選項重複出現的範例對此，我們預計探索對比式學習(ContrastiveLearning)[7][35]來提升生成品質。對比式學習於自然語言之應用中主要想法為透過選取PositiveSample以及NegativSample使得模型去學習到生成句子跟同義句子的相同以及跟異類句子的不同之處，這樣我們可以使用重複選項當作我們的NegativeSample，然後資料集中的干擾選項當作PostiveSample，期望模型可以學習避免生成出重複的誤導選項。因此，我們預計探索對比式•研究項目五：使用Open-DomainQuestionAnswering錯誤結果之干擾選項挑選研究(Us-ingIncorrectAnswersfromODQAasDistractors)而除了利用生成模型產生Distractors外，我們發現利用Open-DomainQuestionAnswer-ing(ODQA)模型所產生的錯誤結果是一個值得探討的干擾選項的取得方式。具體而言，現有的ODQA模型主要包含一個Retriever與一個Reader，Retriever負責檢索相關的文章，而Reader利用檢索出來的文章利用閱讀理解模型(ReadingComprehensionModel)來找尋答案片段。由於ODQA並非完美，因此也會有錯誤結果之回報。但我們發現錯誤結果與正確答案並非天差地遠，閱讀理解模型所回報之結果通常也與答案接以圖5為例，當我們下達Q：「下列何者為目前世界上應用最廣的殺草劑，其具備有生長素之特性，常用於闊葉雜草防治，亦為選擇性殺草劑的一種？」，ODQA系統將取得圖5中右半部之答案：「萌前除草劑、2,4-D、非選擇性除草劑、10殺丹粒劑」。其中，2,4-D為正確答案，餘者為錯誤答案。然而我們覺得這些錯誤答案，可以當作Q之干擾選項來使用。有別於基於生成模型研究項目，利用ODQA模型之錯誤答案來當作干擾選項應可提供不同思路進行干擾選項生成。這樣的想法乍看可行，但仍有若干技術點需進行探討。諸如，如何控制ODQA模型之錯誤？我們需要在Reader部分弱化Reader的能力，或者弱化Retriever能力，來製造ODQA的錯誤？如何從錯誤回報結果挑選干擾選項？這些皆是尚待深入研究之議題。也因此觸發本研究項目之規劃。Figure5:利用ODQA模型之錯誤答案來當作Distractors•研究項目六：基於NodeCentrality考量主題涵蓋性之選擇題題組生成(MCQGroupGenerationconsideringTopicCoveragebasedGraphNodeCentrality)上述所規劃的選擇題生成做法，主要針對單題進行生成。若要產生多題/題組考題，就是重複相同流程，獨立產生個別考題後，整合為一題組。然而，題組生成做法應當考慮題與題之間之關聯性與互補性。因此，我們認為比較好的做法應當為根據使用者所指定的主題，透過(1)知識圖譜子圖萃取來涵蓋所指定主題之內容與(2)透過主題重要概念萃取：藉由知識圖譜上NodeCentrality方式來萃取關鍵字，以為題幹之生成。此外，如何根據主題互補性，來挑選個別生成之題組，用以組成選擇題題組也是需進行•研究項目七：考量題幹可用性與干擾選項間關聯之Retrieve-and-Generate架構優化(Op-timizingRetrieve-and-GenerateFrameworkconsideringStemUtilityandDistractorCorrela-tion)本研究項目著眼於優化現有Retrieve-and-Generate架構設定。Retrieve-and-Generate架構雖然簡單，但我們可以發現其於單字等級的干擾選項生成已有一定的可用性。我們預計可優化之處為：(1)我們發現引用現有系統於題幹生成時，會有題幹生成結果不佳的狀況。對此，我們發現其中的一個原因為Retriever所檢索之文章品質不佳(文章過短或者與主題不相關)，導致不良之題幹生成。因此如何改善檢索機制來挑選較佳的文章用以進行生成，便是值得探討的課題。再者，題幹生成不佳也有部分原因是來自於問句生成模型本身之生成錯誤，如何有效過濾生成不佳的題幹也因此是一個可需進行研究之項目。(2)如同前言所述，干擾選項之挑選應該考量彼此間之關聯，而非獨立挑選。如何納入此觀點來進行干擾選項之生成也因此成為預計關注之議題。又再者，我們基於word2vec方式之字詞等級干擾選項挑選是否可以引入語言模型之詞向量來求取•研究項目八：生成難度可控之選擇題生成研究Difficulty-ControlledMCQGeneration發展自動化生成考題之可能願景為：針對個人化學習自對應生成難度適中之測驗，以增加學習與檢測之效率。因此，如何針對現有技術探討生成結果難度控制也因此成為一可探索之研究議題。由於我們的基礎架構為基於文章檢索來取得包含使用者所給定的關鍵字之文章，並使用該文章與關鍵字為問題生成模型之輸入，來生成選擇題問句。我們若挑選較為冷門之文章進行生成，則預期可以生成較為困難之題幹。而就干擾選項而言，若我們基於研究項目五「使用Open-DomainQuestionAnswering錯誤結果之干擾選項挑選研究」，利用ODQA來取得干擾選項之作法。我們可藉由調整輸入給閱讀理解模型之文章來產生難度較高的錯誤。具體而言，給定一有包含正確答案A之文章C，我們先將答案片段A於文章D中刪除，而後將其輸入至閱讀理解模型，藉此產生錯誤之閱讀理解結果，以這樣的錯誤結果當干擾選項預期將有較高之誘導能力。相類似的想法也可以套用為挑選Retriever認為相關度比較不高的文章以為閱讀理解模型之輸入。此外，就生成題目之廣度與深度而言，我們可藉由控制取得之文章來調配題組之廣度與深度。例如，若我們以單篇或少數篇文章進行生成，我們所生成的主題題組針對測驗內容而言，應該可產生知識性較為深入之試題。而若針對較多篇文章，則應可產生範圍較廣之測驗試題。上述之研究議題，乍看可行但仍有許多細節需進行探索，如上述研究背景目的所述，本研究計畫擬探索基於現有Retrieve-and-GeneratePipeline之選擇題自動生成架構進行強化。我們以三年為期，規劃在此目標下擬定八項關鍵技術進行研究探討。如Figure6所示，八個工作項目又可區分為三項主軸：「選擇題生成品質提升」、「反事實選項生成」與「群組生成與難度控制」。其中，於「選擇題生成品質提升」研究主軸中，我們探索利用語言模型與知識圖譜架構來生成，將選擇題選項生成問題設定成為一Text2Text任務，輸入為問題題幹與正確答案所串接之TextSequence，而輸出為所生成之distractor串接之TextSequence。此外，我們也嘗試引入對比式學習來改善Text2Text生成模式之品質。我們也預期規劃探索利用ODQA方式來取得干擾選項。而於「反事實選項生成」研究主軸中，我們探索反事實陳述句之生成。我們預計嘗試基於MaskedLM取代重要名詞之反事實語句生成作法與基於知識圖譜知識三元組萃取改寫之作法。於第三個研究主軸中，我們則嘗試針對題組生成與難度可控制之干擾選項生成進行研究。MultipleChoice,Multi-此資料集特色在於為Multi-hop形式，答案需從複數文檔中推理而成；資料來自維基百科以及PubMed，PubMed為生物醫學論文的搜尋引擎，在兩個來源各自提供51,318(Wiki)和2,508(PubMed)筆問題。資料來自國高中英文考試，提供超過10萬筆問題以及2.8萬篇文本，題型上為英文閱讀測驗，會需要閱讀故事或對話紀錄，而針對該文本會有複數題目，每個題目此資料集的挑戰在於，內容來自於7個不同領域(新聞、小學自然科學、旅遊、小說故事等等)，為文本與題目提供多樣性，總計約1萬筆問題與800篇文本。MultipleChoice,Open-為Open-domain形式，並不提供文本，只有問題與答案；包含12,247筆問題、一個正確答案和四個干擾選項；題目類型為知識與常識題，來自於ConceptNet與同時包含兩種形式資料，填空題與Open-domain；填空題形式命名為QUASAR-S，其中資料來源自StackOverflow，提供約37K筆問題。Open-domain形式命名為QUASAR-T，提供約43K筆問題。來自StanfordUniversity的閱讀理解資料集，包含了超過100K筆問題和536篇對應的維基百科文本；延續了SQuAD1.1的題目外，[31]另外加入約50K筆無答案的題目，進一步測試機器能否在無對應答案時回答否，而MedQA是基於美國醫師執照考試(USMLE)所產生的資料集，其中問題皆為較為困難的專業醫學相關的知識，且包含了英文、簡體中文、繁體中文分別有約12K、34K、14K個問題集，這種專業的知識能使機器針對單一領域產生更具參考價值的distractor，目前此任務排名上最佳為ValentinLiévin團隊做出的Codex5-shot訓練機器學習模型進行閱讀理解(MachineReadingComprehensionTask)為最常見的深度學習自然語言任務。諸如，SQuAD[32]、RACE[21]、DROP[10]與HotPotQA[41]等資料集，皆是測驗/訓練機器學習模型於答案找尋、邏輯推理與摘要之能力。其中，許多機器閱讀理解任務採用多選題方式進行測驗。選擇題的自動答題與選擇題的自動生成為一體兩面之應用。因此具有選擇題樣態的資料集為我們選擇題自動生成研究之根本資料集，我們將目前所探知之相選擇題與干擾選項生成之相關研究可按應用類型可區隔為英文學習測驗與知識學習測驗。干擾選項生成研究於英文學習測驗應用，主要以自動生成閱讀理解與克漏字類型的考題為目標。閱讀理解之干擾選項生成與本研究計畫之知識類型選擇題之自動出題目標不同。因此本研究回顧中，我們不做此分類每一研究細部說明。我們將此類型之相關研究按不同象限做分類：(1)干擾選項層級之差異(生成字詞等級與句子等級之差異)象限，(2)題目類型(閱讀理解與克漏字測驗)，(3)生成方式(CandidateRanking與Generative)。我們按此三象限，將目前之相關研究摘要整理於Table5中。而知識學習測驗類型的選擇題生成，則於這一兩年逐漸吸引相關研究投入。我們認為與本研究計畫較為相關之研究有[22][40][3]，以下我們針對個別文獻討論如下。首先，與我們有相同目標者為[22]，也是針對特定知識主題生成選擇題。其主要作法Table5:現有閱讀測驗與克漏字干擾選項生成研究整理表為：先由人工給定一個主題，程式使用該主題名稱的維基百科頁面作為起始點(例如給定“體育”後搜尋體育相關維基百科)，保留熱門文章並取出文章的第一段擷取摘要，由此產生大量句子。並將這些句子依單純英文文法規則生成簡單5W問句與答案，例如who問句是將第一個動詞前的名詞作為正確答案並從句子中移除，what問句是將第一個夾在動詞間的名詞移除作為正確答案並從句子中移除，移除答案後的句子再帶入問題模板生成制式問句。生出五種問句後再依英文文法的詞與詞的相關性去決定哪個問句最適合。在干擾選項生成的部分，先隨機選取其他句子所產生的答案，再藉由語言模型去排序與問題的相關性，取前三高之預測為干擾選項。但本論文並未使用問句生成模型，採取之作法也與本計畫所規劃有所不同。於論文中[40]，則探討如何讓干擾選項更加多樣且與正確答案相差不遠(以具誘導性)。對此，創造了一款監督式分類任務來找出文章中屬於關鍵句子(及對產生干擾選項生成有用的句子)，作者在訓練之前會將訓練用的干擾選項和文章中的每個句子做比對，若該句子和干擾選項相似度達到一定程度，則該句子將成為文章中的關鍵句，透過這個方式在文章中能被修改成為新的干擾選項的句子將會達到多樣性的效果，且透過相似度對比，干擾選項也不會跟原始文章相差太遠從而提升干擾選項的品質。本研究之作法可供我們借鏡參考對關鍵文章/句選擇設計，用以提升干擾選項之多樣性。而與本計畫研究項目五「使用Open-DomainQuestionAnswer錯誤結果之干擾選項」走非生成想法相似之研究為[3]。其主要想法為作者發現於干擾選項資料集中，有許多干擾選項一再被使用。促使論文作者想為新的問題檢索以重複使用干擾選項。作者提出一個Transformer-baseddistractorretrievalmodels，該模型通過考慮問題的上下文來選擇干擾選項模型包含三個部分，分別為D-SIM模型、Q-SIM模型以及DQ-SIM模型。在D-SIM模型中使用mBERT計算同一個MCQ中問題和答案以及對應的干擾選項的向量，並計算他們之間的相似度。而在Q-SIM模型中作者假設不同問題共享一個或多個干擾選項或答案，代表問題可能在語意上相關，因此他們相關的干擾選項可以用作彼此的良好候選干擾選項，因此作者將訓練資料重新排序，使這些共享至少一個干擾選項或答案的問題聚集在一起，接著將資料送進Q-SIM模型計算不同的MCQ中問題之間的相似度。在DQ-SIM模型線性結合前兩個模型，從各自的優勢中獲益，透過基於分數和基於排名的合併策略計算來自D-SIM模型和Q-SIM模型的分數，並設計演算法使得高排名之干擾選項有較大的權重。重複使用現有資料集中之干擾選項為一創新想法，但仍與我們之ODQA做法有所不同。[OpenDomainQuestionAnswering相關研究]本計畫的選擇題生成架構主要基於兩元件:(1)文章萃取與(2)問句生成。目前我們所使用的文章萃取機制為傳統之BM25檢索。而近年來由於ODQA研究領域之發展，基於深度學習之DenseRetrieval技巧也紛紛於ODQA研究領域中所提出。我們預期DenseRetrieval技術也將於我們知識類型選擇題自動出題中擔任關鍵角色。茲簡要回顧現有ODQA之研究如下。ODQA架構中Retriver之重要性不言可喻。不同層級的語意檢索也影響著ODQA的效能。於[28]研究中，作者提出一個簡單而有效的pipeline系統，研究段落和句子級別語義檢索及其對下游任務的潛在影響，作法上是先檢索段落級別的段落，然後從上一步檢索後取得的段落中再去檢索句子級別的段落。該系統在事實驗證和Multi-hopQA方面進行評估，實驗結果顯示QA任務與事實驗證皆仰來上游檢索之結果。於[23]論文中，作者在ODQA架構的retreiver端提出了一個有效率的弱監督式學習的方法(InverseClozeTask)來對retreiver進行預訓練，給予模型完全無人工標記的維基百科原始資料，藉由論文中的方法將資料切分成固定長度的段落，並隨機選取其中的一個句子當作標籤，讓模型去學習如何從給定的句子去資料庫中找尋與之最相關的文章。在實驗數據中此方法訓練出的ODQA模型分數高於傳統的BM25檢索方法。而於DPR[16]論文中，作者為解決傳統BM25無法檢索語意相似段落的問題，提出了基於深度學習模型之DensePassageRetrieval，並提出使用in-batchnegative的技術，在密集向量的檢索取得極佳的效果改進。我們也預計引入中文DRP模型來提升目前題幹生成所需之而於GAR[27]研究中，作者們探討傳統基於BM25的檢索無法檢索沒有單詞重疊或語意相似的段落的問題，而基於密集向量的檢索方式在計算上又過於複雜，且會有資訊損失的問題。該論文透過預訓練語言模型自動生成與查詢語句相關的內容，可增加查詢語句的資訊量，隨後透過BM25進行檢索，該方法比起傳統BM25能檢索到更精確的段落，且執行效率也快於基於密集向量的檢索方式。透過GAR中的PLM可對查詢語句的資訊進行補充，原論文中是補充答案、相關段落及段落標題，可基於該手法讓PLM生成問題與原始答案的關聯，對這些關聯進行組合或刪除即可檢索與原答案有相似關係的查詢語句，可檢索到適合製作干於[19]，作者觀察到基於語言模型之檢索技術雖然效果很好，但比以往的方法需要極高的計算成本，特別是在計算每個query與document間的relevancescore時。本篇論文作者提出了ColBERT，引入lateinteraction架構，對query與document進行獨立的encoding，於模型輸出最終階段才進行相似度運算。而[18]中，則使用近一步基於ColBERT當基礎模型來進行retreiver端的訓練，使用了Relevance-guidedSupervision(RGS)的方法來訓練retreiver的模型。相較於傳統的retreiver訓練資料，傳統方法需人工標註的訓練資料，但作者認為人工標住資料往往存在偏見，進而影響模型的效能，所以在論文中提出了用訓練好的模型來找尋最適合的訓練資料，並把這些資料用於訓練下一個模型，使用這個方式來進行後續的訓練，最後得出最佳的模型。此方法訓練出的模型分數明顯高於目前opendomain的其他方式。在找尋訓練資料時，模型會先搜尋前k筆資料，並把前20筆資料中含有shortanswer的資料當成possitiveexample，而前20筆以外剩餘的資料中不含有shortanswer的資料當成negativeexample，此方法尋找的負面訓練資料交由reader進行預測能得到與正確答案相關的錯誤答案，如能改良ranking的方式，應用於干擾選項生成，預期將能提升干擾選項之品質。[ContrastiveLearning於自然語言生成相關研究]對比式學習始見於電腦視覺領域，其於影像應用上有著顯著的效果提升。因此，許多自然語言處理之研究者也將其引用至自然語言生成領域，諸如[11][7][35]。於[11]研究中，作者們提出了使用ContrastiveLearning的作法來學習SentenceEmbedding，他們使用BERT作為基礎模型，透過不同的dropoutmask使模型能用同樣的輸入產生不同的SentenceEmbedding，但它們之間的距離會是近的，也就可作為PositiveSample。然後再使用其他in-batch的句子作為NegativeSample，以達到非監督式訓練。而在有訓練資料的情況下，如NLI資料集，則可以將Eentailment視為PositiveSample，Contradiction以及其他in-batch的句子視為Negative而再基於[11]研究後，於[7]研究中，作者們提出加上了Discriminator來加強SentenceEmbedding的學習效果。具體而言，對每筆輸入來說，會透過SentenceEncoder產生一個SentenceEmbedding，同時也會隨機的Masking輸入句子其中的幾個Token，並透過一個fixed的Generator來還原MaskToken，而Discriminator要試著透過沒被修改過的SentenceEmbedding的資訊去辨認修改後的句子中的哪一個Token有被修改過，整個框架會透過SentenceEmbedding的ContrastiveLearningLoss以及Discriminator的ReplacedTokenDetection而與我們預計引入對比式學習來提升句子等級的干擾選項生成效果想法接近的研究也可見於[35]。該研究指出發現了以往autoregressive模型使用BeamSearch去決定要生成的文本時，會產生語意不自然以及沒意義的重複文字，儘管後續有人使用取樣以及修改ObjectiveFunction來嘗試解決這個問題，但這個方法時常會產生語意不連貫的問題。因此作者提出了使用ContrastiveLearningLoss加上原先的MaximumLikelihoodEstimationLoss來訓練模型，並且使用ContrastiveSearch來作為decodingmethod。[Counter-FactualSentenceGeneration相關研究]於Counter-FactualGeneration研究中，[44,33]為相關研究之先驅。主要想法是使用神經網路來生成反事實敘述，這個概念背後的想法是同時使用語言模型和各種採樣的策略來達成生成反事實敘述的效果。接續之前的成果，[39]提出了Polyjuice架構，可以控制輸出的反事實敘述的類型。另外一種生成干擾選項的研究是使用干擾選項生成(DistractorGeneration)的手法，干擾選項生成(DistractorGeneration)的目標是給予了一篇文章、問題和正確答案後，模型能夠生成似是而非的干擾選項。於研究[24]中，作者最先提出使用了神經網路來生成干擾選項，而研究[8]則延續了前者的成果，把重點放在生成複數的干擾選項並提出了一個負樣本學習策略(negativetrainingstrategy)來加強干擾選項生成。但這上述兩個研究主要都著眼於閱讀理解題型中干擾選項之生成，與本研究知識類型選擇題生成目標所有不同。而在關於AbductiveInferenceGeneration的相關研究中，於[30]中作者提出了故事改寫的任務，此任務目標是要改寫一段故事的敘述並且導致產生與原先不同的結局，與顯永的論文的方式的差異為故事改寫任務是根據給定的誤導推論來生成不同的結局，在MIG任務中是給定敘述句與其前後文來生成誤導推論。[2]研究中是讓模型根據上下文來推論中間適合加入的句子，此研究主要是產生正確的推論敘述，而在MIG任務[29]主要是產生相反的誤導推論敘述。於我們先前的研究[29]中，我們提出一個自然語言的誤導推論生成任務，任務步驟是當給定了一個陳述事實的句子並給定其上下文，將其定義為(x,y,z)與誤導推論ˆy，模型會生成與該事實相反的推論句子，由此可以讓模型產生出多選題似是而非的干擾選項。然而這樣任務與我們所設定之反事實出題仍有所不同。而與我們生成Counter-Factual陳述句想法接近的研究為：[46]。於該研究中，作者探討有別於過往的是非題，將文章中原始句子作為True的問句並通過反義詞替換關鍵字生成False問句，本篇作者通過使用一種新穎的masking-and-infilling策略來生成更靈活與複雜的問句。要被mask的關鍵字找法有很多種，像是SRL結果中的predicate，句子中連接詞(”that”、”when”、”since”)之後的部分或是數字等等，來找出值得提問的句子。Infilling的作法則是以BART為基底作為生成模型，將原始句子的關鍵字mask後，提供前後句子作為BART的上下文後產生句子填入。作者為了找值得提問的句子所提出的方法亦可用來作為多選題產生題目的答案選擇方法，而infilling的想法如果使用得宜，在提供出題範圍的前後文後，也能將欲出題句子挖空讓BART模型填補後產生選項。雖然與我們目標相似，但仍與我整合知識圖譜與語言模型為近年語言模型訓練熱門關注議題。於[42]研究中，作者們提出一CounterfactualGenerationControllableCounterfactualGenerationDistractorGenerationMisleadingInferenceGeneration種能將文本和知識圖譜的資訊進行深度雙向、自我監督的預訓練的方法，透過作者的方法可以將一些特定領域的KnowledgeBase資料集結合相關文獻去訓練語言知識模型，讓語言模型和知識圖譜之間的資訊互相交換，並且在回答特定領域問答的任務上，表現優於現有的語言模型和知識圖譜增強模型。我們想透過作者提供的預訓練模型，來做多選題選項生成，透過知識圖譜所額外提供的資訊，讓在考慮生成選項時，除了題目和答案的文字外，用這些文字從知識圖譜萃取相關的知識圖譜子圖幫助生成選項有更多參考資訊。知識圖譜於問句生成之研究也見於[5][13]。於[5]研究中，作者之想法為使用知識圖譜進行問題生成任務時，通常只會取出資料與關係的三元組合，並不能完整利用知識圖譜中豐富的資訊，因此作者基於GraphNeuralNetworks提出了BidirectionalGatedGraphNeuralNetwork(BiGGNN)，將知識圖譜中的node及edge編碼成向量，使得每個node都包含了周圍其他node的資訊，並對答案的node進行標註，隨後透過RNN進行問句生成，生成過程中RNN會決定生成新文字或直接複製某個node的資訊放進問題。我們應可借鏡此研究論文的想法，可將知識圖譜及答案的資訊完整編碼，可基於此特性，在生成問題時同時考慮答案附近的node，讓模型在建立問題的同時也能根據附近的node產生干擾選項。而於[13]研究中，則提出的ISEEQ架構，主要目的是希望從簡短的敘述句中去尋找更多訊息來反問使用者是否符合關於敘述句背後未提及的情況，在使用時，使用者只需輸入簡短的敘述句，模型會先從知識圖譜擴展並增加query的關鍵字，透過這些關鍵字模型就能retrieve更多相關的文章進行ranking，在從這些文章中使用QG模型產生一些原始query中未提及但可能相關背景知識，例如當使用者說道:“我感到疲勞且渾身無力”，ISEEQ能回覆:“請問病徵持續了多久?”，或是“你最近14天有出國過嗎?”這類可能與其他知識相關的問句。這種方式最大的貢獻及是讓機器學習到”反問”的能力，而非一味的透過query生成出結果。這樣的想法，或許可引用來提升題幹生成的精確度，舉例來說:有位教師想出關於日治時期的歷史題目，我們就能先藉由ISEEQ來詢問是要出與日治時期戰爭相關的題目或是與日治時期建設相關的題目，以此來取得更精確的題幹生成需求。[1]J.Araki,D.Rajagopal,S.Sankaranarayanan,S.Holm,Y.Yamakawa,andT.Mitamura.Gen-eratingquestionsandmultiple-choiceanswersusingsemanticanalysisoftexts.InProceedingsofCOLING2016,the26thInternationalConferenceonComputationalLinguistics:TechnicalPapers,pages1125–1136,2016.[2]C.Bhagavatula,R.L.Bras,C.Malaviya,K.Sakaguchi,A.Holtzman,H.Rashkin,D.Downey,S.W.-t.Yih,andY.Choi.Abductivecommonsensereasoning.arXivpreprintarXiv:1908.05739,[3]S.K.Bitew,A.Hadifar,L.Sterckx,J.Deleu,C.Develder,andT.Demeester.Learningtoreusedistractorstosupportmultiplechoicequestiongenerationineducation.IEEETransactionsonLearningTechnologies,2022.[4]Y.-H.ChanandY.-C.Fan.Arecurrentbert-basedmodelforquestiongeneration.InProceedingsofthe2ndWorkshoponMachineReadingforQuestionAnswering,pages154–162,2019.[5]Y.Chen,L.Wu,andM.J.Zaki.Towardsubgraphguidedknowledgegraphquestiongenerationwithgraphneuralnetworks.arXivpreprintarXiv:2004.06015,2020.[6]S.-H.Chiang,S.-C.Wang,andY.-C.Fan.Cdgp:Automaticclozedistractorgenerationbasedonpre-trainedlanugagemodel.InFindingsoftheAssociationforComputationalLinguistics:[7]Y.-S.Chuang,R.Dangovski,H.Luo,Y.Zhang,S.Chang,M.Soljac￿ic￿,S.-W.Li,W.-t.Yih,Y.Kim,andJ.Glass.Diffcse:Difference-basedcontrastivelearningforsentenceembeddings.arXivpreprintarXiv:2204.10298,2022.[8]H.-L.Chung,Y.-H.Chan,andY.-C.Fan.Abert-baseddistractorgenerationschemewithmulti-taskingandnegativeanswertrainingstrategies.InFindingsoftheAssociationforComputationalLinguistics:EMNLP2020,pages4390–4400,2020.[9]B.Dhingra,K.Mazaitis,andW.W.Cohen.Quasar:Datasetsforquestionansweringbysearchandreading.arXivpreprintarXiv:1707.03904,2017.[10]D.Dua,Y.Wang,P.Dasigi,G.Stanovsky,S.Singh,andM.Gardner.ingcomprehensionbenchmarkrequiringdiscretereasoningoverparagraphs.arXiv:1903.00161,2019.[11]T.Gao,X.Yao,andD.Chen.Simcse:Simplecontrastivelearningofsentenceembeddings.arXivpreprintarXiv:2104.08821,2021.[12]Y.Gao,L.Bing,P.Li,I.King,andM.R.Lyu.Generatingdistractorsforreadingcomprehen-sionquestionsfromrealexaminations.InProceedingsoftheAAAIConferenceonArtificialIntelligence,volume33,pages6423–6430,2019.[13]M.Gaur,K.Gunaratna,V.Srinivasan,andH.Jin.Iseeq:Informationseekingquestiongenerationusingdynamicmeta-informationretrievalandknowledgegraphs.InProceedingsoftheAAAIConferenceonArtificialIntelligence,volume36,pages10672–10680,2022.[14]Q.Guo,C.Kulkarni,A.Kittur,J.P.Bigham,andE.Brunskill.Questimator:Generatingknowl-edgeassessmentsforarbitrarytopics.InIJCAI-16:ProceedingsoftheAAAITwenty-FifthInternationalJointConferenceonArtificialIntelligence,2016.[15]D.Jin,E.Pan,N.Oufattole,W.-H.Weng,H.Fang,andP.Szolovits.Whatdiseasedoesthispa-tienthave?alarge-scaleopendomainquestionansweringdatasetfrommedicalexams.AppliedSciences,11(14):6421,2021.[16]V.Karpukhin,B.Og￿uz,S.Min,P.Lewis,L.Wu,S.Edunov,D.Chen,andW.-t.Yih.Densepassageretrievalforopen-domainquestionanswering.arXivpreprintarXiv:2004.04906,2020.[17]D.Khashabi,S.Chaturvedi,M.Roth,S.Upadhyay,andD.Roth.Lookingbeyondthesurface:Achallengesetforreadingcomprehensionovermultiplesentences.InProceedingsofthe2018ConferenceoftheNorthAmericanChapteroftheAssociationforComputationalLinguistics:HumanLanguageTechnologies,Volume1(LongPapers),pages252–262,2018.[18]O.Khattab,C.Potts,andM.Zaharia.Relevance-guidedsupervisionforopenqawithcolbert.TransactionsoftheAssociationforComputationalLinguistics,9:929–944,2021.[19]O.KhattabandM.Zaharia.Colbert:Efficientandeffectivepassagesearchviacontextualizedlateinteractionoverbert.InProceedingsofthe43rdInternationalACMSIGIRconferenceonresearchanddevelopmentinInformationRetrieval,pages39–48,2020.[20]G.Kumar,R.E.Banchs,andL.F.D’Haro.Revup:Automaticgap-fillquestiongenerationfromeducationaltexts.InProceedingsoftheTenthWorkshoponInnovativeUseofNLPforBuildingEducationalApplications,pages154–161,2015.[21]G.Lai,Q.Xie,H.Liu,Y.Yang,andE.Hovy.Race:Large-scalereadingcomprehensiondatasetfromexaminations.arXivpreprintarXiv:1704.04683,2017.[22]G.LeBerre,C.Cerisara,P.Langlais,andG.Lapalme.Unsupervisedmultiple-choicequestiongenerationforout-of-domainq&afine-tuning.InProceedingsofthe60thAnnualMeetingoftheAssociationforComputationalLinguistics(Volume2:ShortPapers),pages732–738,2022.[23]K.Lee,M.-W.Chang,andK.Toutanova.Latentretrievalforweaklysupervisedopendomainquestionanswering.arXivpreprintarXiv:1906.00300,2019.[24]C.Liang,X.Yang,N.Dave,D.Wham,B.Pursel,andC.L.Giles.Distractorgenerationformultiplechoicequestionsusinglearningtorank.InProceedingsofthethirteenthworkshoponinnovativeuseofNLPforbuildingeducationalapplications,pages284–290,2018.[25]C.Liang,X.Yang,D.Wham,B.Pursel,R.Passonneaur,andC.L.Giles.Distractorgenera-tionwithgenerativeadversarialnetsforautomaticallycreatingfill-in-the-blankquestions.InProceedingsoftheKnowledgeCaptureConference,pages1–4,2017.[26]V.Liévin,C.E.Hother,andO.Winther.Canlargelanguagemodelsreasonaboutmedicalquestions?arXivpreprintarXiv:2207.08143,2022.[27]Y.Mao,P.He,X.Liu,Y.Shen,J.Gao,J.Han,andW.Chen.Generation-augmentedretrievalforopen-domainquestionanswering.arXivpreprintarXiv:2009.08553,2020.[28]Y.Nie,S.Wang,andM.Bansal.Revealingtheimportanceofsemanticretrievalformachinereadingatscale.arXivpreprintarXiv:1909.08041,2019.[29]H.-Y.Peng,H.-L.Chung,Y.-H.Chan,andY.-C.Fan.Misleadinginferencegenerationviaproxi-malpolicyoptimization.InPacific-AsiaConferenceonKnowledgeDiscoveryandDataMining,pages497–509.Springer,2022.[30]L.Qin,A.Bosselut,A.Holtzman,C.Bhagavatula,E.Clark,andY.Choi.Counterfactualstoryreasoningandgeneration.arXivpreprintarXiv:1909.04076,2019.[31]P.Rajpurkar,R.Jia,andP.Liang.Knowwhatyoudon’tknow:Unanswerablequestionsforsquad.arXivpreprintarXiv:1806.03822,2018.[32]P.Rajpurkar,J.Zhang,K.Lopyrev,andP.Liang.Squad:100,000+questionsformachinecomprehensionoftext.arXivpreprintarXiv:1606.05250,2016.[33]S.Ren,Y.Deng,K.He,andW.Che.Generatingnaturallanguageadversarialexamplesthroughprobabilityweightedwordsaliency.InProceedingsofthe57thannualmeetingoftheassociationforcomputationallinguistics,pages1085–1097,2019.[34]S.RenandK.Q.Zhu.Knowledge-drivendistractorgenerationforcloze-stylemultiplechoicequestions.InProceedingsoftheAAAIConferenceonArtificialIntelligence,volume35,pages[35]Y.Su,T.Lan,Y.Wang,D.Yogatama,L.Kong,andN.Collier.Acontrastiveframeworkforneuraltextgeneration.arXivpreprintarXiv:2202.06417,2022.[36]A.Talmor,J.Herzig,N.Lourie,andJ.Berant.Commonsenseqa:Aquestionansweringchallengetargetingcommonsenseknowledge.arXivpreprintarXiv:1811.00937,2018.[37]J.Welbl,N.F.Liu,andM.Gardner.Crowdsourcingmultiplechoicesciencequestions.arXivpreprintarXiv:1707.06209,2017.[38]J.Welbl,P.Stenetorp,andS.Riedel.Constructingdatasetsformulti-hopreadingcomprehensionacrossdocuments.TransactionsoftheAssociationforComputationalLinguistics,6:287–302,[39]T.Wu,M.T.Ribeiro,J.Heer,andD.S.Weld.Polyjuice:Automated,general-purposecounter-factualgeneration.arXivpreprintarXiv:2101.00288,1(2),2021.[40]J.Xie,N.Peng,Y.Cai,T.Wang,andQ.Huang.Diversedistractorgenerationforconstruct-inghigh-qualitymultiplechoicequestions.IEEE/ACMTransactionsonAudio,Speech,andLanguageProcessing,30:280–291,2021.[41]Z.Yang,P.Qi,S.Zhang,Y.Bengio,W.W.Cohen,R.Salakhutdinov,andC.D.Manning.Hotpotqa:Adatasetfordiverse,explainablemulti-hopquestionanswering.arXiv:1809.09600,2018.[42]M.Yasunaga,A.Bosselut,H.Ren,X.Zhang,C.D.Manning,P.Liang,andJ.Leskovec.Deepbidirectionallanguage-knowledgegraphpretraining.arXivpreprintarXiv:2210.09338,2022.[43]C.Y.Yeung,J.S.Lee,andB.K.Tsou.Difficulty-awaredistractorgenerationforgap-fillitems.InProceedingsoftheThe17thAnnualWorkshopoftheAustralasianLanguageTechnologyAssociation,pages159–164,2019.[44]C.Zhang,Y.Sun,H.Chen,andJ.Wang.Generatingadequatedistractorsformultiple-choicequestions.arXivpreprintarXiv:2010.12658,2020.[45]X.Zhou,S.Luo,andY.Wu.Co-attentionhierarchicalnetwork:Generatingcoherentlongdistractorsforreadingcomprehension.InProceedingsoftheAAAIConferenceonArtificialIntelligence,volume34,pages9725–9732,2020.[46]B.Zou,P.Li,L.Pan,andA.Aw.Automatictrue/falsequestiongenerationforeducationalpur-pose.InProceedingsofthe17thWorkshoponInnovativeUseofNLPforBuildingEducationalApplications(BEA2022),pages61–70,2022.於本研究計畫中，我們目標基於自然語言生成技術進行選擇題自動出題研究。我們預期的展示情境為：給定一知識範圍(如農業文集)，我們可於系統直接輸入一組主題關鍵詞，而後自動生成關於該主題相關之選擇題題組(含字詞等級、句子等級、挑選是非類型之選擇題)。然而，如同前言背景中所言，現有技術離目標仍有許多待強化之處，對此我們擬定三大出題目標：字詞等級干擾選項、句子等級干擾選項與反事實干擾選項進行研究。針對此三大目標，我們擬定(1)基於知識圖譜之強化與(2)基於生成語言模型與對比學習之強化三個核心技術進行探討。研究技術項目與項目間之關係與進行規劃如Figure7所示。於第一年度中，我們規劃「考量題幹可用性與干擾選項間關聯之Retrieve-and-Generate架構優化」、「基於語言模型與知識圖譜之Text2Text干擾選項生成」與「基於Masked-LM之反事實干擾選項生成研究」進行研究。「考量題幹可用性與干擾選項間關聯之Retrieve-and-Generate架構優化」著眼於優化目前https://mcq-demo.nlpnchu.org之WordLevel之選擇題生成系統。希冀自動過濾相似或語意不正確題幹，並且考量干擾選項彼此間之關聯進行最終干擾選項集合之綜合挑選。「基於生成語言模型之Text2Text干擾選項生成模型」則考量Text2Text模式利用生成模型直接生成干擾選項序列，藉此嘗試生成句子等級之干擾選項。並進一步地探索結合知識圖譜與語言模型之干擾選項生成，希冀透過知識圖譜之整合來生成更有誘導性且合理之字詞等級干擾選項。此外，於「基於Masked-LM之反事實干擾選項生成研究」項目中，我們預計著手挑戰反事實干擾選項生成，我們嘗試利用透過NER擷取給定事實陳述句中之關鍵字並Masked-LM該關鍵字來填入機率較低之字詞，以為反事實語句生成之作法。我們預期第一年度工作完成後，將可優化現有單詞等級干擾選項之選擇題自動生成系統，並於第二年度中，我們則規劃「基於知識圖譜強化之反事實干擾選項生成」、「基於對比式學習之干擾選項生成模型強化」與「利用Open-DomainQuestionAnswering錯誤結果之干擾選項挑選」。於此年度中，進一步嘗試利用知識圖譜萃取三元組知識後進行反事實語句之生成，基本想法為透過萃取三元組後再進一步輸入語言模型，並希望語言模型適當地進行反事實語句的改寫。此一部分預計使用第一年度之「基於Masked-LM之反事實干擾選項生成研究」之基礎以為訓練資料。對於字詞等級的干擾選項，我們則探索利用Open-DomainQuestionAnswering模型所錯誤回報之答案當作干擾選項。我們希望透過此不同之想法來取得更多樣的干擾選項。再者，如前言所述，生成模型生成序列時常出現生成結果重複的狀況，對此我們嘗試利用對比式來改善生成模型這方面的缺點，以優化干擾選項之生成。第二年度工作項目完成後，我們可初步提供反事實干擾選項之選擇題生成、優化後之句子等級干擾選項生成與基於ODQA方式取得之字詞等級干擾選項生成。於第三年度中，我們則規劃「基於NodeCentrality考量主題涵蓋性之選擇題題組生成」與「生成難度可控之干擾選項生成研究」。如同前言所述，一個理想的選擇題出題系統應該考慮題組生成，整體題目之主題涵蓋性，對此我們也將嘗試基於知識圖譜涵蓋的方式考慮節點centrality重要度方式來進行題組之生成。同時我們也將嘗試題目難易度之控制生成研究。我們的想法是當自動生成成為可能後未來的出題應該是個人化模式；針對個人的學習狀況進行測驗，因此我們規劃「個人化與生成難度可控之干擾選項生成研究」進行探討。第三年度，我們也預計投入人力進行系統的實作開發與整合。以下茲針對擬訂之研究議題其問題挑戰與1.基於語言模型與知識圖譜之Text2Text干擾選項生成模型近年來，語言模型除了使用CandicateGeneratingandRanking架構的MaskedLM之外，使用Encoder-Decoder架構的Text2Text模型於生成任務上也有很好的成果，因此我們的想法是把干擾選項生成Model視為一個Text2Text任務，嘗試讓模型直接生成干擾選項。且使用Text2Text模型生成Distractor有別於用MaskedLM生成Distractor，我們希望模型能生成的Distractor有以下特性(1)非單詞之Distractor:目前在Distractor生成上多為單詞生成，要生成一段敘述/句子的干擾選項還沒有很好的做法。(2)特定domain之MCQ:目前的MCQ多為英文測驗，若想針對特定領域出選擇題，使用MaskedLM生成的Distractor會因語料庫限制而為達到上述的兩種目標需求，我們設Q為輸入之問句，A為輸入問句之答案，並將Q與A字串接字串當作輸入，訓練模型輸出一連串問句Q的干擾選項字串D=(d1,d2,d3)。模型架構如圖2所示，我們將「哪一種技術最適用於嚴重特殊傳染性肺炎之病原快速篩檢?」與「聚合￿連鎖反應」接在一起輸入至Text2Text模型，而模型生成了「重組DNA技術」、「電腦斷層掃描」與「組織培養」三個干擾選項，三個選項皆為醫療檢測方式且都不是單詞，符合如前言所述，我們也預計探索使用知識圖譜來強化生成干擾選項的能力。如Figure8的圖例，我們可以使用語言模型結合醫療知識圖譜，利用給定的文本從知識圖譜中萃取有包含這些topicentities以及相鄰的nodeentities的子圖，再分別將文本與子圖輸入至語言模型和GraphNeuralNetwork(GNN)，讓模型學習找出關鍵的資訊與強化network中重要entitynodes跟edges的權重，藉由GNN的額外資訊與語言模型能力生成更好的選項。Figure8為我們規劃做法的概觀。主要分兩階段進行：(1)關鍵字子圖與GNNEmbedding萃取與(2)GNNEmbedding與LanugageEmbedding間之FusionEncoder生成。具體而言，第一個階段是將從知識圖譜裡萃取相關的知識圖譜子圖。我們一開始先使用NERModel抓取題幹中的entities，再透過這些entities透過知識圖譜去尋找entitynodes以及所有跟entitynodes相鄰3-hop內的bridgenodes，將檢索出來的結果當成知識圖譜子圖，並在後續第二階段提供以Figure9的圖例，我們先將問題透過NERModel斷詞得到”白血病”、”發病”、”器官”與”發病原因”當作entities，再透過這些entities當作關鍵字透過Retriever從知識圖譜萃取Figure8:結合知識圖譜與語言模型生成之構想架構圖與這些詞相關的node和edge，如利用”白血病”與”器官”能找到”肝臟”、”骨髓”、”肺部”等等entities，我們能利用這些entities來當作後續的生成選項。第二個階段則將文本與第一階段得到的知識圖譜子圖分別輸入至語言模型與GNN進行訓練，訓練目標為將Input的[MASK]Token還原來產生干擾選項，先將語言模型的Input額外新增[MASK]Token，經過N層LMLayer做self-attention後，透過FusionEncoder再結合知識圖譜的資訊與原本的文本去生成選項，透過以上的手法，來強化干擾選項之生成。2.考量題幹可用性與干擾選項間關聯之Retrieve-and-Generate架構優化如同研究目的所述，本研究項目著眼於優化現有Retrieve-and-Generate生成架構。我們•文章檢索器過濾:在Retrieving-and-Generating方法中Retriever會檢索回傳含有主題字詞的文章，但有部分文章並不適合當作問句生成模型的輸入，那些文章含有太多的雜訊，我們想嘗試在這步驟先過濾一些文章。我們的想法是利用詞向量中的相似詞再次檢索Retriever回傳的文章，把過於不相關的文章濾掉，減少問句生成模型輸出過於複•生成題幹品質自動過濾：問句生成模型輸出的問句，會再交由使用者挑選，但為了更好的效果，我們想嘗試利用語言模型做初步的篩選，減少人工挑選時間。我們想嘗試利用ODQA模型來過濾問句，將問句生成模型輸出的問句輸入至問答模型後，根據問答模型的回答結果與信心分數來決定該問句是否合適當作MCQ。如Figure10所例，我們將生成之問句再次投入ODQA模型，透過答案選項之信心度來為問句生成過濾之基Figure10:Unanswerablequestionsolution•考量干擾選項間關聯之干擾選項挑選:如前言所述，一個可以提升選擇題生成品質￿考量干擾選項彼此之間之難易度。而在Retrieving-and-Generating方法中，生成干擾選項的方式為取出詞向量模型中與主題字詞相似之詞，使用相似度前三名當作干擾選項，因此有選項過於相近而造成問題太難的可能。為做改善，我們取出所有相似詞的向量與主題字詞向量計算相對距離，設定各個選項與答案的相似度範圍(距離範圍)，再從中隨機挑選相似詞，讓干擾選項彼此之間難易度有所區分以提升試題之鑑別度。•基於語言模型之干擾選項候選生成:雖然詞向量模型生成distractor過程迅速，但如果有主題字詞沒有被詞向量模型看過或有主題字詞一字多意，就無法生成適當的干擾選項。這個問題我們想嘗試使用MaskedLM來解決，我們將Retriever回傳的文章中含有主題字詞的段落記錄下來，移除主題字詞後輸入MaskedLM，再從MaskedLM輸出的候選字中選出distractor，使用這個方式生成的干擾選項不會被上述的缺點受限，也更Figure11:OptimizedRetrievingandGeneratingFramework3.基於Masked-LM之反事實干擾選項生成研究本研究項目為基於Masked-LM技巧來生成反事實干擾選項。我們需要先查找與事實相符之敘述，再修改成反事實干擾選項。我們預計之作法為：首先讓使用者先輸入考題關鍵字，再使用retriever查找相關文章，其目的為從相關文章中選取與事實相符之敘述。接著，將retriever檢索到的文章回傳給使用者，由使用者選取與事實相符之敘述。並按照以下之步驟進行Masked-LM反事實干擾選項生成。為了抽換專有名詞，我們先使用命名實體與事實相符之敘述中的所有專有名詞，以下圖為例”腎小管藉壓力差方式再吸收葡萄糖回微血管”，透過命名實體模型會萃取出專透過命名實體模型，我們會萃取出多個與事實相符之敘述中的專有名詞，我們需要選擇一個專有名詞行抽換。我們認為該專有名詞在相關文章中的重要程度越高，越高機率是考題中的重點。為了確保抽換專有名詞的品質，我們使用BM25來查看各個專有Figure12:AutoEncoder模型與AutoRegressive模型做MaskLM3.使用Maskedlanguagemodel模型進行改寫Maskedlanguagemodel，簡稱為MaskLM，該任務的會將輸入中的部分內容遮擋起來，模型會根據其他未被遮擋的內容，預測出被遮擋的內容或與其擁有相似語義的內容，現有的MaskLM模型分為兩種，分別為AutoEncoder框架的MaskLM模型與AutoRe-gressive框架的MaskLM模型，經典模型如BERT與BART。針對MaskLM任務，兩種模型進行預測的方式也有所不同，AutoEncoder框架的模型會預測出字，而AutoRe-gressive框架的模型會預測出字、詞或是句子，詳細範例如Figure12。根據抽換專有名詞的需求，我們將使用Autoregressive模型做MaskLM任務來抽換重要的專有名詞。然而模型預測出的內容眾多，有可能是原先遮擋的內容、遮擋內容的同義詞或是與遮擋內容擁有相似語義的內容。理想的情況下，模型預測的內容最高機率為原先遮擋的內容，若我們選擇原先遮擋的內容或是遮擋內容的同義詞進行抽換，會造成與事實相符之敘述未改變，便無法生成反事實干擾選項。為了避免此狀況發生，我們使用NLI任務中的換句話說(MRPC)任務來檢測其是否為干擾選項。換句話說任務主要在判斷兩句話的語意是否相同，為了避免MaskLM模型選用同義詞進行抽換，即模型生成的干擾選項仍是正確的，我們將與事實相符之敘述與生成之干擾選項作為模型的輸入，若模型認為兩句話具有相同語義，我們會讓MaskLM模型預4.基於知識圖譜強化之反事實干擾選項生成於本研究項目中，我們嘗試利用知識圖譜提供語言模型額外資訊達成反事實干擾選項生成。我們的構想如Figure4所示，包含以下步驟。首先我們會讓使用者先輸入考題內文，再將內文使用斷詞或NER模型萃取entity，利用這些entity於知識圖譜中查找相關子圖，子圖必須包含內文中所有的entitynodes以及所有跟entitynodes相鄰節點，以此為知識生成之基礎。接著我們將檢索到的子圖內的nodes與edges萃取成三元組，每筆三元組的結構是以node,edge,node組成。如同圖中所示，每一筆三元組描述一基本事實陳述，如導體可以3.將與事實相符之敘述修改為反事實干擾選項為了將與事實相符之敘述更改為反事實干擾選項，我們將前一步驟所萃取的三元組與考題內文當作作語言模型的輸入來生成反事實干擾選項。我們預計透過生成語言模型與輸入給定的問句和三元組來生成與問句情境相符合的反事實干擾選項，以FigurejQuery36305983518858060475_1676211393343為例，生成語言模型在給定關於與導體相關的題目敘述以及從知識圖譜萃取與導體相關的三元組後，模型藉由這些三元組推斷其中的關係，比如”導體可以導電->石墨是導體->筆芯是由石墨製成”，從上述的三元組推斷出”筆芯可以導電”的事實，也可以改寫成”筆芯不能導電”的反事實干擾選項。5.使用Open-DomainQuestionAnswering錯誤結果之干擾選項挑選研究如前言所述，我們預計使用ODQA模型所回答之錯誤結果來當作干擾選項(如Figure5)。以我們先前曾開發https://agri.nlpnchu.org/之農業問答系統為例，我們發現以上想法所面臨的挑於ODQA的架構下，可能會遇到ODQA正確回答所下達之問題，其回傳結果皆為可行之答案，也因此無法被使用來當干擾選項，對此，我們探索兩種作法來進行調整。–1.使用BM25，且刪除尋找出來文章的正確答案在使用BM25的情境下，因為是使用關鍵字比對，所以會找出許多跟正確答案相關的文章，我們將找到文章的正確答案刪除，這樣這個文章會變成絕對回答不出正確答案的文章，可是因為Reader對文字有一定的理解能力，並且具有關鍵字的文章中也很可能會有，他會從剩下可能的答案中，尋找一個最相近的選項，這樣就可以較容易生出干擾選項。如Figure13所示，我們將答案”2,4-D”從文章中直接刪除後，再交給Reader進行閱讀，藉此確保正確答案不會被回傳。Figure13:基於ODQA架構之Retriever弱化方法–2.使用DensePassageRetrieverDensePassageRetriever[16]，DensePassageRetriever可以將資料庫的文章轉為向量的方式表示，在搜尋的時候也將Query轉為向量，並且比對兩個向量的相似度，這樣就可以找出相似的文章，可是我們想說，可以利用DPR找尋文章相似度的這一點，理論上相似的文章跟Query算出來的相似度會是最高的，我們可以將相似度高的文章拿出來，並且查看有沒有答案出現在上面，如果沒有答案，我們就將他丟給Reader，因為我們覺得如我相似度高，就代表Retriever認為裡面有答案，可是裡面並沒有含有我們的正解，所以這篇文章就含Retriever認為是答案，但是實際上不是答案的文字，所以這時候我們將這篇文章傳給Reader，就會有比較高在我們使用baseline的情境中，有時候也會出現不相關的答案，例如在??中，問模型關於殺草劑的問題，會出現的人力除草，人力除草跟題目所問的殺草劑名稱是不相關的，所以我們需要用一個方法把他過濾掉。在這種情況下，我們如果加入正確答案當作Query一起輸入Retriever，就會增加模型找到正確passage的機率，對我們想找跟正確答案不相關但是相似度較高的Passage會產生反效果，所以我們採用的是從Reader輸出之後把他過濾掉的方法。當Reader從Retriever拿到Passage之後，我們會將正確答案去除，之後再將剩餘的答案丟入一個LanguageModel作判斷，如果他跟答案的距離過於遙遠，就放入NegativeList，如果是相關的就放入PositiveList，之後要挑相關答案的時候可以從PositiveList裡面挑選。6.基於對比式學習之干擾選項生成模型強化ContrastiveLearning的設計上需要考量如何去設定PositiveSample以及NegativeSample。在我們的目標中，因為不希望模型會重複的生成出相同意思的句子，所以我們預計使用同義的句子當作NegativeSample，而資料集中的干擾選項可以被當作PositiveSample。然而，由於我們的資料集中並沒有NagativeSample的選項，也就是同義於模型生成的選項。對此，我們的想法為透過迭代生成來產生我們需要的NegativeSample。具體而言，如Figure14所舉例，在DistractorGenerator的第一次生成中，我們會輸入Question以及Answer使模型生成出干擾選項。而在第二次迭代中，我們除了會輸入QuestionAnswerPair外，還會再加上在上一次迭代中生成的干擾選項，以提示模型先前已生成的Distractor的資訊。對於第二次迭代的輸出來說，第一次迭代輸出的Distractor，即為它的NegativeSample，因為DistractorGenerator必須要生成出與上一次迭代生成的Distractor意義相異的句子，所以它們之間的SentenceEmbedding不能太過接近。Figure14:IterationGenerater示意，在第一次的迭代中生成的Distractor會作為第二次迭代的輸入，以提示模型不要生成出類似的DistractorFigure15:對比式學習於干擾選項生成之預想架構我們所規劃之整體框架，如Figure15所示。我們使用多次迭代的方式來生成Distractor，對於第一次迭代來說，我們會從其他in-batch的data中取得Sentence當作NegativeSample。而在第二次以後的迭代來說，除了其他in-batch的Sentence以外，我們還會將此次迭代前所生成的Distractor加入我們的NegativeSample中。透過NegativeSample和模型輸出的Distractor的dotproduct可以計算出ContrastiveLoss，此外我們也會加入Distractor和Label計算的CrossEntropyLoss，來計算出我們的TotalLoss15。ContrastiveLoss定義如下:x為生成的distractor，p為資料集中的Distractor(Label)，N為NegativeSampleSet，N={n1,n2,...nj}，其中包括先前迭代生成的Distractor以及其他in-batch的選項。7.基於NodeCentrality考量主題涵蓋性之選擇題題組生成如同前言所述，本研究議題之目標為考量主題涵蓋性之選擇題題組生成。因此，我們預計根據使用者所指定的主題，按Figure16架構規劃來進行題組生成。1.透過知識圖譜子圖萃取來涵蓋所指定主題之內容：藉由知識圖譜上NodeCentrality方式來萃取關鍵字，以為題幹之生成。我們的基本想法為在知識圖譜上我們定位出與使用者輸入之主題關鍵字相關之結點後，我們可以試者套入PageRank或者其他圖形上計算NodeCentrality重要節點挑選，並以重要節點為題幹生成之主軸。2.透過上一步取得之重要答案片段後，進行問題之生成。並生成較多的問題數量以為CandidateQuestionPool，而後將問題將題組生成問題設定一成為一選擇最佳化議題(Optimization)：根據主題生成大量的單一題目，而後定義UtilityScore來計算個別題目組合後之優劣，來挑選最佳化的題組以為輸出。於本研究項目中，我們預計探索以下兩種生成難度控制方式。•調整Masked-LM比例之反事實選項難度控制：現有的Masked-LM模型會將重要的專有名詞進行遮擋，模型會透過未遮擋的內容對該專有名詞進行預測。理想的情況下，當我們僅對該專有名詞進行遮擋時，模型可成功地預測出遮擋的專有名詞或語義相近的專有名詞，這不僅是因為MaskLM模型夠強大，也是因為未遮擋的內容提供模型足夠多的資訊讓模型預測，若未遮擋的內容提供的資訊不夠多，則模型預測出的內容有可能會與遮擋的內容語義相差很多。我們根據Masked-LM模型的此特性對題目的難易度進行控制，若我們將多個專有名詞進行遮擋，再讓模型預測時，模型會因為提供的資訊不足，而預測出語義較不相近的專有名詞，即較不可能的答案。以下表為例，我們可以觀察到當[Mask]越多時，我們要判斷該事實是否正確就越形容易。[Mask]借壓力差吸收[Mask]回微血管[Mask]借壓力差吸收[Mask]回[Mask]•調整ODQA檢索文章之難度控制："}, "1676211510.5607574": {"單位": "", "標題": "研究計畫之背景與目的", "作者": "", "發布日": 0, "摘要": "", "全文": "自然語言生成(NaturalLanguageGeneration)將是未來深度學習領域之熱門研究議題。2022年年末ChatGPT語言模型所帶來的示範性與展示效果預期將吸引更多研究目光。而個人於中興大學之研究團隊近年於自然語言生成之耕耘已建立初步研究成果。在過去朝著自然語言生成方向前進，我們探索自動生成試題之研究方向前進。於前期研究計畫「基於深度學習語言模型之閱讀測驗自動化生成研究109-2221-E-005-058-MY3」中，我們著眼於基於深度學習模型進行閱讀測驗(ReadingComprehensionTesting)題型之自動出題。在計畫執行過程中，我們•Ying-HongChan,Yao-ChungFan:KeywordProvisionQuestionGenerationforFacilitatingEducationalReadingComprehensionPreparation.INLG(InternationalNaturalLanguageGen-erationConference)2022.•Ho-LamChung,Ying-HongChan,Yao-ChungFan:ABERT-basedDistractorGenerationSchemewithMulti-taskingandNegativeAnswerTrainingStrategies.EMNLP(EmpiricalMethodsinNaturalLanguageProcessing)Findings2020.•Shang-HsuanChiang,Ssu-ChengWang,Yao-ChungFan:CDGP:AutomaticClozeDistractorGenerationbasedonPre-trainedLanguageModel.EMNLP(EmpiricalMethodsinNaturalLanguageProcessing)Findings2022.•Hsien-YungPeng,Ho-LamChung,Ying-HongChan,Yao-ChungFan:MisleadingInferenceGenerationviaProximalPolicyOptimization.PAKDD(AdvancesinKnowledgeDiscoveryandDataMining-26thPacific-AsiaConference)2022此外我們也將前期計畫之自動出題研究成果進行實作，可於以下兩個網址進行實測。•問句生成QuestionGeneration：給定文章、指定片段，透過自然語言生成模型進行問句生成。目前系統可於https://classic.queratorai.com/進行實測。•克漏字干擾選項生成DistractorGeneration：此一部分針對給定之文章進行克漏字誘導選項生成。目前系統可於：https://cdgp-demo.nlpnchu.org/進行實測。基於過去之研究成果累積，本計劃延續自然語言生成技術於自動出題之研究成果，擬定知識類型選擇題(MultipleChoiceQuestion,MCQ)自動生成為目標。以四技二專統一測驗考試為例(如Table2所示)，知識類型選擇題將包含題幹與四個選項(其中包含正確答案與干擾選項)，主要針對某一知識範圍進行測驗。我們認為知識範圍選擇題之自動生成可分為兩部分：(1)問題題幹(Stem)生成與(2)干擾選項(Distractor)生成。題幹生成主要目標為選擇題問題本身之生成，而干擾選項則著眼於誘導選項之生成。知識類型選擇題為學習測驗最常見的測驗模式之一。對比於閱讀測驗生成，知識類型選擇題的自動生成，將可減輕教師準備個人化教學測驗之負擔，也為學生提供更有效率之學習檢測方式。以現有我們成果為基礎，一個基本之自動選擇題出題系統作法為(如圖1)所示：我們可利用資訊檢索技術(如BM25Retriever)，從給定之文集資料庫(目前使用2021中文維基百科所有中文文章)中檢索主題字詞相關文章後，結合前期計畫所開發之問句生成模型(QuestionGeneration)[4]生成題幹，而後再利用詞向量模型(例如word2vec模型)找出相近字詞以為干擾選項。我們將這樣的構想先行實作一系統(稱之為Retrieve-and-GeneratePipeline)，目前此系統可於https://mcq-demo.nlpnchu.org/使用。使用者可藉由輸入主題相關字詞，選擇題生成系統將自動生成輸入字詞相關之選擇題結果。舉例而言，我們以「骨髓、白血病、淋巴」為主題字詞輸入至系統。我們挑選較佳之成果顯示於Table1。自動生成選擇題AutomaticGeneratedMCQQ1:哪一種癌症種類的統稱中可以看出它會造成不正常白血球的大量增生?(a)白血病、(b)乳腺癌、(c)腎病、(d)帕金森氏症Q1:白血病的發病原因通常是發生在哪一個器官上?(a)肝臟、(b)淋巴、(c)甲狀腺、(d)骨髓Q:白血病是最常見的兒童癌症，其中80%的病例是慢性的哪一種性病變?(a)淋巴結、(b)微血管、(c)淋巴、(d)黏膜Table1:目前選擇題自動生成結果範例。目前系統可於https://mcq-demo.nlpnchu.org/。目前我們挑選較佳之生成結果顯示於此Table中。建議審查委員自行輸入關鍵字詞組，來觀察我們Figure1:Retrieve-and-Generate基本架構。本例中我們以”骨髓、白血病、淋巴”為關鍵詞進行文章檢索，而再使用”骨髓”為答案，分別生成問句與干擾選項。從展示之結果中，我們發現自動選擇題出題之可能性。但仔細觀察會發現現有系統離實際應用仍有空間須待努力。也因此觸發本期研究計畫之提出。具體而言，我們認為選擇題自•句子等級之干擾選項生成(Sentence-LevelDistractorGeneration)首先，關於干擾選項的部分，若仰賴詞向量模型來產生干擾選項，我們所能取得的干擾選項便僅限於單詞(Word-Level)等級。從統測試題中，我們發現許多題目之干擾選項屬於句子等級(Sentence-Level)。以Table2中Q3與Q4選擇題為例，我們可以看到選項皆為句子等級之描述，此為目前基於詞向量來產生干擾選項的做法所不能。•反事實干擾選項生成(Counter-FactualSentenceGeneration)我們觀察到選擇題題型中，有著下列何者為錯誤或正確類型之考題。其干擾選項是一個較長的描述句。以Table2為例，Q5與Q6為分別詢問下列選項何者正確/錯誤。此題型，為變像之是非題。除了句子等級生成外，核心技術牽涉的是反事實(Counter-Factual)句子之生成。如何生成此類型之句子以為干擾選項也將是研究重點所在。•選擇題生成品質提升(MCQQuestionQualityImprovement)此外，我們也看到retreive-and-generatepipeline的做法，所產生的題幹本身有時是一個過於空泛之問題(例如，下列何種疾病為兒童常見之癌症？)或者並非語意正確之語句(例如，中樞性的什麼症狀是缺乏抗利尿激素的?)。如何有效判斷生成不良之語句，減低人工進一步過濾/挑選題目之負擔也是需進行研究之議題。此外，基於詞向量之干擾選項作法也有待商榷;隨機從相近詞向量挑選distractor，是否能確保distractor之誘導品類型I:單詞等級的干擾選項Word-LevelDistractorsQ1：淋巴器官是人體防禦系統中重要的一環，可提供淋巴細胞發育或活化所需要的條件與狀態。下列何者是淋巴球進行防禦與活化的器官？(a)骨髓(b)胸腺(c)肝臟(d)脾臟Q2：健康人體感覺神經傳入大腦的訊息大部分經過轉接，(a)視丘(b)下視丘(c)中腦(d)橋腦類型II:句子等級的干擾選項Sentence-LevelDistractors(a)無就巢性(b)體型大(c)著肉多(d)耐粗食Q4：下列何者為「聯合國國際糧農組織」(FAO)成立最重要的目標及首要任務？(a)消除及戰勝飢餓(b)聯絡及交流農業資訊(c)自然環境的永續及保護(d)實現國際間的公平貿易類型III:句子等級且與事實相違背的干擾選項Counter-FactualDistractorsQ5：有關人類上皮細胞的細胞週期之敘述，下列何者正確？(c)後期，分離的染色分體往細胞中央移動(d)末期，細胞膜內凹且細胞質開始一分為二Q6：有關一健康成人泌尿系統功能的敘述，下列何者錯誤？(a)腎臟每日約產生濾液180公升及尿液1.5公升(b)腎小管藉壓力差方式再吸收葡萄糖回微血管(c)腎臟藉排尿量多寡來維持體內水分的恆定(d)若出現尿毒症狀時，可使用血液透析方式進行治療Table2:四技二專統一測驗選擇題類型概觀。Q1與Q2擷取自110學年度護理基礎生物考科，Q3與Q4110學年度農業群農業概論考科，Q5與Q6取自110學年度農業群基礎生物考科質？也許所挑選的distractor並不具誘導性，使得受測者很輕易地可挑選出正確答案，或者挑選到答案的同義詞，造成一題有多個正確答案。根據先前計畫執行訪談教師所知，於測驗時為考量受測全體鑑別度，教師出題時會考量干擾選項彼此之間之難易度；通常三個干擾選項中，會有一個離正確答案較遠之干擾選項(誘導性較低)，一個次之，一個最為困難最具有誘導性。藉此來有適當的鑑別度。目前我們直接使用詞向量之作法並未考量此設計。•主題題組生成(MCQQuestionGroupGeneration)目前我們的系統僅為單一關鍵字詞之出題。我們認為一個良好的自動出題應該是以題組為單位進行出題；使用者指定一主題知識，系統根據此一主題知識範圍進行5-10題之選擇題自動生成，題目之間也應該彼此考量。目前Retreive-and-Generatepipeline的做法，我們使用使用者給定的關鍵字為答案，進行題目之生成。然而比較好的做法應為將使用者給定的關鍵字集合為知識範圍之指定，系統根據該範圍自動挑選重要概念關鍵字進行答案挑選，而後再進行選擇題題組之生成。針對上述之研究挑戰，於本研究規劃中我們擬定下列八項研究議題進行探討。•研究項目ㄧ：基於語言模型與知識圖譜之Text2Text干擾選項生成模型(DistractorGenerationbasedonText2TextLanguageModelandKnowledgeGraph)Figure2:Text2Text干擾選項生成架構Figure3:基於Masked-LM之反事實干擾選項現有之Distractor生成研究多是基於CandicateGenerating-and-Ranking架構，先透過知識庫(如Wordnet)或者透過MaskedLM的方式取得候選字後，再適當進行候選詞特徵排序以為最終干擾選項。對此我們有不同之想法，我們預計嘗試把干擾選項生成設定為一個Text2Text任務；如圖2所例，預期的輸入為題幹Q與答案A之串接字串，透過ModelEncoder與Decoder轉化輸出為一連串之干擾選項字串“重組DNA技術[SEP]電腦斷層掃描[SEP]組織培養”。我們透過語言生成模型直接將輸入對比轉換為輸出。此外，透過此一Text2Text方式，生成句子等級的干擾選項也將為可能。再者，近年來語言模型研究趨勢除了將模型參數量與訓練資料持續增加外，引入知識圖譜來強化生成模型於常識理解與推理能力也成為研究社群的關注所在[5][13]。也因此，探索整合知識圖譜於選擇題自動生成也因此成為我們規劃之研究項目。希冀藉由整合知識圖譜來提升干擾選項之誘導性與題幹生成之正確性。•研究項目二：基於Masked-LM之反事實干擾選項生成研究(Counter-FactualSentenceGenerationbasedonMasked-LM)如同前言所述，“下列何者正確/錯誤”題目為常見之類型選擇題題型。對此，我們預計嘗試以下之作法。首先，我們假定可取得正確敘述之語句S。我們先針對語句S中的字詞進行命名實體辨識(NamedEntityRecognition,NER)，其後再利用語言模型針對所取得之命名實體進行WholeWordMasking(WWM)，填入機率較低之填空結果，藉此生成反事實描述之語句。如Figure4所例，我們將所給定之事實描述，藉由NER方式挑選”葡萄糖”一次進行Masked，再藉由語言模型推測該Masked字詞，並挑選機率相對較低之預測填入。藉此創造出反事實之陳述句。然而，如何確保不挑選到同義詞？要進行多少個字詞之Masked？如何自動從主題文章中挑選事實陳述句以進行改寫？等等•研究項目三：基於知識圖譜強化之反事實干擾選項生成(Counter-FactualSentenceGen-erationbasedonKnowledge-GraphAugmentedLanguageModel)針對反事實干擾選項生成之議題，除了研究項目二之Masked-LM作法外，我們另一個構想為利用知識圖譜來強化Counter-Factual語句之生成。如Figure4所示，我們基本想法為先萃取與題幹相關聯之知識圖譜子圖(LocalSubgraph)並取得知識三元組，以此輸入至語言模型中，訓練語言模型反向改寫三元組語句，以此為反事實干擾選項。然而，此模式之訓練資料該如何取得？如何從知識圖譜上萃取子圖，並生成合適三元組？如何引入GraphNeuralNetwork來萃取結構？為此研究項目需進行探討之工作內容。•研究項目四：基於對比式學習之干擾選項生成模型強化(DistractorGenerationModelbasedonContrastiveLearning)我們過去的研究經驗[8]與相關研究[35]皆指出當使用Generative語言模型來生成句子Figure4:基於知識圖譜強化之反事實干擾選項生成構想層級之輸出時，模型經常會產生語意之相同，但字面的結果。如Table3所示，選項A跟C雖然字面敘述上不同，但實際上都是描述同樣的意思。生成重複選項將導致選擇副交感神經興奮會導致使心跳加速，血壓上升Table3:模型生成的誤導選項重複出現的範例對此，我們預計探索對比式學習(ContrastiveLearning)[7][35]來提升生成品質。對比式學習於自然語言之應用中主要想法為透過選取PositiveSample以及NegativSample使得模型去學習到生成句子跟同義句子的相同以及跟異類句子的不同之處，這樣我們可以使用重複選項當作我們的NegativeSample，然後資料集中的干擾選項當作PostiveSample，期望模型可以學習避免生成出重複的誤導選項。因此，我們預計探索對比式•研究項目五：使用Open-DomainQuestionAnswering錯誤結果之干擾選項挑選研究(Us-ingIncorrectAnswersfromODQAasDistractors)而除了利用生成模型產生Distractors外，我們發現利用Open-DomainQuestionAnswer-ing(ODQA)模型所產生的錯誤結果是一個值得探討的干擾選項的取得方式。具體而言，現有的ODQA模型主要包含一個Retriever與一個Reader，Retriever負責檢索相關的文章，而Reader利用檢索出來的文章利用閱讀理解模型(ReadingComprehensionModel)來找尋答案片段。由於ODQA並非完美，因此也會有錯誤結果之回報。但我們發現錯誤結果與正確答案並非天差地遠，閱讀理解模型所回報之結果通常也與答案接以圖5為例，當我們下達Q：「下列何者為目前世界上應用最廣的殺草劑，其具備有生長素之特性，常用於闊葉雜草防治，亦為選擇性殺草劑的一種？」，ODQA系統將取得圖5中右半部之答案：「萌前除草劑、2,4-D、非選擇性除草劑、10殺丹粒劑」。其中，2,4-D為正確答案，餘者為錯誤答案。然而我們覺得這些錯誤答案，可以當作Q之干擾選項來使用。有別於基於生成模型研究項目，利用ODQA模型之錯誤答案來當作干擾選項應可提供不同思路進行干擾選項生成。這樣的想法乍看可行，但仍有若干技術點需進行探討。諸如，如何控制ODQA模型之錯誤？我們需要在Reader部分弱化Reader的能力，或者弱化Retriever能力，來製造ODQA的錯誤？如何從錯誤回報結果挑選干擾選項？這些皆是尚待深入研究之議題。也因此觸發本研究項目之規劃。Figure5:利用ODQA模型之錯誤答案來當作Distractors•研究項目六：基於NodeCentrality考量主題涵蓋性之選擇題題組生成(MCQGroupGenerationconsideringTopicCoveragebasedGraphNodeCentrality)上述所規劃的選擇題生成做法，主要針對單題進行生成。若要產生多題/題組考題，就是重複相同流程，獨立產生個別考題後，整合為一題組。然而，題組生成做法應當考慮題與題之間之關聯性與互補性。因此，我們認為比較好的做法應當為根據使用者所指定的主題，透過(1)知識圖譜子圖萃取來涵蓋所指定主題之內容與(2)透過主題重要概念萃取：藉由知識圖譜上NodeCentrality方式來萃取關鍵字，以為題幹之生成。此外，如何根據主題互補性，來挑選個別生成之題組，用以組成選擇題題組也是需進行•研究項目七：考量題幹可用性與干擾選項間關聯之Retrieve-and-Generate架構優化(Op-timizingRetrieve-and-GenerateFrameworkconsideringStemUtilityandDistractorCorrela-tion)本研究項目著眼於優化現有Retrieve-and-Generate架構設定。Retrieve-and-Generate架構雖然簡單，但我們可以發現其於單字等級的干擾選項生成已有一定的可用性。我們預計可優化之處為：(1)我們發現引用現有系統於題幹生成時，會有題幹生成結果不佳的狀況。對此，我們發現其中的一個原因為Retriever所檢索之文章品質不佳(文章過短或者與主題不相關)，導致不良之題幹生成。因此如何改善檢索機制來挑選較佳的文章用以進行生成，便是值得探討的課題。再者，題幹生成不佳也有部分原因是來自於問句生成模型本身之生成錯誤，如何有效過濾生成不佳的題幹也因此是一個可需進行研究之項目。(2)如同前言所述，干擾選項之挑選應該考量彼此間之關聯，而非獨立挑選。如何納入此觀點來進行干擾選項之生成也因此成為預計關注之議題。又再者，我們基於word2vec方式之字詞等級干擾選項挑選是否可以引入語言模型之詞向量來求取•研究項目八：生成難度可控之選擇題生成研究Difficulty-ControlledMCQGeneration發展自動化生成考題之可能願景為：針對個人化學習自對應生成難度適中之測驗，以增加學習與檢測之效率。因此，如何針對現有技術探討生成結果難度控制也因此成為一可探索之研究議題。由於我們的基礎架構為基於文章檢索來取得包含使用者所給定的關鍵字之文章，並使用該文章與關鍵字為問題生成模型之輸入，來生成選擇題問句。我們若挑選較為冷門之文章進行生成，則預期可以生成較為困難之題幹。而就干擾選項而言，若我們基於研究項目五「使用Open-DomainQuestionAnswering錯誤結果之干擾選項挑選研究」，利用ODQA來取得干擾選項之作法。我們可藉由調整輸入給閱讀理解模型之文章來產生難度較高的錯誤。具體而言，給定一有包含正確答案A之文章C，我們先將答案片段A於文章D中刪除，而後將其輸入至閱讀理解模型，藉此產生錯誤之閱讀理解結果，以這樣的錯誤結果當干擾選項預期將有較高之誘導能力。相類似的想法也可以套用為挑選Retriever認為相關度比較不高的文章以為閱讀理解模型之輸入。此外，就生成題目之廣度與深度而言，我們可藉由控制取得之文章來調配題組之廣度與深度。例如，若我們以單篇或少數篇文章進行生成，我們所生成的主題題組針對測驗內容而言，應該可產生知識性較為深入之試題。而若針對較多篇文章，則應可產生範圍較廣之測驗試題。上述之研究議題，乍看可行但仍有許多細節需進行探索，如上述研究背景目的所述，本研究計畫擬探索基於現有Retrieve-and-GeneratePipeline之選擇題自動生成架構進行強化。我們以三年為期，規劃在此目標下擬定八項關鍵技術進行研究探討。如Figure6所示，八個工作項目又可區分為三項主軸：「選擇題生成品質提升」、「反事實選項生成」與「群組生成與難度控制」。其中，於「選擇題生成品質提升」研究主軸中，我們探索利用語言模型與知識圖譜架構來生成，將選擇題選項生成問題設定成為一Text2Text任務，輸入為問題題幹與正確答案所串接之TextSequence，而輸出為所生成之distractor串接之TextSequence。此外，我們也嘗試引入對比式學習來改善Text2Text生成模式之品質。我們也預期規劃探索利用ODQA方式來取得干擾選項。而於「反事實選項生成」研究主軸中，我們探索反事實陳述句之生成。我們預計嘗試基於MaskedLM取代重要名詞之反事實語句生成作法與基於知識圖譜知識三元組萃取改寫之作法。於第三個研究主軸中，我們則嘗試針對題組生成與難度可控制之干擾選項生成進行研究。MultipleChoice,Multi-此資料集特色在於為Multi-hop形式，答案需從複數文檔中推理而成；資料來自維基百科以及PubMed，PubMed為生物醫學論文的搜尋引擎，在兩個來源各自提供51,318(Wiki)和2,508(PubMed)筆問題。資料來自國高中英文考試，提供超過10萬筆問題以及2.8萬篇文本，題型上為英文閱讀測驗，會需要閱讀故事或對話紀錄，而針對該文本會有複數題目，每個題目此資料集的挑戰在於，內容來自於7個不同領域(新聞、小學自然科學、旅遊、小說故事等等)，為文本與題目提供多樣性，總計約1萬筆問題與800篇文本。MultipleChoice,Open-為Open-domain形式，並不提供文本，只有問題與答案；包含12,247筆問題、一個正確答案和四個干擾選項；題目類型為知識與常識題，來自於ConceptNet與同時包含兩種形式資料，填空題與Open-domain；填空題形式命名為QUASAR-S，其中資料來源自StackOverflow，提供約37K筆問題。Open-domain形式命名為QUASAR-T，提供約43K筆問題。來自StanfordUniversity的閱讀理解資料集，包含了超過100K筆問題和536篇對應的維基百科文本；延續了SQuAD1.1的題目外，[31]另外加入約50K筆無答案的題目，進一步測試機器能否在無對應答案時回答否，而MedQA是基於美國醫師執照考試(USMLE)所產生的資料集，其中問題皆為較為困難的專業醫學相關的知識，且包含了英文、簡體中文、繁體中文分別有約12K、34K、14K個問題集，這種專業的知識能使機器針對單一領域產生更具參考價值的distractor，目前此任務排名上最佳為ValentinLiévin團隊做出的Codex5-shot訓練機器學習模型進行閱讀理解(MachineReadingComprehensionTask)為最常見的深度學習自然語言任務。諸如，SQuAD[32]、RACE[21]、DROP[10]與HotPotQA[41]等資料集，皆是測驗/訓練機器學習模型於答案找尋、邏輯推理與摘要之能力。其中，許多機器閱讀理解任務採用多選題方式進行測驗。選擇題的自動答題與選擇題的自動生成為一體兩面之應用。因此具有選擇題樣態的資料集為我們選擇題自動生成研究之根本資料集，我們將目前所探知之相選擇題與干擾選項生成之相關研究可按應用類型可區隔為英文學習測驗與知識學習測驗。干擾選項生成研究於英文學習測驗應用，主要以自動生成閱讀理解與克漏字類型的考題為目標。閱讀理解之干擾選項生成與本研究計畫之知識類型選擇題之自動出題目標不同。因此本研究回顧中，我們不做此分類每一研究細部說明。我們將此類型之相關研究按不同象限做分類：(1)干擾選項層級之差異(生成字詞等級與句子等級之差異)象限，(2)題目類型(閱讀理解與克漏字測驗)，(3)生成方式(CandidateRanking與Generative)。我們按此三象限，將目前之相關研究摘要整理於Table5中。而知識學習測驗類型的選擇題生成，則於這一兩年逐漸吸引相關研究投入。我們認為與本研究計畫較為相關之研究有[22][40][3]，以下我們針對個別文獻討論如下。首先，與我們有相同目標者為[22]，也是針對特定知識主題生成選擇題。其主要作法Table5:現有閱讀測驗與克漏字干擾選項生成研究整理表為：先由人工給定一個主題，程式使用該主題名稱的維基百科頁面作為起始點(例如給定“體育”後搜尋體育相關維基百科)，保留熱門文章並取出文章的第一段擷取摘要，由此產生大量句子。並將這些句子依單純英文文法規則生成簡單5W問句與答案，例如who問句是將第一個動詞前的名詞作為正確答案並從句子中移除，what問句是將第一個夾在動詞間的名詞移除作為正確答案並從句子中移除，移除答案後的句子再帶入問題模板生成制式問句。生出五種問句後再依英文文法的詞與詞的相關性去決定哪個問句最適合。在干擾選項生成的部分，先隨機選取其他句子所產生的答案，再藉由語言模型去排序與問題的相關性，取前三高之預測為干擾選項。但本論文並未使用問句生成模型，採取之作法也與本計畫所規劃有所不同。於論文中[40]，則探討如何讓干擾選項更加多樣且與正確答案相差不遠(以具誘導性)。對此，創造了一款監督式分類任務來找出文章中屬於關鍵句子(及對產生干擾選項生成有用的句子)，作者在訓練之前會將訓練用的干擾選項和文章中的每個句子做比對，若該句子和干擾選項相似度達到一定程度，則該句子將成為文章中的關鍵句，透過這個方式在文章中能被修改成為新的干擾選項的句子將會達到多樣性的效果，且透過相似度對比，干擾選項也不會跟原始文章相差太遠從而提升干擾選項的品質。本研究之作法可供我們借鏡參考對關鍵文章/句選擇設計，用以提升干擾選項之多樣性。而與本計畫研究項目五「使用Open-DomainQuestionAnswer錯誤結果之干擾選項」走非生成想法相似之研究為[3]。其主要想法為作者發現於干擾選項資料集中，有許多干擾選項一再被使用。促使論文作者想為新的問題檢索以重複使用干擾選項。作者提出一個Transformer-baseddistractorretrievalmodels，該模型通過考慮問題的上下文來選擇干擾選項模型包含三個部分，分別為D-SIM模型、Q-SIM模型以及DQ-SIM模型。在D-SIM模型中使用mBERT計算同一個MCQ中問題和答案以及對應的干擾選項的向量，並計算他們之間的相似度。而在Q-SIM模型中作者假設不同問題共享一個或多個干擾選項或答案，代表問題可能在語意上相關，因此他們相關的干擾選項可以用作彼此的良好候選干擾選項，因此作者將訓練資料重新排序，使這些共享至少一個干擾選項或答案的問題聚集在一起，接著將資料送進Q-SIM模型計算不同的MCQ中問題之間的相似度。在DQ-SIM模型線性結合前兩個模型，從各自的優勢中獲益，透過基於分數和基於排名的合併策略計算來自D-SIM模型和Q-SIM模型的分數，並設計演算法使得高排名之干擾選項有較大的權重。重複使用現有資料集中之干擾選項為一創新想法，但仍與我們之ODQA做法有所不同。[OpenDomainQuestionAnswering相關研究]本計畫的選擇題生成架構主要基於兩元件:(1)文章萃取與(2)問句生成。目前我們所使用的文章萃取機制為傳統之BM25檢索。而近年來由於ODQA研究領域之發展，基於深度學習之DenseRetrieval技巧也紛紛於ODQA研究領域中所提出。我們預期DenseRetrieval技術也將於我們知識類型選擇題自動出題中擔任關鍵角色。茲簡要回顧現有ODQA之研究如下。ODQA架構中Retriver之重要性不言可喻。不同層級的語意檢索也影響著ODQA的效能。於[28]研究中，作者提出一個簡單而有效的pipeline系統，研究段落和句子級別語義檢索及其對下游任務的潛在影響，作法上是先檢索段落級別的段落，然後從上一步檢索後取得的段落中再去檢索句子級別的段落。該系統在事實驗證和Multi-hopQA方面進行評估，實驗結果顯示QA任務與事實驗證皆仰來上游檢索之結果。於[23]論文中，作者在ODQA架構的retreiver端提出了一個有效率的弱監督式學習的方法(InverseClozeTask)來對retreiver進行預訓練，給予模型完全無人工標記的維基百科原始資料，藉由論文中的方法將資料切分成固定長度的段落，並隨機選取其中的一個句子當作標籤，讓模型去學習如何從給定的句子去資料庫中找尋與之最相關的文章。在實驗數據中此方法訓練出的ODQA模型分數高於傳統的BM25檢索方法。而於DPR[16]論文中，作者為解決傳統BM25無法檢索語意相似段落的問題，提出了基於深度學習模型之DensePassageRetrieval，並提出使用in-batchnegative的技術，在密集向量的檢索取得極佳的效果改進。我們也預計引入中文DRP模型來提升目前題幹生成所需之而於GAR[27]研究中，作者們探討傳統基於BM25的檢索無法檢索沒有單詞重疊或語意相似的段落的問題，而基於密集向量的檢索方式在計算上又過於複雜，且會有資訊損失的問題。該論文透過預訓練語言模型自動生成與查詢語句相關的內容，可增加查詢語句的資訊量，隨後透過BM25進行檢索，該方法比起傳統BM25能檢索到更精確的段落，且執行效率也快於基於密集向量的檢索方式。透過GAR中的PLM可對查詢語句的資訊進行補充，原論文中是補充答案、相關段落及段落標題，可基於該手法讓PLM生成問題與原始答案的關聯，對這些關聯進行組合或刪除即可檢索與原答案有相似關係的查詢語句，可檢索到適合製作干於[19]，作者觀察到基於語言模型之檢索技術雖然效果很好，但比以往的方法需要極高的計算成本，特別是在計算每個query與document間的relevancescore時。本篇論文作者提出了ColBERT，引入lateinteraction架構，對query與document進行獨立的encoding，於模型輸出最終階段才進行相似度運算。而[18]中，則使用近一步基於ColBERT當基礎模型來進行retreiver端的訓練，使用了Relevance-guidedSupervision(RGS)的方法來訓練retreiver的模型。相較於傳統的retreiver訓練資料，傳統方法需人工標註的訓練資料，但作者認為人工標住資料往往存在偏見，進而影響模型的效能，所以在論文中提出了用訓練好的模型來找尋最適合的訓練資料，並把這些資料用於訓練下一個模型，使用這個方式來進行後續的訓練，最後得出最佳的模型。此方法訓練出的模型分數明顯高於目前opendomain的其他方式。在找尋訓練資料時，模型會先搜尋前k筆資料，並把前20筆資料中含有shortanswer的資料當成possitiveexample，而前20筆以外剩餘的資料中不含有shortanswer的資料當成negativeexample，此方法尋找的負面訓練資料交由reader進行預測能得到與正確答案相關的錯誤答案，如能改良ranking的方式，應用於干擾選項生成，預期將能提升干擾選項之品質。[ContrastiveLearning於自然語言生成相關研究]對比式學習始見於電腦視覺領域，其於影像應用上有著顯著的效果提升。因此，許多自然語言處理之研究者也將其引用至自然語言生成領域，諸如[11][7][35]。於[11]研究中，作者們提出了使用ContrastiveLearning的作法來學習SentenceEmbedding，他們使用BERT作為基礎模型，透過不同的dropoutmask使模型能用同樣的輸入產生不同的SentenceEmbedding，但它們之間的距離會是近的，也就可作為PositiveSample。然後再使用其他in-batch的句子作為NegativeSample，以達到非監督式訓練。而在有訓練資料的情況下，如NLI資料集，則可以將Eentailment視為PositiveSample，Contradiction以及其他in-batch的句子視為Negative而再基於[11]研究後，於[7]研究中，作者們提出加上了Discriminator來加強SentenceEmbedding的學習效果。具體而言，對每筆輸入來說，會透過SentenceEncoder產生一個SentenceEmbedding，同時也會隨機的Masking輸入句子其中的幾個Token，並透過一個fixed的Generator來還原MaskToken，而Discriminator要試著透過沒被修改過的SentenceEmbedding的資訊去辨認修改後的句子中的哪一個Token有被修改過，整個框架會透過SentenceEmbedding的ContrastiveLearningLoss以及Discriminator的ReplacedTokenDetection而與我們預計引入對比式學習來提升句子等級的干擾選項生成效果想法接近的研究也可見於[35]。該研究指出發現了以往autoregressive模型使用BeamSearch去決定要生成的文本時，會產生語意不自然以及沒意義的重複文字，儘管後續有人使用取樣以及修改ObjectiveFunction來嘗試解決這個問題，但這個方法時常會產生語意不連貫的問題。因此作者提出了使用ContrastiveLearningLoss加上原先的MaximumLikelihoodEstimationLoss來訓練模型，並且使用ContrastiveSearch來作為decodingmethod。[Counter-FactualSentenceGeneration相關研究]於Counter-FactualGeneration研究中，[44,33]為相關研究之先驅。主要想法是使用神經網路來生成反事實敘述，這個概念背後的想法是同時使用語言模型和各種採樣的策略來達成生成反事實敘述的效果。接續之前的成果，[39]提出了Polyjuice架構，可以控制輸出的反事實敘述的類型。另外一種生成干擾選項的研究是使用干擾選項生成(DistractorGeneration)的手法，干擾選項生成(DistractorGeneration)的目標是給予了一篇文章、問題和正確答案後，模型能夠生成似是而非的干擾選項。於研究[24]中，作者最先提出使用了神經網路來生成干擾選項，而研究[8]則延續了前者的成果，把重點放在生成複數的干擾選項並提出了一個負樣本學習策略(negativetrainingstrategy)來加強干擾選項生成。但這上述兩個研究主要都著眼於閱讀理解題型中干擾選項之生成，與本研究知識類型選擇題生成目標所有不同。而在關於AbductiveInferenceGeneration的相關研究中，於[30]中作者提出了故事改寫的任務，此任務目標是要改寫一段故事的敘述並且導致產生與原先不同的結局，與顯永的論文的方式的差異為故事改寫任務是根據給定的誤導推論來生成不同的結局，在MIG任務中是給定敘述句與其前後文來生成誤導推論。[2]研究中是讓模型根據上下文來推論中間適合加入的句子，此研究主要是產生正確的推論敘述，而在MIG任務[29]主要是產生相反的誤導推論敘述。於我們先前的研究[29]中，我們提出一個自然語言的誤導推論生成任務，任務步驟是當給定了一個陳述事實的句子並給定其上下文，將其定義為(x,y,z)與誤導推論ˆy，模型會生成與該事實相反的推論句子，由此可以讓模型產生出多選題似是而非的干擾選項。然而這樣任務與我們所設定之反事實出題仍有所不同。而與我們生成Counter-Factual陳述句想法接近的研究為：[46]。於該研究中，作者探討有別於過往的是非題，將文章中原始句子作為True的問句並通過反義詞替換關鍵字生成False問句，本篇作者通過使用一種新穎的masking-and-infilling策略來生成更靈活與複雜的問句。要被mask的關鍵字找法有很多種，像是SRL結果中的predicate，句子中連接詞(”that”、”when”、”since”)之後的部分或是數字等等，來找出值得提問的句子。Infilling的作法則是以BART為基底作為生成模型，將原始句子的關鍵字mask後，提供前後句子作為BART的上下文後產生句子填入。作者為了找值得提問的句子所提出的方法亦可用來作為多選題產生題目的答案選擇方法，而infilling的想法如果使用得宜，在提供出題範圍的前後文後，也能將欲出題句子挖空讓BART模型填補後產生選項。雖然與我們目標相似，但仍與我整合知識圖譜與語言模型為近年語言模型訓練熱門關注議題。於[42]研究中，作者們提出一CounterfactualGenerationControllableCounterfactualGenerationDistractorGenerationMisleadingInferenceGeneration種能將文本和知識圖譜的資訊進行深度雙向、自我監督的預訓練的方法，透過作者的方法可以將一些特定領域的KnowledgeBase資料集結合相關文獻去訓練語言知識模型，讓語言模型和知識圖譜之間的資訊互相交換，並且在回答特定領域問答的任務上，表現優於現有的語言模型和知識圖譜增強模型。我們想透過作者提供的預訓練模型，來做多選題選項生成，透過知識圖譜所額外提供的資訊，讓在考慮生成選項時，除了題目和答案的文字外，用這些文字從知識圖譜萃取相關的知識圖譜子圖幫助生成選項有更多參考資訊。知識圖譜於問句生成之研究也見於[5][13]。於[5]研究中，作者之想法為使用知識圖譜進行問題生成任務時，通常只會取出資料與關係的三元組合，並不能完整利用知識圖譜中豐富的資訊，因此作者基於GraphNeuralNetworks提出了BidirectionalGatedGraphNeuralNetwork(BiGGNN)，將知識圖譜中的node及edge編碼成向量，使得每個node都包含了周圍其他node的資訊，並對答案的node進行標註，隨後透過RNN進行問句生成，生成過程中RNN會決定生成新文字或直接複製某個node的資訊放進問題。我們應可借鏡此研究論文的想法，可將知識圖譜及答案的資訊完整編碼，可基於此特性，在生成問題時同時考慮答案附近的node，讓模型在建立問題的同時也能根據附近的node產生干擾選項。而於[13]研究中，則提出的ISEEQ架構，主要目的是希望從簡短的敘述句中去尋找更多訊息來反問使用者是否符合關於敘述句背後未提及的情況，在使用時，使用者只需輸入簡短的敘述句，模型會先從知識圖譜擴展並增加query的關鍵字，透過這些關鍵字模型就能retrieve更多相關的文章進行ranking，在從這些文章中使用QG模型產生一些原始query中未提及但可能相關背景知識，例如當使用者說道:“我感到疲勞且渾身無力”，ISEEQ能回覆:“請問病徵持續了多久?”，或是“你最近14天有出國過嗎?”這類可能與其他知識相關的問句。這種方式最大的貢獻及是讓機器學習到”反問”的能力，而非一味的透過query生成出結果。這樣的想法，或許可引用來提升題幹生成的精確度，舉例來說:有位教師想出關於日治時期的歷史題目，我們就能先藉由ISEEQ來詢問是要出與日治時期戰爭相關的題目或是與日治時期建設相關的題目，以此來取得更精確的題幹生成需求。[1]J.Araki,D.Rajagopal,S.Sankaranarayanan,S.Holm,Y.Yamakawa,andT.Mitamura.Gen-eratingquestionsandmultiple-choiceanswersusingsemanticanalysisoftexts.InProceedingsofCOLING2016,the26thInternationalConferenceonComputationalLinguistics:TechnicalPapers,pages1125–1136,2016.[2]C.Bhagavatula,R.L.Bras,C.Malaviya,K.Sakaguchi,A.Holtzman,H.Rashkin,D.Downey,S.W.-t.Yih,andY.Choi.Abductivecommonsensereasoning.arXivpreprintarXiv:1908.05739,[3]S.K.Bitew,A.Hadifar,L.Sterckx,J.Deleu,C.Develder,andT.Demeester.Learningtoreusedistractorstosupportmultiplechoicequestiongenerationineducation.IEEETransactionsonLearningTechnologies,2022.[4]Y.-H.ChanandY.-C.Fan.Arecurrentbert-basedmodelforquestiongeneration.InProceedingsofthe2ndWorkshoponMachineReadingforQuestionAnswering,pages154–162,2019.[5]Y.Chen,L.Wu,andM.J.Zaki.Towardsubgraphguidedknowledgegraphquestiongenerationwithgraphneuralnetworks.arXivpreprintarXiv:2004.06015,2020.[6]S.-H.Chiang,S.-C.Wang,andY.-C.Fan.Cdgp:Automaticclozedistractorgenerationbasedonpre-trainedlanugagemodel.InFindingsoftheAssociationforComputationalLinguistics:[7]Y.-S.Chuang,R.Dangovski,H.Luo,Y.Zhang,S.Chang,M.Soljac￿ic￿,S.-W.Li,W.-t.Yih,Y.Kim,andJ.Glass.Diffcse:Difference-basedcontrastivelearningforsentenceembeddings.arXivpreprintarXiv:2204.10298,2022.[8]H.-L.Chung,Y.-H.Chan,andY.-C.Fan.Abert-baseddistractorgenerationschemewithmulti-taskingandnegativeanswertrainingstrategies.InFindingsoftheAssociationforComputationalLinguistics:EMNLP2020,pages4390–4400,2020.[9]B.Dhingra,K.Mazaitis,andW.W.Cohen.Quasar:Datasetsforquestionansweringbysearchandreading.arXivpreprintarXiv:1707.03904,2017.[10]D.Dua,Y.Wang,P.Dasigi,G.Stanovsky,S.Singh,andM.Gardner.ingcomprehensionbenchmarkrequiringdiscretereasoningoverparagraphs.arXiv:1903.00161,2019.[11]T.Gao,X.Yao,andD.Chen.Simcse:Simplecontrastivelearningofsentenceembeddings.arXivpreprintarXiv:2104.08821,2021.[12]Y.Gao,L.Bing,P.Li,I.King,andM.R.Lyu.Generatingdistractorsforreadingcomprehen-sionquestionsfromrealexaminations.InProceedingsoftheAAAIConferenceonArtificialIntelligence,volume33,pages6423–6430,2019.[13]M.Gaur,K.Gunaratna,V.Srinivasan,andH.Jin.Iseeq:Informationseekingquestiongenerationusingdynamicmeta-informationretrievalandknowledgegraphs.InProceedingsoftheAAAIConferenceonArtificialIntelligence,volume36,pages10672–10680,2022.[14]Q.Guo,C.Kulkarni,A.Kittur,J.P.Bigham,andE.Brunskill.Questimator:Generatingknowl-edgeassessmentsforarbitrarytopics.InIJCAI-16:ProceedingsoftheAAAITwenty-FifthInternationalJointConferenceonArtificialIntelligence,2016.[15]D.Jin,E.Pan,N.Oufattole,W.-H.Weng,H.Fang,andP.Szolovits.Whatdiseasedoesthispa-tienthave?alarge-scaleopendomainquestionansweringdatasetfrommedicalexams.AppliedSciences,11(14):6421,2021.[16]V.Karpukhin,B.Og￿uz,S.Min,P.Lewis,L.Wu,S.Edunov,D.Chen,andW.-t.Yih.Densepassageretrievalforopen-domainquestionanswering.arXivpreprintarXiv:2004.04906,2020.[17]D.Khashabi,S.Chaturvedi,M.Roth,S.Upadhyay,andD.Roth.Lookingbeyondthesurface:Achallengesetforreadingcomprehensionovermultiplesentences.InProceedingsofthe2018ConferenceoftheNorthAmericanChapteroftheAssociationforComputationalLinguistics:HumanLanguageTechnologies,Volume1(LongPapers),pages252–262,2018.[18]O.Khattab,C.Potts,andM.Zaharia.Relevance-guidedsupervisionforopenqawithcolbert.TransactionsoftheAssociationforComputationalLinguistics,9:929–944,2021.[19]O.KhattabandM.Zaharia.Colbert:Efficientandeffectivepassagesearchviacontextualizedlateinteractionoverbert.InProceedingsofthe43rdInternationalACMSIGIRconferenceonresearchanddevelopmentinInformationRetrieval,pages39–48,2020.[20]G.Kumar,R.E.Banchs,andL.F.D’Haro.Revup:Automaticgap-fillquestiongenerationfromeducationaltexts.InProceedingsoftheTenthWorkshoponInnovativeUseofNLPforBuildingEducationalApplications,pages154–161,2015.[21]G.Lai,Q.Xie,H.Liu,Y.Yang,andE.Hovy.Race:Large-scalereadingcomprehensiondatasetfromexaminations.arXivpreprintarXiv:1704.04683,2017.[22]G.LeBerre,C.Cerisara,P.Langlais,andG.Lapalme.Unsupervisedmultiple-choicequestiongenerationforout-of-domainq&afine-tuning.InProceedingsofthe60thAnnualMeetingoftheAssociationforComputationalLinguistics(Volume2:ShortPapers),pages732–738,2022.[23]K.Lee,M.-W.Chang,andK.Toutanova.Latentretrievalforweaklysupervisedopendomainquestionanswering.arXivpreprintarXiv:1906.00300,2019.[24]C.Liang,X.Yang,N.Dave,D.Wham,B.Pursel,andC.L.Giles.Distractorgenerationformultiplechoicequestionsusinglearningtorank.InProceedingsofthethirteenthworkshoponinnovativeuseofNLPforbuildingeducationalapplications,pages284–290,2018.[25]C.Liang,X.Yang,D.Wham,B.Pursel,R.Passonneaur,andC.L.Giles.Distractorgenera-tionwithgenerativeadversarialnetsforautomaticallycreatingfill-in-the-blankquestions.InProceedingsoftheKnowledgeCaptureConference,pages1–4,2017.[26]V.Liévin,C.E.Hother,andO.Winther.Canlargelanguagemodelsreasonaboutmedicalquestions?arXivpreprintarXiv:2207.08143,2022.[27]Y.Mao,P.He,X.Liu,Y.Shen,J.Gao,J.Han,andW.Chen.Generation-augmentedretrievalforopen-domainquestionanswering.arXivpreprintarXiv:2009.08553,2020.[28]Y.Nie,S.Wang,andM.Bansal.Revealingtheimportanceofsemanticretrievalformachinereadingatscale.arXivpreprintarXiv:1909.08041,2019.[29]H.-Y.Peng,H.-L.Chung,Y.-H.Chan,andY.-C.Fan.Misleadinginferencegenerationviaproxi-malpolicyoptimization.InPacific-AsiaConferenceonKnowledgeDiscoveryandDataMining,pages497–509.Springer,2022.[30]L.Qin,A.Bosselut,A.Holtzman,C.Bhagavatula,E.Clark,andY.Choi.Counterfactualstoryreasoningandgeneration.arXivpreprintarXiv:1909.04076,2019.[31]P.Rajpurkar,R.Jia,andP.Liang.Knowwhatyoudon’tknow:Unanswerablequestionsforsquad.arXivpreprintarXiv:1806.03822,2018.[32]P.Rajpurkar,J.Zhang,K.Lopyrev,andP.Liang.Squad:100,000+questionsformachinecomprehensionoftext.arXivpreprintarXiv:1606.05250,2016.[33]S.Ren,Y.Deng,K.He,andW.Che.Generatingnaturallanguageadversarialexamplesthroughprobabilityweightedwordsaliency.InProceedingsofthe57thannualmeetingoftheassociationforcomputationallinguistics,pages1085–1097,2019.[34]S.RenandK.Q.Zhu.Knowledge-drivendistractorgenerationforcloze-stylemultiplechoicequestions.InProceedingsoftheAAAIConferenceonArtificialIntelligence,volume35,pages[35]Y.Su,T.Lan,Y.Wang,D.Yogatama,L.Kong,andN.Collier.Acontrastiveframeworkforneuraltextgeneration.arXivpreprintarXiv:2202.06417,2022.[36]A.Talmor,J.Herzig,N.Lourie,andJ.Berant.Commonsenseqa:Aquestionansweringchallengetargetingcommonsenseknowledge.arXivpreprintarXiv:1811.00937,2018.[37]J.Welbl,N.F.Liu,andM.Gardner.Crowdsourcingmultiplechoicesciencequestions.arXivpreprintarXiv:1707.06209,2017.[38]J.Welbl,P.Stenetorp,andS.Riedel.Constructingdatasetsformulti-hopreadingcomprehensionacrossdocuments.TransactionsoftheAssociationforComputationalLinguistics,6:287–302,[39]T.Wu,M.T.Ribeiro,J.Heer,andD.S.Weld.Polyjuice:Automated,general-purposecounter-factualgeneration.arXivpreprintarXiv:2101.00288,1(2),2021.[40]J.Xie,N.Peng,Y.Cai,T.Wang,andQ.Huang.Diversedistractorgenerationforconstruct-inghigh-qualitymultiplechoicequestions.IEEE/ACMTransactionsonAudio,Speech,andLanguageProcessing,30:280–291,2021.[41]Z.Yang,P.Qi,S.Zhang,Y.Bengio,W.W.Cohen,R.Salakhutdinov,andC.D.Manning.Hotpotqa:Adatasetfordiverse,explainablemulti-hopquestionanswering.arXiv:1809.09600,2018.[42]M.Yasunaga,A.Bosselut,H.Ren,X.Zhang,C.D.Manning,P.Liang,andJ.Leskovec.Deepbidirectionallanguage-knowledgegraphpretraining.arXivpreprintarXiv:2210.09338,2022.[43]C.Y.Yeung,J.S.Lee,andB.K.Tsou.Difficulty-awaredistractorgenerationforgap-fillitems.InProceedingsoftheThe17thAnnualWorkshopoftheAustralasianLanguageTechnologyAssociation,pages159–164,2019.[44]C.Zhang,Y.Sun,H.Chen,andJ.Wang.Generatingadequatedistractorsformultiple-choicequestions.arXivpreprintarXiv:2010.12658,2020.[45]X.Zhou,S.Luo,andY.Wu.Co-attentionhierarchicalnetwork:Generatingcoherentlongdistractorsforreadingcomprehension.InProceedingsoftheAAAIConferenceonArtificialIntelligence,volume34,pages9725–9732,2020.[46]B.Zou,P.Li,L.Pan,andA.Aw.Automatictrue/falsequestiongenerationforeducationalpur-pose.InProceedingsofthe17thWorkshoponInnovativeUseofNLPforBuildingEducationalApplications(BEA2022),pages61–70,2022.於本研究計畫中，我們目標基於自然語言生成技術進行選擇題自動出題研究。"}}